{"video_titles":{"pPN8d0E3900":"Capsule Networks (CapsNets) \u2013 Tutorial","2Kawrd5szHE":"How to implement CapsNets using TensorFlow"},"playlist_ids":{"pPN8d0E3900":"PLuTYjXW7aAt3HLCATBOkkXtifCVAm0O_A","2Kawrd5szHE":"PLuTYjXW7aAt3HLCATBOkkXtifCVAm0O_A"},"channel_ids":{"pPN8d0E3900":"UCCvGd1WBMpFQ_vtC89VF2qA","2Kawrd5szHE":"UCCvGd1WBMpFQ_vtC89VF2qA"},"autogen":{"pPN8d0E3900":[{"text":"hey I'm overly Asian and in this video","start":0.0,"duration":3.99},{"text":"I'll tell you all about capsule networks","start":2.04,"duration":4.279},{"text":"a hot new architecture for neural nets","start":3.99,"duration":4.95},{"text":"Jeffrey Henson had the idea of capsule","start":6.319,"duration":4.181},{"text":"Network several years ago and he","start":8.94,"duration":3.81},{"text":"published a paper in 2011 that","start":10.5,"duration":4.65},{"text":"introduced many of the key ideas but he","start":12.75,"duration":3.69},{"text":"had a hard time making them work","start":15.15,"duration":4.41},{"text":"properly until now a few weeks ago in","start":16.44,"duration":6.03},{"text":"October 2017 a paper called dynamic","start":19.56,"duration":5.37},{"text":"routing between capsules was published","start":22.47,"duration":4.41},{"text":"by Sarah saber Nicholas frost","start":24.93,"duration":3.81},{"text":"and of course Geoffrey Hinton they","start":26.88,"duration":3.45},{"text":"managed to reach state-of-the-art","start":28.74,"duration":4.22},{"text":"performance on the MS dataset and","start":30.33,"duration":5.28},{"text":"demonstrated considerably better results","start":32.96,"duration":5.02},{"text":"than convolutional neural nets on highly","start":35.61,"duration":5.64},{"text":"overlapping digits so what are capsule","start":37.98,"duration":5.73},{"text":"networks exactly well in computer","start":41.25,"duration":4.43},{"text":"graphics you start with an abstract","start":43.71,"duration":4.259},{"text":"representation of a scene for example a","start":45.68,"duration":5.62},{"text":"rectangle at position X 20 and y equals","start":47.969,"duration":5.91},{"text":"30 rotated by 16 degrees and and so on","start":51.3,"duration":5.13},{"text":"each object type has various","start":53.879,"duration":5.461},{"text":"instantiation parameters then you call","start":56.43,"duration":5.25},{"text":"some rendering function and boom you get","start":59.34,"duration":6.029},{"text":"an image inverse graphics is just the","start":61.68,"duration":5.85},{"text":"reverse process you start with an image","start":65.369,"duration":4.531},{"text":"and you try to find what objects it","start":67.53,"duration":4.83},{"text":"contains and what their instantiation","start":69.9,"duration":5.609},{"text":"parameters are a capsule Network is","start":72.36,"duration":5.189},{"text":"basically a neural network that tries to","start":75.509,"duration":5.61},{"text":"perform inverse graphics it is composed","start":77.549,"duration":6.781},{"text":"of many capsules a capsule is any","start":81.119,"duration":5.131},{"text":"function that tries to predict the","start":84.33,"duration":3.75},{"text":"presence and the instantiation","start":86.25,"duration":4.68},{"text":"parameters of a particular object at a","start":88.08,"duration":5.64},{"text":"given location for example the network","start":90.93,"duration":5.82},{"text":"above contains 50 capsules the arrows","start":93.72,"duration":5.67},{"text":"represent the output vectors of these","start":96.75,"duration":5.82},{"text":"capsules capsules output vectors the","start":99.39,"duration":5.909},{"text":"black arrows correspond to capsules that","start":102.57,"duration":5.339},{"text":"try to find rectangles while the blue","start":105.299,"duration":4.771},{"text":"arrows represent the output of capsules","start":107.909,"duration":5.67},{"text":"looking for triangles the length of an","start":110.07,"duration":5.67},{"text":"activation vector represents the","start":113.579,"duration":4.711},{"text":"estimated probability that the object","start":115.74,"duration":4.32},{"text":"the capsule is looking for is indeed","start":118.29,"duration":4.53},{"text":"present so you can see that most arrows","start":120.06,"duration":5.07},{"text":"are tiny meaning the capsules didn't","start":122.82,"duration":5.279},{"text":"detect anything but two arrows are quite","start":125.13,"duration":3.45},{"text":"long","start":128.099,"duration":2.61},{"text":"this means that the capsules at these","start":128.58,"duration":4.23},{"text":"locations are pretty confident that they","start":130.709,"duration":3.191},{"text":"found what they were looking","start":132.81,"duration":3.819},{"text":"for in this case a rectangle and the","start":133.9,"duration":6.6},{"text":"triangle so the orientation of the","start":136.629,"duration":5.911},{"text":"activation vector encodes the","start":140.5,"duration":4.079},{"text":"instantiation parameters of the object","start":142.54,"duration":3.93},{"text":"for example in this case the object's","start":144.579,"duration":3.931},{"text":"rotation but it could be also its","start":146.47,"duration":4.23},{"text":"thickness how stretched or skewed it is","start":148.51,"duration":4.8},{"text":"its exact location it might be slight","start":150.7,"duration":5.009},{"text":"translations and so on for simplicity","start":153.31,"duration":4.319},{"text":"I'll just focus on the rotation","start":155.709,"duration":4.17},{"text":"parameter but in real capsule Network","start":157.629,"duration":5.22},{"text":"the activation vectors may have five ten","start":159.879,"duration":6.42},{"text":"dimensions or more in practice a good","start":162.849,"duration":6.181},{"text":"way to implement this is to apply a","start":166.299,"duration":5.671},{"text":"couple convolutional layers just like in","start":169.03,"duration":5.609},{"text":"a regular convolutional neural net this","start":171.97,"duration":4.739},{"text":"will output an array containing a bunch","start":174.639,"duration":4.621},{"text":"of feature maps you can then reshape","start":176.709,"duration":5.28},{"text":"this array to get a set of vectors for","start":179.26,"duration":4.949},{"text":"each location for example suppose the","start":181.989,"duration":4.92},{"text":"convolutional layers output an array","start":184.209,"duration":5.101},{"text":"containing say 18 feature maps","start":186.909,"duration":5.371},{"text":"2 times 9 you can easily reshape this","start":189.31,"duration":5.25},{"text":"array to get two vectors of nine","start":192.28,"duration":5.73},{"text":"dimensions each for every location you","start":194.56,"duration":5.28},{"text":"could also get three vectors of six","start":198.01,"duration":4.589},{"text":"dimensions each and so on something that","start":199.84,"duration":4.44},{"text":"would look like the capsule network","start":202.599,"duration":3.631},{"text":"represented here with two vectors at","start":204.28,"duration":5.069},{"text":"each location the last step is to ensure","start":206.23,"duration":6.21},{"text":"that no vector is longer than one since","start":209.349,"duration":5.31},{"text":"the vectors length is meant to represent","start":212.44,"duration":4.379},{"text":"a probability it cannot be greater than","start":214.659,"duration":4.41},{"text":"one to do this we apply a squashing","start":216.819,"duration":4.411},{"text":"function it preserves the vectors","start":219.069,"duration":4.98},{"text":"orientation but it squashes it to ensure","start":221.23,"duration":5.899},{"text":"that its length is between zero and one","start":224.049,"duration":6.3},{"text":"one key feature of capsule networks is","start":227.129,"duration":5.801},{"text":"that they preserve detailed information","start":230.349,"duration":5.63},{"text":"about the object's location and its pose","start":232.93,"duration":6.029},{"text":"throughout the network for example if I","start":235.979,"duration":5.921},{"text":"rotate the image slightly notice that","start":238.959,"duration":4.801},{"text":"the activation vectors also change","start":241.9,"duration":4.29},{"text":"slightly right this is called","start":243.76,"duration":5.879},{"text":"equivariance in a regular convolutional","start":246.19,"duration":5.519},{"text":"neural net there are generally several","start":249.639,"duration":5.25},{"text":"pooling layers and unfortunately these","start":251.709,"duration":5.52},{"text":"pooling layers tend to lose information","start":254.889,"duration":5.07},{"text":"such as the precise location and pose of","start":257.229,"duration":4.951},{"text":"the objects it's really not a big deal","start":259.959,"duration":4.441},{"text":"if you just want to classify the whole","start":262.18,"duration":3.11},{"text":"image","start":264.4,"duration":2.72},{"text":"but it makes it challenging to perform","start":265.29,"duration":4.62},{"text":"accurate image segmentation or object","start":267.12,"duration":5.79},{"text":"detection which require precise you know","start":269.91,"duration":6.39},{"text":"location and pose the fact that capsules","start":272.91,"duration":5.55},{"text":"are equivariance makes them very","start":276.3,"duration":5.25},{"text":"promising for these applications alright","start":278.46,"duration":5.22},{"text":"so now let's see how capsule networks","start":281.55,"duration":4.32},{"text":"can handle objects that are composed of","start":283.68,"duration":4.35},{"text":"a hierarchy of parts for example","start":285.87,"duration":4.2},{"text":"consider a boat centered at position X","start":288.03,"duration":5.55},{"text":"equals 22 and y equals 28 and rotated by","start":290.07,"duration":4.47},{"text":"16 degrees","start":293.58,"duration":3.9},{"text":"this boat is compotes is composed of","start":294.54,"duration":5.55},{"text":"parts in this case one rectangle and one","start":297.48,"duration":5.16},{"text":"triangle so this is how would be rent","start":300.09,"duration":5.64},{"text":"rendered now we want to do the reverse","start":302.64,"duration":4.77},{"text":"we want universe graphics so we want to","start":305.73,"duration":3.9},{"text":"go from the image to this whole","start":307.41,"duration":4.17},{"text":"hierarchy of parts with their","start":309.63,"duration":4.95},{"text":"instantiation parameters similarly we","start":311.58,"duration":5.34},{"text":"could also draw a draw a house using the","start":314.58,"duration":4.29},{"text":"same parts a rectangle in a triangle","start":316.92,"duration":3.81},{"text":"that this arm organized in a different","start":318.87,"duration":5.31},{"text":"way so the trick will be to try to come","start":320.73,"duration":6.03},{"text":"to go from this image containing a","start":324.18,"duration":4.5},{"text":"rectangle in a triangle and figure out","start":326.76,"duration":4.59},{"text":"not only that the rectangle and triangle","start":328.68,"duration":5.46},{"text":"are at this location and this","start":331.35,"duration":4.8},{"text":"orientation but also that they are part","start":334.14,"duration":3.33},{"text":"of a boat not a house","start":336.15,"duration":4.02},{"text":"so yeah let's let's figure out how it","start":337.47,"duration":4.56},{"text":"would do this the first step we have","start":340.17,"duration":3.75},{"text":"already seen we run a couple of","start":342.03,"duration":3.69},{"text":"convolutional layers we reshape the","start":343.92,"duration":4.44},{"text":"output to get vectors and we squash them","start":345.72,"duration":5.43},{"text":"this gives us the output of the primary","start":348.36,"duration":4.41},{"text":"capsules we've got the first layer","start":351.15,"duration":4.56},{"text":"already the next step is where most of","start":352.77,"duration":4.68},{"text":"the magic and complexity of capsule","start":355.71,"duration":4.71},{"text":"networks takes place every capsule in","start":357.45,"duration":5.55},{"text":"the first layer tries to predict the","start":360.42,"duration":5.22},{"text":"output of every capsule in the next","start":363.0,"duration":5.07},{"text":"layer you might want to pause to think","start":365.64,"duration":4.47},{"text":"about what this means that the capsules","start":368.07,"duration":5.19},{"text":"at the primary the first layer tried to","start":370.11,"duration":5.1},{"text":"predict what the second layer capsules","start":373.26,"duration":4.83},{"text":"will output for example let's consider","start":375.21,"duration":5.04},{"text":"the capsule that detected the rectangle","start":378.09,"duration":6.24},{"text":"I'll call it the rectangle capsule let's","start":380.25,"duration":6.84},{"text":"suppose that there are two capsules in","start":384.33,"duration":5.46},{"text":"the next layer the house capsule and the","start":387.09,"duration":3.51},{"text":"boat capsule","start":389.79,"duration":3.6},{"text":"since the rectangle capsule detected a","start":390.6,"duration":5.569},{"text":"rectangle rotated by 16 degrees","start":393.39,"duration":5.089},{"text":"it predicts that the house capsule will","start":396.169,"duration":4.8},{"text":"detect a house rotated by 16 degrees","start":398.479,"duration":4.5},{"text":"that makes sense and the boat capsule","start":400.969,"duration":4.56},{"text":"will detect a boat rotated by 16 degrees","start":402.979,"duration":4.981},{"text":"as well that's what would be consistent","start":405.529,"duration":6.781},{"text":"with the orientation of the rectangle so","start":407.96,"duration":6.9},{"text":"to make this prediction what the","start":412.31,"duration":4.05},{"text":"rectangle capsule does is simply","start":414.86,"duration":4.5},{"text":"computes the dot product of a","start":416.36,"duration":6.54},{"text":"transformation matrix W IJ with its own","start":419.36,"duration":7.169},{"text":"activation vector at U at UI during","start":422.9,"duration":5.31},{"text":"training the network will gradually","start":426.529,"duration":3.75},{"text":"learn a transformation matrix for each","start":428.21,"duration":4.289},{"text":"pair of capsules in the first and second","start":430.279,"duration":4.47},{"text":"layer in other words it will learn all","start":432.499,"duration":4.711},{"text":"the part hole relationships for example","start":434.749,"duration":4.32},{"text":"the angle between the wall and the roof","start":437.21,"duration":3.979},{"text":"of a house and so on","start":439.069,"duration":4.65},{"text":"now let's see what the triangle capsule","start":441.189,"duration":6.01},{"text":"predicts right this time it's a bit more","start":443.719,"duration":5.91},{"text":"interesting given the rotation angle of","start":447.199,"duration":4.74},{"text":"the triangle it predicts that the house","start":449.629,"duration":4.891},{"text":"capsule will detect an upside-down house","start":451.939,"duration":5.7},{"text":"and the boat capsule will detect about","start":454.52,"duration":5.73},{"text":"rotated by 16 degrees these are the","start":457.639,"duration":5.131},{"text":"positions that would be consistent with","start":460.25,"duration":5.339},{"text":"the you know rotation angle of the","start":462.77,"duration":6.75},{"text":"triangle now we've got a bunch of","start":465.589,"duration":5.79},{"text":"predicted outputs what do we do with","start":469.52,"duration":4.41},{"text":"them well as you can see the rectangle","start":471.379,"duration":4.951},{"text":"capsule and the triangle capsules","start":473.93,"duration":5.099},{"text":"strongly agree on what the boat capsule","start":476.33,"duration":4.859},{"text":"will output in other words they agree","start":479.029,"duration":4.831},{"text":"that a boat positioned in this way would","start":481.189,"duration":4.23},{"text":"explain their own positions and","start":483.86,"duration":3.959},{"text":"rotations and they totally disagree on","start":485.419,"duration":4.43},{"text":"what the house capsule will output","start":487.819,"duration":4.5},{"text":"therefore it makes sense to assume that","start":489.849,"duration":4.84},{"text":"the rectangle and triangle are part of a","start":492.319,"duration":6.15},{"text":"boat not a house now that we know that","start":494.689,"duration":5.46},{"text":"the rectangle and triangle are part of","start":498.469,"duration":4.38},{"text":"the boat the outputs of the rectangle","start":500.149,"duration":5.19},{"text":"capsule and the triangle capsule really","start":502.849,"duration":4.05},{"text":"concern only the boat capsule this","start":505.339,"duration":3.54},{"text":"there's no need to send these outputs to","start":506.899,"duration":3.901},{"text":"any other capsule this would just add","start":508.879,"duration":4.44},{"text":"noise they should be sent only to the","start":510.8,"duration":3.45},{"text":"boat capsule","start":513.319,"duration":3.72},{"text":"this is called routing by agreements","start":514.25,"duration":5.699},{"text":"there are several benefits first since","start":517.039,"duration":5.49},{"text":"capsule outputs are only routed to the","start":519.949,"duration":4.38},{"text":"appropriate capsule in the next layer","start":522.529,"duration":4.441},{"text":"these capsules will get a cleaner input","start":524.329,"duration":3.551},{"text":"signal","start":526.97,"duration":3.46},{"text":"more accurate accurately determine the","start":527.88,"duration":5.82},{"text":"pose of the object second by looking at","start":530.43,"duration":5.37},{"text":"the paths of the activations you can","start":533.7,"duration":4.29},{"text":"easily navigate the hierarchy of parts","start":535.8,"duration":4.38},{"text":"and know exactly which part belongs to","start":537.99,"duration":4.53},{"text":"which objects like the rectangle biller","start":540.18,"duration":4.53},{"text":"belongs to the boats or the triangle","start":542.52,"duration":4.01},{"text":"belongs to the boat and so on","start":544.71,"duration":5.04},{"text":"lastly routing this by agreement helps","start":546.53,"duration":5.8},{"text":"parse crowded scenes with overlapping","start":549.75,"duration":4.68},{"text":"objects we will see this in a few slides","start":552.33,"duration":5.16},{"text":"but first let's look at how running by","start":554.43,"duration":5.43},{"text":"agreements is implemented in capsule","start":557.49,"duration":5.76},{"text":"networks here I have represented the","start":559.86,"duration":6.42},{"text":"various poses of the boat as predicted","start":563.25,"duration":5.1},{"text":"by the lower level capsules for example","start":566.28,"duration":4.47},{"text":"one of these circles may represent what","start":568.35,"duration":4.89},{"text":"the rectangle capsule thinks about the","start":570.75,"duration":5.4},{"text":"most likely pose of the boat and another","start":573.24,"duration":4.95},{"text":"circle may represent what the triangle","start":576.15,"duration":4.08},{"text":"capsule thinks and if we suppose that","start":578.19,"duration":4.23},{"text":"there are many other low level capsules","start":580.23,"duration":4.59},{"text":"then we might get a cloud of prediction","start":582.42,"duration":4.59},{"text":"vectors for the boat capsule like this","start":584.82,"duration":4.98},{"text":"in this example there are two post","start":587.01,"duration":4.65},{"text":"parameters one represents the rotation","start":589.8,"duration":3.72},{"text":"angle and the other represents the size","start":591.66,"duration":4.23},{"text":"of the boat as I mentioned earlier post","start":593.52,"duration":4.26},{"text":"parameters may capture many different","start":595.89,"duration":3.45},{"text":"kinds of visual features like skew","start":597.78,"duration":3.69},{"text":"thickness and so on or location a","start":599.34,"duration":4.14},{"text":"precise location so the first thing we","start":601.47,"duration":4.38},{"text":"do is we compute the mean of these","start":603.48,"duration":5.22},{"text":"predictions this gives us this vector","start":605.85,"duration":6.42},{"text":"the next step is to measure the distance","start":608.7,"duration":6.15},{"text":"between each predicted vector and the","start":612.27,"duration":4.98},{"text":"meet vector so I will use here the","start":614.85,"duration":5.58},{"text":"Euclidean distance but capsule networks","start":617.25,"duration":4.97},{"text":"actually use the scalar product","start":620.43,"duration":3.96},{"text":"basically we want to measure how much","start":622.22,"duration":4.9},{"text":"each predicted vector agrees with the","start":624.39,"duration":5.49},{"text":"mean predicted vector use using this","start":627.12,"duration":4.74},{"text":"agreement measure we can update the","start":629.88,"duration":3.99},{"text":"weights of every predicted vector","start":631.86,"duration":3.53},{"text":"accordingly","start":633.87,"duration":4.17},{"text":"note that the predicted vectors that are","start":635.39,"duration":5.47},{"text":"far from the mean now have a very small","start":638.04,"duration":5.73},{"text":"weight and the ones closest to the mean","start":640.86,"duration":4.86},{"text":"have a much stronger weight I've","start":643.77,"duration":4.16},{"text":"represent them represented them in black","start":645.72,"duration":5.19},{"text":"now we can compute the mean once again","start":647.93,"duration":6.31},{"text":"or I should say the weighted mean and","start":650.91,"duration":4.95},{"text":"notice that it moves slightly towards","start":654.24,"duration":4.65},{"text":"the cluster towards the center of the","start":655.86,"duration":5.43},{"text":"cluster so next we can once again update","start":658.89,"duration":7.5},{"text":"the weights and now most of the vectors","start":661.29,"duration":8.06},{"text":"within the cluster have turned black and","start":666.39,"duration":6.33},{"text":"again we can update the mean and we can","start":669.35,"duration":5.02},{"text":"repeat this process a few times in","start":672.72,"duration":3.6},{"text":"practice you know three to five","start":674.37,"duration":5.04},{"text":"iterations are generally sufficient this","start":676.32,"duration":5.22},{"text":"might remind you I suppose of the","start":679.41,"duration":4.08},{"text":"k-means clustering algorithm if you know","start":681.54,"duration":5.04},{"text":"it okay so this is how we find clusters","start":683.49,"duration":5.789},{"text":"of agreement now let's see how the whole","start":686.58,"duration":6.59},{"text":"algorithm works in a bit more details","start":689.279,"duration":7.291},{"text":"first for every predicted output we","start":693.17,"duration":5.68},{"text":"start by setting a raw routing weight","start":696.57,"duration":7.709},{"text":"bij equal to zero next we apply the","start":698.85,"duration":8.489},{"text":"softmax function softmax function to","start":704.279,"duration":4.891},{"text":"these raw weights for each primary","start":707.339,"duration":4.891},{"text":"capsule this gives the actual routing","start":709.17,"duration":4.979},{"text":"weights for each predicted output in","start":712.23,"duration":7.83},{"text":"this example 0.5 each equal weights next","start":714.149,"duration":7.68},{"text":"we compute a weighted sum of the","start":720.06,"duration":4.769},{"text":"predictions for each capsule in the next","start":721.829,"duration":6.331},{"text":"layer this might give vectors longer","start":724.829,"duration":6.151},{"text":"than one so as usual we apply the squash","start":728.16,"duration":8.16},{"text":"function and voila we now have the","start":730.98,"duration":7.83},{"text":"actual outputs of the house capsule and","start":736.32,"duration":4.95},{"text":"boat capsule but this is not the final","start":738.81,"duration":4.17},{"text":"output is just the end of the first","start":741.27,"duration":5.7},{"text":"round the first iteration now we can see","start":742.98,"duration":6.39},{"text":"which predictions were most accurate for","start":746.97,"duration":4.89},{"text":"example the rectangle capsule made a","start":749.37,"duration":4.77},{"text":"great prediction for the boat capsules","start":751.86,"duration":4.26},{"text":"output it really matches it pretty","start":754.14,"duration":5.1},{"text":"closely this is estimated by computing","start":756.12,"duration":5.19},{"text":"the scalar product of the predicted","start":759.24,"duration":5.61},{"text":"output vector U hat j-jake I and the","start":761.31,"duration":6.87},{"text":"actual product vector V J this scalar","start":764.85,"duration":5.76},{"text":"product is simply added to the predicted","start":768.18,"duration":8.37},{"text":"outputs raw routing weight bij so the","start":770.61,"duration":8.43},{"text":"weight of this particular predicted","start":776.55,"duration":6.21},{"text":"output is increased when there is a","start":779.04,"duration":5.82},{"text":"strong agreement the scalar product is","start":782.76,"duration":4.17},{"text":"going to be large so good predictions","start":784.86,"duration":2.66},{"text":"will","start":786.93,"duration":3.23},{"text":"have a higher weight on the other hand","start":787.52,"duration":5.1},{"text":"the rectangle capsule made a pretty bad","start":790.16,"duration":4.83},{"text":"prediction for the house capsules output","start":792.62,"duration":5.25},{"text":"so the scalar product in this case will","start":794.99,"duration":5.43},{"text":"be quite small and the raw routing","start":797.87,"duration":4.59},{"text":"weights of these predicted vector will","start":800.42,"duration":5.34},{"text":"not grow much next we update the routing","start":802.46,"duration":5.88},{"text":"weights by computing the softmax of the","start":805.76,"duration":5.07},{"text":"raw weights once again and as you can","start":808.34,"duration":4.65},{"text":"see the rectangle capsules predicted","start":810.83,"duration":4.41},{"text":"vector for the boat capsule now has a","start":812.99,"duration":5.22},{"text":"weight of 0.8 while it's predicted","start":815.24,"duration":4.83},{"text":"vector for the house capsule dropped","start":818.21,"duration":6.81},{"text":"down to 0.2 so most of its output is not","start":820.07,"duration":7.26},{"text":"going to go to the boat capsule not the","start":825.02,"duration":5.58},{"text":"house capsule once again we compute the","start":827.33,"duration":5.13},{"text":"weighted sum of all the predicted","start":830.6,"duration":3.9},{"text":"outputs vectors for each capsule in the","start":832.46,"duration":4.53},{"text":"next layer that is the house capsule in","start":834.5,"duration":4.56},{"text":"the boat capsule and this time the house","start":836.99,"duration":4.17},{"text":"capsule gets so little input that its","start":839.06,"duration":4.83},{"text":"output is a tiny vector on the other","start":841.16,"duration":4.68},{"text":"hand the boat capsule gets so much input","start":843.89,"duration":4.65},{"text":"that its output that it outputs a vector","start":845.84,"duration":5.16},{"text":"much longer than 1 so again we squash it","start":848.54,"duration":5.97},{"text":"and that's the end of round 2 and as you","start":851.0,"duration":5.43},{"text":"can see in just a couple iterations we","start":854.51,"duration":3.99},{"text":"have already ruled out the house and","start":856.43,"duration":4.8},{"text":"clearly chosen the boat after perhaps","start":858.5,"duration":5.85},{"text":"one or two more rounds we can stop and","start":861.23,"duration":4.89},{"text":"proceed to the next capsule layer in","start":864.35,"duration":5.85},{"text":"exactly the same way so as I mentioned","start":866.12,"duration":5.94},{"text":"earlier routing by agreement is really","start":870.2,"duration":4.2},{"text":"great to handle crowded scenes such as","start":872.06,"duration":6.12},{"text":"the one represented in this image one","start":874.4,"duration":5.55},{"text":"way to interpret this image as you can","start":878.18,"duration":3.0},{"text":"see there's a little bit of ambiguity","start":879.95,"duration":4.32},{"text":"you can see a house upside down in the","start":881.18,"duration":6.0},{"text":"middle however if this was the case then","start":884.27,"duration":5.07},{"text":"there would be no explanation for the","start":887.18,"duration":5.46},{"text":"bottom rectangle or the top triangle no","start":889.34,"duration":6.36},{"text":"reason for them to be where they are the","start":892.64,"duration":5.07},{"text":"best way to interpret the image is that","start":895.7,"duration":4.74},{"text":"there is a house at the top and a boat","start":897.71,"duration":5.19},{"text":"at the bottom and routing by agreement","start":900.44,"duration":4.71},{"text":"will tend to choose this solution since","start":902.9,"duration":4.11},{"text":"it makes all the capsules perfectly","start":905.15,"duration":3.63},{"text":"happy each of them making perfect","start":907.01,"duration":3.75},{"text":"predictions for the capsules in the next","start":908.78,"duration":6.54},{"text":"layer the ambiguity is explained away ok","start":910.76,"duration":7.08},{"text":"so what can you do with a capsule","start":915.32,"duration":3.68},{"text":"network now that you know","start":917.84,"duration":3.5},{"text":"how it works well for one you can create","start":919.0,"duration":5.73},{"text":"a nice image classifier of course just","start":921.34,"duration":5.55},{"text":"have one capsule per class in the top","start":924.73,"duration":4.71},{"text":"capsule layer and that's almost all","start":926.89,"duration":4.71},{"text":"there is to it all you need to add is a","start":929.44,"duration":4.5},{"text":"layer that computes the length of the","start":931.6,"duration":4.59},{"text":"top layer activation vectors and this","start":933.94,"duration":3.51},{"text":"gives you the estimated class","start":936.19,"duration":4.23},{"text":"probabilities you could then just train","start":937.45,"duration":4.59},{"text":"the network by minimizing the cross","start":940.42,"duration":3.69},{"text":"entropy loss has an irregular","start":942.04,"duration":4.29},{"text":"classification neural network and you","start":944.11,"duration":4.86},{"text":"know you'd be done however in the paper","start":946.33,"duration":4.59},{"text":"they use a margin loss that makes it","start":948.97,"duration":5.22},{"text":"possible to detect multiple classes in","start":950.92,"duration":6.51},{"text":"the image so without going into too much","start":954.19,"duration":6.03},{"text":"detail this margin loss is such that if","start":957.43,"duration":5.58},{"text":"an object of class K is present in the","start":960.22,"duration":5.31},{"text":"image then the corresponding top-level","start":963.01,"duration":4.92},{"text":"capsule should help put a vector whose","start":965.53,"duration":5.79},{"text":"squared length is at least 0.9 it should","start":967.93,"duration":6.06},{"text":"be long conversely if an object of class","start":971.32,"duration":5.31},{"text":"K is not present in the image then the","start":973.99,"duration":4.95},{"text":"capsule should output a short vector one","start":976.63,"duration":4.76},{"text":"whose squared length is shorter than 0.1","start":978.94,"duration":5.7},{"text":"so the total loss is the sum of losses","start":981.39,"duration":6.88},{"text":"for all classes in the paper they also","start":984.64,"duration":5.97},{"text":"add a decoder network on top of the","start":988.27,"duration":4.65},{"text":"capsule network it's just a three you","start":990.61,"duration":3.96},{"text":"know three connected layers fully","start":992.92,"duration":3.84},{"text":"connected with a sigmoid activation","start":994.57,"duration":4.65},{"text":"function in the output layer and it","start":996.76,"duration":5.19},{"text":"learns to reconstruct the input image by","start":999.22,"duration":4.44},{"text":"minimizing the squared difference","start":1001.95,"duration":4.47},{"text":"between the reconstructed image and the","start":1003.66,"duration":7.35},{"text":"input image the full loss is margin loss","start":1006.42,"duration":6.6},{"text":"we discussed earlier plus the","start":1011.01,"duration":3.87},{"text":"reconstruction loss scaled down","start":1013.02,"duration":4.2},{"text":"considerably so as to ensure that the","start":1014.88,"duration":4.89},{"text":"margin loss dominates training the","start":1017.22,"duration":4.47},{"text":"benefit of applying this reconstruction","start":1019.77,"duration":4.02},{"text":"loss is that it forces the network to","start":1021.69,"duration":4.35},{"text":"preserve all the information required to","start":1023.79,"duration":5.01},{"text":"construct to reconstruct the image up to","start":1026.04,"duration":5.73},{"text":"the top layer of the capsule network its","start":1028.8,"duration":6.33},{"text":"output layer this constraint acts a bit","start":1031.77,"duration":5.939},{"text":"like a regularizer it reduces the risk","start":1035.13,"duration":5.88},{"text":"of overfitting and helps generalize to","start":1037.709,"duration":7.23},{"text":"new examples and that's it you know how","start":1041.01,"duration":5.079},{"text":"a","start":1044.939,"duration":3.34},{"text":"capsule Network works and how to train","start":1046.089,"duration":4.111},{"text":"it let's look a little bit at some of","start":1048.279,"duration":3.63},{"text":"the figures in the paper which I find","start":1050.2,"duration":3.66},{"text":"interesting so this is figure one","start":1051.909,"duration":5.01},{"text":"showing a full capsule network for mmm","start":1053.86,"duration":5.939},{"text":"mist you can see the the first two","start":1056.919,"duration":4.951},{"text":"regular convolutional layers whose","start":1059.799,"duration":4.561},{"text":"output is reshaped and squashed to get","start":1061.87,"duration":4.77},{"text":"the activation vectors of the primary","start":1064.36,"duration":5.37},{"text":"capsules and these primary capsules are","start":1066.64,"duration":6.45},{"text":"organized in a six by six grid with 32","start":1069.73,"duration":5.91},{"text":"primary capsules in each cell of this","start":1073.09,"duration":5.73},{"text":"grid and each primary capsule outputs an","start":1075.64,"duration":6.36},{"text":"eight dimensional vector so this first","start":1078.82,"duration":6.03},{"text":"layer of capsules is fully connected to","start":1082.0,"duration":5.46},{"text":"the ten output capsules which output","start":1084.85,"duration":5.699},{"text":"sixteen dimensional lectures the length","start":1087.46,"duration":4.74},{"text":"of these vectors is yoots","start":1090.549,"duration":3.931},{"text":"is used to compute the margin loss as","start":1092.2,"duration":5.13},{"text":"explained earlier now this is figure two","start":1094.48,"duration":3.809},{"text":"from the paper","start":1097.33,"duration":3.36},{"text":"it shows the decoder sitting on top of","start":1098.289,"duration":4.62},{"text":"the caps net it is composed of two fully","start":1100.69,"duration":4.38},{"text":"connected relu layers plus a fully","start":1102.909,"duration":4.88},{"text":"connected sigmoid layer which outputs","start":1105.07,"duration":5.76},{"text":"784 numbers that correspond to the pixel","start":1107.789,"duration":5.441},{"text":"intensities of the richens reconstructed","start":1110.83,"duration":6.209},{"text":"image which is 28 by 28 pixel the","start":1113.23,"duration":5.28},{"text":"squared difference between this","start":1117.039,"duration":4.26},{"text":"reconstructed image and the input image","start":1118.51,"duration":6.33},{"text":"gives the reconstruction loss right and","start":1121.299,"duration":6.0},{"text":"this is figure four from the paper also","start":1124.84,"duration":4.29},{"text":"interesting one nice thing about capsule","start":1127.299,"duration":4.141},{"text":"networks is that the activation vectors","start":1129.13,"duration":4.26},{"text":"are often quite interpretable","start":1131.44,"duration":4.07},{"text":"for example this image shows the","start":1133.39,"duration":4.169},{"text":"reconstructions that you get when you","start":1135.51,"duration":4.029},{"text":"gradually modify one of the 16","start":1137.559,"duration":3.84},{"text":"dimensions of the top layer capsules","start":1139.539,"duration":4.411},{"text":"output you can see that the first","start":1141.399,"duration":4.591},{"text":"dimension seems to represent you know","start":1143.95,"duration":5.87},{"text":"scale and thickness the fourth dimension","start":1145.99,"duration":7.02},{"text":"represents a localized skew if you look","start":1149.82,"duration":6.28},{"text":"at how it that number 4 is modified from","start":1153.01,"duration":4.59},{"text":"the left to the right the fifth","start":1156.1,"duration":3.84},{"text":"represents the width of the digit plus a","start":1157.6,"duration":4.35},{"text":"slight translation to get the exact","start":1159.94,"duration":3.719},{"text":"position so as you can see it's rather","start":1161.95,"duration":5.24},{"text":"clear what most of these parameters do","start":1163.659,"duration":6.031},{"text":"ok to conclude let's summarize the pros","start":1167.19,"duration":4.209},{"text":"and cons capsule networks have reached","start":1169.69,"duration":4.109},{"text":"state-of-the-art accuracy on a missed on","start":1171.399,"duration":4.951},{"text":"so far 10 they they got a bit over 10","start":1173.799,"duration":4.141},{"text":"percent error which is far from a","start":1176.35,"duration":2.88},{"text":"state-of-the-art but it's","start":1177.94,"duration":3.81},{"text":"- what was first obtained with other","start":1179.23,"duration":4.5},{"text":"techniques before you know years of","start":1181.75,"duration":3.87},{"text":"efforts were put into them so it's still","start":1183.73,"duration":2.569},{"text":"a good start","start":1185.62,"duration":3.36},{"text":"capsule networks require less training","start":1186.299,"duration":5.201},{"text":"data they offer equivariance which means","start":1188.98,"duration":4.199},{"text":"that position and pose information are","start":1191.5,"duration":4.32},{"text":"preserved and this is very promising for","start":1193.179,"duration":4.671},{"text":"image segmentation and object detection","start":1195.82,"duration":4.83},{"text":"the routing by agreement algorithm is","start":1197.85,"duration":5.079},{"text":"great for crowded scenes the routing","start":1200.65,"duration":4.38},{"text":"tree also maps the hierarchy of objects","start":1202.929,"duration":4.411},{"text":"parts so every part is associated to a","start":1205.03,"duration":4.8},{"text":"whole and it's rather robust to","start":1207.34,"duration":5.339},{"text":"rotations translations and other FIM","start":1209.83,"duration":5.88},{"text":"transformations the activation vectors","start":1212.679,"duration":5.551},{"text":"are somewhat interpretable and finally","start":1215.71,"duration":4.95},{"text":"obviously its sentence idea so don't bet","start":1218.23,"duration":5.37},{"text":"against it however there are a few cons","start":1220.66,"duration":5.73},{"text":"first as I mentioned the results are not","start":1223.6,"duration":5.22},{"text":"yet state-of-the-art on cipher 10 even","start":1226.39,"duration":4.02},{"text":"though it's a good start plus it's still","start":1228.82,"duration":3.39},{"text":"unclear whether capsule networks can","start":1230.41,"duration":4.35},{"text":"scale to larger images such as the image","start":1232.21,"duration":4.62},{"text":"net data set you know what will the","start":1234.76,"duration":5.22},{"text":"accuracy be capsule networks are also","start":1236.83,"duration":5.61},{"text":"quite slow to Train in large part","start":1239.98,"duration":4.439},{"text":"because of the routing by agreement","start":1242.44,"duration":4.56},{"text":"algorithm which has an inner loop as you","start":1244.419,"duration":3.331},{"text":"saw earlier","start":1247.0,"duration":3.78},{"text":"finally there is only one capsule of any","start":1247.75,"duration":4.86},{"text":"type in a given location so it's","start":1250.78,"duration":3.54},{"text":"impossible for a capsule network to","start":1252.61,"duration":3.87},{"text":"detect two objects of the same type if","start":1254.32,"duration":4.859},{"text":"they are too close to one another this","start":1256.48,"duration":4.8},{"text":"is called crowding and it's been","start":1259.179,"duration":4.021},{"text":"observed in human vision as well so it's","start":1261.28,"duration":5.49},{"text":"probably not a showstopper alright I","start":1263.2,"duration":5.49},{"text":"highly recommend you take a look at the","start":1266.77,"duration":4.11},{"text":"code of a caps net implementation such","start":1268.69,"duration":4.59},{"text":"as the ones listed here I'll leave the","start":1270.88,"duration":4.169},{"text":"links in the video description below if","start":1273.28,"duration":3.63},{"text":"you take your time you should have no","start":1275.049,"duration":3.271},{"text":"problem understanding everything the","start":1276.91,"duration":3.45},{"text":"code is doing the main difficulty in","start":1278.32,"duration":3.989},{"text":"implementing caps nets is that it","start":1280.36,"duration":3.93},{"text":"contains an inner loop for the routing","start":1282.309,"duration":4.321},{"text":"by agreement algorithm implementing","start":1284.29,"duration":3.9},{"text":"loops in Kerris and tensorflow","start":1286.63,"duration":3.78},{"text":"can be a little bit trickier than in pi","start":1288.19,"duration":5.46},{"text":"torch but it can be done so if you don't","start":1290.41,"duration":4.8},{"text":"have a particular preference then I","start":1293.65,"duration":3.12},{"text":"would say that the PI torch code is","start":1295.21,"duration":4.56},{"text":"probably the easiest to understand and","start":1296.77,"duration":5.01},{"text":"that's all I had I hope you enjoyed this","start":1299.77,"duration":4.08},{"text":"video if you did please thumbs up share","start":1301.78,"duration":4.38},{"text":"comment subscribe blah blah blah it's my","start":1303.85,"duration":4.709},{"text":"first real YouTube video and if people","start":1306.16,"duration":4.11},{"text":"find it useful I'm","start":1308.559,"duration":4.021},{"text":"make some more if you want to learn more","start":1310.27,"duration":4.44},{"text":"about machine learning deep learning and","start":1312.58,"duration":4.02},{"text":"deep reinforcement learning you may want","start":1314.71,"duration":4.08},{"text":"to read my Holly book hands on machine","start":1316.6,"duration":3.63},{"text":"learning with scikit-learn and","start":1318.79,"duration":4.41},{"text":"tensorflow it covers a ton of topics","start":1320.23,"duration":4.77},{"text":"with many code examples that you will","start":1323.2,"duration":4.44},{"text":"find on my github account so I'll leave","start":1325.0,"duration":4.55},{"text":"the links in the video description","start":1327.64,"duration":4.95},{"text":"that's all for today have fun and see","start":1329.55,"duration":5.49},{"text":"you next time","start":1332.59,"duration":2.45}],"2Kawrd5szHE":[{"text":"hi I'm over LaVon and today I'm going to","start":0.35,"duration":5.11},{"text":"show you how to implement a capsule","start":3.72,"duration":4.23},{"text":"network using tensor flow in my previous","start":5.46,"duration":4.41},{"text":"video I presented the key ideas behind","start":7.95,"duration":4.08},{"text":"capsule networks a recently published","start":9.87,"duration":4.41},{"text":"neural net architecture if you haven't","start":12.03,"duration":4.29},{"text":"seen this video I encourage you to do so","start":14.28,"duration":4.62},{"text":"now today I will focus on the tensor","start":16.32,"duration":3.75},{"text":"flow implementation","start":18.9,"duration":3.99},{"text":"I wrote a jupiter notebook containing","start":20.07,"duration":4.83},{"text":"all the code and detailed explanations","start":22.89,"duration":4.02},{"text":"and i published it on my github account","start":24.9,"duration":4.949},{"text":"as always I'll put all the links in the","start":26.91,"duration":4.98},{"text":"video description below so I encourage","start":29.849,"duration":5.071},{"text":"you to clone it and play with it so it","start":31.89,"duration":4.65},{"text":"reaches over ninety-nine point four","start":34.92,"duration":3.63},{"text":"accuracy on the test set which is pretty","start":36.54,"duration":4.44},{"text":"good considering it's a shallow network","start":38.55,"duration":4.86},{"text":"with just two capsule layers and a total","start":40.98,"duration":5.94},{"text":"of about 1200 capsules there's a lot of","start":43.41,"duration":5.85},{"text":"code in the in this notebook so I won't","start":46.92,"duration":4.02},{"text":"go through every single line in this","start":49.26,"duration":3.299},{"text":"video but I'll explain the main","start":50.94,"duration":3.599},{"text":"difficulties I came across and hopefully","start":52.559,"duration":3.511},{"text":"this will be useful to you for other","start":54.539,"duration":3.901},{"text":"tensor flow implementations not just","start":56.07,"duration":4.86},{"text":"caps nets okay let's build a network","start":58.44,"duration":5.16},{"text":"first we need to feed the input images","start":60.93,"duration":5.34},{"text":"to the network and that's our input","start":63.6,"duration":5.49},{"text":"layer we implement it using a simple","start":66.27,"duration":5.34},{"text":"tensor flow placeholder the batch size","start":69.09,"duration":5.04},{"text":"is unspecified so that we can pass any","start":71.61,"duration":5.13},{"text":"number of images in each batch in this","start":74.13,"duration":5.97},{"text":"example 5 note that we directly send","start":76.74,"duration":6.15},{"text":"Tony 8 by 28 pixel images with a single","start":80.1,"duration":4.71},{"text":"channel since the images are grayscale","start":82.89,"duration":3.99},{"text":"color images would typically have 3","start":84.81,"duration":4.11},{"text":"channels for red green and blue and","start":86.88,"duration":5.4},{"text":"that's it for the input layer next let's","start":88.92,"duration":6.12},{"text":"build the primary capsule layer for each","start":92.28,"duration":5.97},{"text":"digit in the batch it will output 32","start":95.04,"duration":6.36},{"text":"maps each containing a 6 by 6 grid of 8","start":98.25,"duration":6.299},{"text":"dimensional vectors the capsules in this","start":101.4,"duration":5.34},{"text":"particular map seem to detect the start","start":104.549,"duration":4.801},{"text":"of a line segment that you can see that","start":106.74,"duration":4.559},{"text":"the output vectors are long in the","start":109.35,"duration":3.629},{"text":"locations where there's a start of a","start":111.299,"duration":4.5},{"text":"line and the orientation of the ad","start":112.979,"duration":5.731},{"text":"vector gives the pose parameters in this","start":115.799,"duration":5.701},{"text":"case I've represented the rotation angle","start":118.71,"duration":4.47},{"text":"but the vectors a dimensional","start":121.5,"duration":3.78},{"text":"orientation would also capture things","start":123.18,"duration":4.259},{"text":"like the thickness of the line the","start":125.28,"duration":4.199},{"text":"precise location of the start of the","start":127.439,"duration":4.981},{"text":"line relative to the cell in the 6 by 6","start":129.479,"duration":4.161},{"text":"grid and so","start":132.42,"duration":3.45},{"text":"the implementation is really","start":133.64,"duration":2.95},{"text":"straightforward","start":135.87,"duration":2.61},{"text":"first we define two regular","start":136.59,"duration":4.44},{"text":"convolutional layers the input of the","start":138.48,"duration":5.28},{"text":"first layer is X the placeholder that","start":141.03,"duration":4.95},{"text":"will contain the input images we will","start":143.76,"duration":4.77},{"text":"feed at runtime the second layer takes","start":145.98,"duration":5.07},{"text":"the output of the first layer of course","start":148.53,"duration":5.19},{"text":"and we use the parameters specified in","start":151.05,"duration":5.46},{"text":"the paper the second layer is configured","start":153.72,"duration":6.45},{"text":"to output 256 feature maps each feature","start":156.51,"duration":7.29},{"text":"map contains 6x6 grid of scalars we want","start":160.17,"duration":6.99},{"text":"a 6x6 a grid of vectors instead so we","start":163.8,"duration":6.0},{"text":"use tensor flows reshape function to get","start":167.16,"duration":5.07},{"text":"32 Maps of eight dimensional vectors","start":169.8,"duration":6.06},{"text":"instead of 256 maps of scalars in fact","start":172.23,"duration":5.759},{"text":"since the primary capsules will be fully","start":175.86,"duration":4.53},{"text":"connected to the digit capsules we can","start":177.989,"duration":7.53},{"text":"simply reshape to one long list of 1152","start":180.39,"duration":7.56},{"text":"output vectors that's a thirty-two times","start":185.519,"duration":4.8},{"text":"six times six for each instance in the","start":187.95,"duration":5.61},{"text":"batch and the last step is to squash the","start":190.319,"duration":5.041},{"text":"vectors to ensure that their length is","start":193.56,"duration":5.13},{"text":"always between zero and one for this we","start":195.36,"duration":6.39},{"text":"use a homemade squash function here it","start":198.69,"duration":5.34},{"text":"is this function implements the squash","start":201.75,"duration":5.73},{"text":"equation given in the paper it squashes","start":204.03,"duration":5.28},{"text":"every vector in an array along the","start":207.48,"duration":4.259},{"text":"specified dimension by default the last","start":209.31,"duration":5.25},{"text":"one so as you can see it involves a","start":211.739,"duration":5.25},{"text":"division by the norm of the vector so","start":214.56,"duration":4.59},{"text":"there's a risk of a division by zero if","start":216.989,"duration":4.201},{"text":"at least one of the vectors is a zero","start":219.15,"duration":4.8},{"text":"vector so you could just add a tiny","start":221.19,"duration":5.19},{"text":"epsilon value in the denominator and it","start":223.95,"duration":4.5},{"text":"would fix the division by zero problem","start":226.38,"duration":4.77},{"text":"however you would still run into another","start":228.45,"duration":5.64},{"text":"issue the norm of a vector has no","start":231.15,"duration":5.339},{"text":"defined gradients when the vector is","start":234.09,"duration":4.979},{"text":"zero so if you just use tensor flows","start":236.489,"duration":4.711},{"text":"norm function to compute the norm in","start":239.069,"duration":4.621},{"text":"this equation then if at least one of","start":241.2,"duration":4.74},{"text":"the vectors is zero the gradients will","start":243.69,"duration":5.46},{"text":"be undefined it will return n a n and","start":245.94,"duration":4.05},{"text":"not","start":249.15,"duration":3.54},{"text":"so as a result when greed in descent","start":249.99,"duration":5.01},{"text":"updates the weights of our model the","start":252.69,"duration":4.8},{"text":"weights will end up being undefined as","start":255.0,"duration":4.89},{"text":"well the model would effectively be dead","start":257.49,"duration":5.07},{"text":"then you don't want that so the trick is","start":259.89,"duration":5.07},{"text":"to compute a safe approximation of the","start":262.56,"duration":4.97},{"text":"norm shown in the equation on the right","start":264.96,"duration":6.09},{"text":"and that's about it that's all for the","start":267.53,"duration":5.98},{"text":"primary capsules apart for computing the","start":271.05,"duration":4.89},{"text":"norm safely it was pretty straight","start":273.51,"duration":5.61},{"text":"forward on to the next layer where all","start":275.94,"duration":5.58},{"text":"the complexity is the digit capsules","start":279.12,"duration":5.64},{"text":"there are just ten of them one for each","start":281.52,"duration":6.27},{"text":"digits 0 to 9 and the output 16","start":284.76,"duration":5.13},{"text":"dimensional vectors in this particular","start":287.79,"duration":3.99},{"text":"example you can see that the longest","start":289.89,"duration":5.1},{"text":"output vector is the one for digit 4 and","start":291.78,"duration":6.21},{"text":"again it's orientation in the 16","start":294.99,"duration":4.8},{"text":"dimensional space gives information","start":297.99,"duration":3.69},{"text":"about the pose of this digit its","start":299.79,"duration":3.99},{"text":"rotation its thickness its Q its","start":301.68,"duration":5.01},{"text":"position and so on by the way note that","start":303.78,"duration":5.43},{"text":"most of the position information in the","start":306.69,"duration":5.22},{"text":"first layer was encoded in the location","start":309.21,"duration":5.37},{"text":"of the active capsules in the 6x6 grid","start":311.91,"duration":5.82},{"text":"so for example if I shift the digit 4","start":314.58,"duration":6.08},{"text":"slightly to the left in the input image","start":317.73,"duration":5.52},{"text":"they then different capsules in the","start":320.66,"duration":6.49},{"text":"first layer get activated see so the","start":323.25,"duration":5.88},{"text":"output of these first layer capsules","start":327.15,"duration":4.32},{"text":"only contains a local shift information","start":329.13,"duration":4.17},{"text":"relative to the position of the capsule","start":331.47,"duration":5.04},{"text":"in the 6x6 grid but in the second","start":333.3,"duration":5.49},{"text":"capsule layer the full position","start":336.51,"duration":4.2},{"text":"information is now encoded in the","start":338.79,"duration":4.95},{"text":"orientation of the output vector in 16","start":340.71,"duration":6.39},{"text":"dimensional space ok now let's see how","start":343.74,"duration":5.61},{"text":"to implement this layer the first step","start":347.1,"duration":3.87},{"text":"is to compute the predicted output","start":349.35,"duration":5.13},{"text":"vectors since this second layer is fully","start":350.97,"duration":5.55},{"text":"connected to the first layer we will","start":354.48,"duration":4.38},{"text":"compute one predicted output for each","start":356.52,"duration":4.77},{"text":"pair of first and second layer capsules","start":358.86,"duration":5.19},{"text":"for example using the output of the","start":361.29,"duration":5.25},{"text":"first primary capsule we can predict the","start":364.05,"duration":5.07},{"text":"output vector of the first digit capsule","start":366.54,"duration":5.19},{"text":"for this we just use a transformation","start":369.12,"duration":6.21},{"text":"matrix w11 which will gradually be","start":371.73,"duration":5.85},{"text":"learned during training and we multiply","start":375.33,"duration":4.41},{"text":"it by the output of the first layer","start":377.58,"duration":6.0},{"text":"capsule this gives us u hat 1 1","start":379.74,"duration":5.76},{"text":"which is the predicted output of the","start":383.58,"duration":4.32},{"text":"first digit capsule based on the output","start":385.5,"duration":5.159},{"text":"of the first primary capsule since the","start":387.9,"duration":4.5},{"text":"primary capsules output eight","start":390.659,"duration":3.841},{"text":"dimensional vectors and the digit","start":392.4,"duration":4.139},{"text":"capsules output sixteen dimensional","start":394.5,"duration":5.099},{"text":"vectors the transformation matrix w11","start":396.539,"duration":6.711},{"text":"must be a sixteen by eight matrix","start":399.599,"duration":6.481},{"text":"next we try to predict the output of the","start":403.25,"duration":5.59},{"text":"second digit capsule still based on the","start":406.08,"duration":5.549},{"text":"output of the first primary capsule note","start":408.84,"duration":4.229},{"text":"that we are using a different","start":411.629,"duration":6.03},{"text":"transformation matrix w12 and we do the","start":413.069,"duration":7.32},{"text":"same for the third capsule using W one","start":417.659,"duration":5.521},{"text":"three and so on for all the digit","start":420.389,"duration":5.851},{"text":"capsules then we move on to the second","start":423.18,"duration":5.82},{"text":"primary capsule and we use its output to","start":426.24,"duration":4.769},{"text":"predict the output of the first digit","start":429.0,"duration":4.83},{"text":"capsule and so on for all the digit","start":431.009,"duration":5.791},{"text":"capsules then we move on to the third","start":433.83,"duration":4.98},{"text":"primary capsule we make ten predictions","start":436.8,"duration":6.44},{"text":"and so on you get the picture there are","start":438.81,"duration":7.53},{"text":"1152 primary capsules multiply six by","start":443.24,"duration":7.899},{"text":"six by 32 and ten digit capsules so we","start":446.34,"duration":6.75},{"text":"end up with eleven thousand five hundred","start":451.139,"duration":4.59},{"text":"and twenty predicted output vectors now","start":453.09,"duration":4.289},{"text":"we could just compute them one by one","start":455.729,"duration":4.41},{"text":"but it would be terribly inefficient so","start":457.379,"duration":4.5},{"text":"let's see how we can get all the","start":460.139,"duration":3.75},{"text":"predicted output vectors in just one","start":461.879,"duration":5.13},{"text":"matte small operation now you know that","start":463.889,"duration":5.49},{"text":"tensorflow mat small function lets you","start":467.009,"duration":4.921},{"text":"multiply two matrices but you may not","start":469.379,"duration":3.96},{"text":"know that you can also use it to","start":471.93,"duration":4.44},{"text":"multiply many matrices in one shot this","start":473.339,"duration":4.561},{"text":"will be incredibly efficient especially","start":476.37,"duration":4.949},{"text":"if you are using a GPU card because it","start":477.9,"duration":4.859},{"text":"will perform all the matrix","start":481.319,"duration":3.541},{"text":"multiplications in parallel in many","start":482.759,"duration":4.801},{"text":"different GPU threads so here's how it","start":484.86,"duration":3.089},{"text":"works","start":487.56,"duration":4.56},{"text":"suppose a b c d e f and g h i j k l are","start":487.949,"duration":6.9},{"text":"all matrices you can put these matrices","start":492.12,"duration":5.88},{"text":"in two arrays each with two rows and","start":494.849,"duration":5.701},{"text":"three columns for example so we have two","start":498.0,"duration":4.83},{"text":"dimensions for this 2x3 grid of matrices","start":500.55,"duration":5.459},{"text":"and each matrix is two-dimensional","start":502.83,"duration":5.73},{"text":"so these arrays are two plus two equals","start":506.009,"duration":5.34},{"text":"four dimensional arrays if you pass","start":508.56,"duration":4.889},{"text":"these arrays these four d arrays two","start":511.349,"duration":4.62},{"text":"matmo it will perform an element-wise","start":513.449,"duration":3.421},{"text":"matrix","start":515.969,"duration":4.07},{"text":"application so the result will be this","start":516.87,"duration":8.1},{"text":"four dimensional array containing a x G","start":520.039,"duration":9.101},{"text":"here and B multiplied by H here and so","start":524.97,"duration":7.35},{"text":"on so let's use this to compute all the","start":529.14,"duration":6.3},{"text":"predicted output vectors we can create","start":532.32,"duration":5.58},{"text":"first 4d array containing all the","start":535.44,"duration":5.64},{"text":"transformation matrices there's one row","start":537.9,"duration":6.03},{"text":"per primary capsule and one column per","start":541.08,"duration":6.21},{"text":"digit capsule the second array must","start":543.93,"duration":5.34},{"text":"contain the output vectors of each","start":547.29,"duration":5.549},{"text":"primary capsule then we just pass these","start":549.27,"duration":6.63},{"text":"two arrays to the mole function and it","start":552.839,"duration":4.801},{"text":"gives us the predicted output vectors","start":555.9,"duration":4.35},{"text":"for all the pairs of primary and digit","start":557.64,"duration":6.27},{"text":"capsules since we need to predict the","start":560.25,"duration":6.3},{"text":"outputs of all ten-digit capsules for","start":563.91,"duration":5.76},{"text":"each primary capsule this array must","start":566.55,"duration":5.31},{"text":"contain ten copies of the primary","start":569.67,"duration":5.1},{"text":"capsules outputs we will use the tile","start":571.86,"duration":5.25},{"text":"function to replicate the first column","start":574.77,"duration":7.11},{"text":"of output vectors ten times but there's","start":577.11,"duration":6.99},{"text":"one additional catch we want to make","start":581.88,"duration":4.44},{"text":"these predictions for all the instances","start":584.1,"duration":4.8},{"text":"in the batch not just one instance so","start":586.32,"duration":4.56},{"text":"there's an additional dimension for the","start":588.9,"duration":4.59},{"text":"batch size it turns out that the primary","start":590.88,"duration":5.67},{"text":"output vectors were already computed for","start":593.49,"duration":4.53},{"text":"every single instance","start":596.55,"duration":3.6},{"text":"so the second array is fine it already","start":598.02,"duration":5.88},{"text":"has this dimension but we need to","start":600.15,"duration":6.059},{"text":"replicate the 4d array containing all","start":603.9,"duration":5.01},{"text":"the transformation matrices so that we","start":606.209,"duration":4.981},{"text":"end up with one copy per instance in the","start":608.91,"duration":5.369},{"text":"batch now if you understand this then","start":611.19,"duration":6.6},{"text":"the code should be pretty clear first we","start":614.279,"duration":5.401},{"text":"create a variable containing all the","start":617.79,"duration":4.83},{"text":"transformation matrices it has a one row","start":619.68,"duration":6.54},{"text":"per primary capsule one column per digit","start":622.62,"duration":7.11},{"text":"castle capsule and it contains 16 by 8","start":626.22,"duration":6.72},{"text":"matrices that's four dimensions and we","start":629.73,"duration":5.22},{"text":"add another dimension at the beginning","start":632.94,"duration":5.61},{"text":"of size one to make it easy to tile this","start":634.95,"duration":6.57},{"text":"array for each instance in the batch now","start":638.55,"duration":4.979},{"text":"the variable is initialized randomly","start":641.52,"duration":4.47},{"text":"using a normal distribution of standard","start":643.529,"duration":4.321},{"text":"deviation 0.01","start":645.99,"duration":4.039},{"text":"that's a hyper parameter you can tweet","start":647.85,"duration":5.089},{"text":"and that's about it we create this","start":650.029,"duration":6.511},{"text":"variable next we want to tile this array","start":652.939,"duration":6.45},{"text":"for each instance so first we need to","start":656.54,"duration":4.89},{"text":"know the batch size we don't actually","start":659.389,"duration":4.081},{"text":"know it at graph construction time it","start":661.43,"duration":3.959},{"text":"will only be known when we run the graph","start":663.47,"duration":4.65},{"text":"but we can use tensor flows shape","start":665.389,"duration":5.01},{"text":"function it creates a tensor that will","start":668.12,"duration":5.13},{"text":"know the shape at run time and we grab","start":670.399,"duration":5.011},{"text":"its first dimension which is the batch","start":673.25,"duration":6.18},{"text":"size then we simply tile our big W array","start":675.41,"duration":6.45},{"text":"along the first dimension to get one","start":679.43,"duration":6.029},{"text":"copy per instance now recall that the","start":681.86,"duration":5.849},{"text":"output of the primary capsules was a","start":685.459,"duration":4.081},{"text":"three dimensional array the first","start":687.709,"duration":4.32},{"text":"dimension is the batch size that we will","start":689.54,"duration":5.279},{"text":"know at runtime then there's one row per","start":692.029,"duration":5.341},{"text":"capsule and each capsule has eight","start":694.819,"duration":4.77},{"text":"dimensions so we need to reshape this","start":697.37,"duration":4.259},{"text":"array a bit to get the shape that we are","start":699.589,"duration":5.25},{"text":"looking for to to do the big mat Mulla","start":701.629,"duration":6.45},{"text":"operation first we add an extra","start":704.839,"duration":5.761},{"text":"dimension at the end using tensor flows","start":708.079,"duration":5.641},{"text":"expand dims function and vectors are now","start":710.6,"duration":6.06},{"text":"represented as column vectors instead of","start":713.72,"duration":4.919},{"text":"one dimensional array each of these is a","start":716.66,"duration":4.44},{"text":"column vector column vector is a matrix","start":718.639,"duration":6.3},{"text":"a 2d array with a single column then we","start":721.1,"duration":5.76},{"text":"add another dimension for the digit","start":724.939,"duration":5.731},{"text":"capsules and we replicate all the output","start":726.86,"duration":5.49},{"text":"vectors ten times across this new","start":730.67,"duration":5.839},{"text":"dimension once per digit capsule and","start":732.35,"duration":7.08},{"text":"lastly we just use map Moll to multiply","start":736.509,"duration":4.87},{"text":"the transformation matrices with the","start":739.43,"duration":4.889},{"text":"primary capsules output vectors and we","start":741.379,"duration":4.801},{"text":"get all the digit capsules predicted","start":744.319,"duration":4.14},{"text":"outputs for each pair of primary and","start":746.18,"duration":4.589},{"text":"digit capsules and for each instance in","start":748.459,"duration":5.73},{"text":"the batch in one shot and that's the end","start":750.769,"duration":5.25},{"text":"of the first step for computing the","start":754.189,"duration":4.801},{"text":"digit capsules outputs we now have a","start":756.019,"duration":5.49},{"text":"bunch of predicted output vectors the","start":758.99,"duration":4.889},{"text":"second step is the routing by agreement","start":761.509,"duration":5.43},{"text":"algorithm so first we set all the","start":763.879,"duration":5.4},{"text":"routing weights to 0 for this we just","start":766.939,"duration":5.851},{"text":"use TF zeros there is one weight for","start":769.279,"duration":5.85},{"text":"each pair of primary and digit capsules","start":772.79,"duration":5.099},{"text":"and for each instance the last two","start":775.129,"duration":5.611},{"text":"dimensions here are equal to 1 they will","start":777.889,"duration":6.271},{"text":"be useful in a minute next we compute","start":780.74,"duration":6.42},{"text":"the softmax of each primary capsules 10","start":784.16,"duration":4.529},{"text":"raw routing weights","start":787.16,"duration":4.38},{"text":"okay so softmax happens along this","start":788.689,"duration":7.14},{"text":"dimension next we compute the weighted","start":791.54,"duration":7.229},{"text":"sum of all the predicted output vectors","start":795.829,"duration":5.461},{"text":"for each digit capsule using the routing","start":798.769,"duration":4.231},{"text":"weights so the weighted sum is along","start":801.29,"duration":5.01},{"text":"this dimension this is pretty","start":803.0,"duration":5.25},{"text":"straightforward tensorflow code first","start":806.3,"duration":3.87},{"text":"multiply the routing weights and the","start":808.25,"duration":4.17},{"text":"predicted vectors this is a element-wise","start":810.17,"duration":4.169},{"text":"multiplication not a matrix","start":812.42,"duration":4.8},{"text":"multiplication then just compute the sum","start":814.339,"duration":5.011},{"text":"over the primary capsule dimension and","start":817.22,"duration":4.589},{"text":"the two dimensions we added earlier for","start":819.35,"duration":3.959},{"text":"the routing weights are useful in the","start":821.809,"duration":4.02},{"text":"multiplication step so that the two","start":823.309,"duration":3.811},{"text":"arrays have the same number of","start":825.829,"duration":4.531},{"text":"dimensions the same rank they don't have","start":827.12,"duration":5.43},{"text":"the exact same shape but they have","start":830.36,"duration":4.86},{"text":"compatible shapes so tensorflow will","start":832.55,"duration":5.07},{"text":"perform broadcasting now if you don't","start":835.22,"duration":4.739},{"text":"know what broadcasting is this should","start":837.62,"duration":5.009},{"text":"make it clear I'm multiplying two","start":839.959,"duration":4.531},{"text":"matrices but one of them just has one","start":842.629,"duration":4.71},{"text":"row so tensorflow will act as if this","start":844.49,"duration":5.159},{"text":"row were repeated the appropriate number","start":847.339,"duration":4.201},{"text":"of times you could achieve the same","start":849.649,"duration":4.41},{"text":"thing using tiling as we did earlier but","start":851.54,"duration":4.769},{"text":"this is more efficient and you may","start":854.059,"duration":4.5},{"text":"wonder why we didn't use broadcasting","start":856.309,"duration":4.14},{"text":"earlier but the reason is it does not","start":858.559,"duration":3.931},{"text":"work for matrix multiplication here","start":860.449,"duration":4.7},{"text":"we're doing element wise multiplication","start":862.49,"duration":5.789},{"text":"okay back to the digit capsules we","start":865.149,"duration":5.74},{"text":"computed a weighted sum of the predicted","start":868.279,"duration":5.4},{"text":"vectors for each digit capsule and we","start":870.889,"duration":5.64},{"text":"just run the squash function and we get","start":873.679,"duration":6.27},{"text":"the outputs of the digit capsules all","start":876.529,"duration":6.511},{"text":"right but wait this is just the end of","start":879.949,"duration":5.31},{"text":"round one of the routing by agreement","start":883.04,"duration":4.859},{"text":"algorithm now on to round two","start":885.259,"duration":5.64},{"text":"so first we need to measure how good","start":887.899,"duration":5.55},{"text":"each prediction was and use this to","start":890.899,"duration":5.071},{"text":"update the routing weights for example","start":893.449,"duration":4.14},{"text":"look at the predictions that we made","start":895.97,"duration":4.429},{"text":"using this primary capsules output","start":897.589,"duration":5.881},{"text":"notice that for example the prediction","start":900.399,"duration":5.831},{"text":"for digit four is excellent and this is","start":903.47,"duration":4.949},{"text":"measured using the scalar product of the","start":906.23,"duration":3.93},{"text":"predicted output vector and the","start":908.419,"duration":4.801},{"text":"actual output vector these two vectors","start":910.16,"duration":4.859},{"text":"are actually represented as column","start":913.22,"duration":4.109},{"text":"vectors meaning a matrix with a single","start":915.019,"duration":4.711},{"text":"column so to compute the scalar product","start":917.329,"duration":5.37},{"text":"we must transpose the predicted column","start":919.73,"duration":6.57},{"text":"vector you had J I to get a row vector","start":922.699,"duration":7.2},{"text":"and multiply this this row vector and","start":926.3,"duration":6.389},{"text":"the actual output vector V J which is a","start":929.899,"duration":5.13},{"text":"column vector we will get a one by one","start":932.689,"duration":4.561},{"text":"matrix containing the scalar product of","start":935.029,"duration":4.56},{"text":"the vectors and now of course we need to","start":937.25,"duration":4.829},{"text":"do this for each predicted vector so","start":939.589,"duration":4.651},{"text":"once again we can use the map mol","start":942.079,"duration":4.2},{"text":"function to perform all the matrix","start":944.24,"duration":4.159},{"text":"multiplications in just one shot","start":946.279,"duration":5.191},{"text":"first we must use the tile function to","start":948.399,"duration":5.17},{"text":"get one copy of the actual output","start":951.47,"duration":4.88},{"text":"vectors V J for each primary capsule","start":953.569,"duration":6.21},{"text":"then we use map mo telling it to","start":956.35,"duration":5.799},{"text":"transpose each metric matrix in the","start":959.779,"duration":6.0},{"text":"first array on the fly and lo and behold","start":962.149,"duration":5.94},{"text":"we get all the scalar products at once","start":965.779,"duration":5.191},{"text":"so now we have a measure of agreements","start":968.089,"duration":5.19},{"text":"between each predicted vector and the","start":970.97,"duration":5.52},{"text":"actual output vector we can then add","start":973.279,"duration":5.841},{"text":"these scalar products to the raw whites","start":976.49,"duration":6.0},{"text":"using a simple addition and the rest of","start":979.12,"duration":5.23},{"text":"round 2 is exactly the same as the end","start":982.49,"duration":4.44},{"text":"of round 1 the code is really identical","start":984.35,"duration":4.979},{"text":"except we're now using the raw routing","start":986.93,"duration":5.219},{"text":"ways of round 2 we compute their softmax","start":989.329,"duration":4.711},{"text":"to get the actual routing weights for","start":992.149,"duration":3.99},{"text":"round 2 then we compute the weighted sum","start":994.04,"duration":4.2},{"text":"of all the predicted vectors for each","start":996.139,"duration":4.32},{"text":"digit capsule and finally we squash the","start":998.24,"duration":5.039},{"text":"results and now we have the new digit","start":1000.459,"duration":5.641},{"text":"capsule outputs and we finish round 2 we","start":1003.279,"duration":5.31},{"text":"could do a few more rounds exactly like","start":1006.1,"duration":5.25},{"text":"this one but I'll stop now and use the","start":1008.589,"duration":4.411},{"text":"current output vectors at the end of","start":1011.35,"duration":3.989},{"text":"round 2 as the output of the digit","start":1013.0,"duration":5.79},{"text":"capsules now you probably noticed that I","start":1015.339,"duration":5.521},{"text":"implemented the routing algorithms loop","start":1018.79,"duration":4.409},{"text":"without an actual loop it's a bit like","start":1020.86,"duration":4.5},{"text":"computing the sum of squares from 1 to","start":1023.199,"duration":4.921},{"text":"100 with this code of course this will","start":1025.36,"duration":5.579},{"text":"build a very big tensor flow graph but","start":1028.12,"duration":5.28},{"text":"it works you can think of it as an","start":1030.939,"duration":5.37},{"text":"unrolled loop now cleaner a way to do","start":1033.4,"duration":4.889},{"text":"this would be to write a for loop in","start":1036.309,"duration":3.5},{"text":"Python like this","start":1038.289,"duration":4.43},{"text":"much better however it's important to","start":1039.809,"duration":4.77},{"text":"understand that the resulting tensor","start":1042.719,"duration":4.05},{"text":"photograph will be absolutely identical","start":1044.579,"duration":4.501},{"text":"to the one produced by the previous code","start":1046.769,"duration":4.921},{"text":"all we're doing here is constructing a","start":1049.08,"duration":5.039},{"text":"graph and tensorflow will not even know","start":1051.69,"duration":4.219},{"text":"that we use the loop to build a graph","start":1054.119,"duration":4.98},{"text":"again this works fine it's just that you","start":1055.909,"duration":5.411},{"text":"end up with a very large graph so you","start":1059.099,"duration":4.56},{"text":"can think of this loop as a static loop","start":1061.32,"duration":4.349},{"text":"that only runs at graph construction","start":1063.659,"duration":5.041},{"text":"time if you want a dynamic loop one that","start":1065.669,"duration":5.521},{"text":"tensorflow itself will run then you must","start":1068.7,"duration":4.319},{"text":"use tensor flows while loop function","start":1071.19,"duration":5.609},{"text":"like this the while loop function takes","start":1073.019,"duration":6.21},{"text":"three parameters the first one is a","start":1076.799,"duration":5.61},{"text":"function that must return a tensor that","start":1079.229,"duration":6.3},{"text":"will determine whether the loop should","start":1082.409,"duration":5.82},{"text":"go on or not at each iteration the","start":1085.529,"duration":5.4},{"text":"second parameter is also a function that","start":1088.229,"duration":5.52},{"text":"must build the body of the loop and that","start":1090.929,"duration":4.56},{"text":"will also be evaluated at each iteration","start":1093.749,"duration":5.01},{"text":"and finally third parameter contains a","start":1095.489,"duration":5.4},{"text":"list of tensors that will be sent to","start":1098.759,"duration":4.5},{"text":"both the condition and loop body","start":1100.889,"duration":5.551},{"text":"function at the first iteration for the","start":1103.259,"duration":5.16},{"text":"following iterations these functions","start":1106.44,"duration":4.469},{"text":"will receive the output of the loop body","start":1108.419,"duration":5.1},{"text":"function so you can pause the video if","start":1110.909,"duration":4.08},{"text":"you need to take a closer look at this","start":1113.519,"duration":3.45},{"text":"code once you get it you can try","start":1114.989,"duration":4.201},{"text":"modifying my caps net implementation to","start":1116.969,"duration":4.5},{"text":"use a dynamic loop rather than a static","start":1119.19,"duration":5.279},{"text":"unrolled loop apart from making the code","start":1121.469,"duration":5.79},{"text":"cleaner and the graph smaller using a","start":1124.469,"duration":4.77},{"text":"dynamic loop allows you to change the","start":1127.259,"duration":4.38},{"text":"number of iterations using the exact","start":1129.239,"duration":5.581},{"text":"same model also if you set the swap","start":1131.639,"duration":5.071},{"text":"memory parameter of the while loop","start":1134.82,"duration":4.019},{"text":"function if you set it to true","start":1136.71,"duration":4.469},{"text":"tensorflow will automatically swap the","start":1138.839,"duration":6.0},{"text":"GPU memory to CPU memory when it can to","start":1141.179,"duration":6.99},{"text":"save GPU memory since CPU Ram is much","start":1144.839,"duration":5.611},{"text":"cheaper and abundant this can really be","start":1148.169,"duration":6.45},{"text":"useful and that's it we've computed the","start":1150.45,"duration":6.929},{"text":"output of the digit capsules cool now","start":1154.619,"duration":5.04},{"text":"the length of each output vector","start":1157.379,"duration":4.77},{"text":"represents the probability that a digit","start":1159.659,"duration":6.45},{"text":"of that class is present in the image so","start":1162.149,"duration":6.72},{"text":"let's compute these probabilities for","start":1166.109,"duration":5.581},{"text":"this we cannot use tensor flows norm","start":1168.869,"duration":3.631},{"text":"function","start":1171.69,"duration":3.479},{"text":"because training will explode if there's","start":1172.5,"duration":4.5},{"text":"a zero vector at any point as I","start":1175.169,"duration":4.351},{"text":"mentioned earlier so instead we used a","start":1177.0,"duration":5.49},{"text":"homemade safe norm function similar to","start":1179.52,"duration":5.18},{"text":"what we did with the squash function and","start":1182.49,"duration":4.98},{"text":"note that the sum of the probabilities","start":1184.7,"duration":5.469},{"text":"don't necessarily add up to one because","start":1187.47,"duration":6.12},{"text":"we are not using a soft max layer this","start":1190.169,"duration":5.161},{"text":"makes it possible to detect multiple","start":1193.59,"duration":4.199},{"text":"different digits in the same image but","start":1195.33,"duration":4.86},{"text":"they all have to be different digits","start":1197.789,"duration":5.071},{"text":"view you can detect a 5 and a 3 but you","start":1200.19,"duration":6.96},{"text":"cannot detect say two fives next let's","start":1202.86,"duration":8.01},{"text":"predict the most likely digit we just","start":1207.15,"duration":6.18},{"text":"used the Arg max function that gives us","start":1210.87,"duration":5.16},{"text":"the index of the highest probability the","start":1213.33,"duration":5.16},{"text":"index happens to be the number of the","start":1216.03,"duration":5.31},{"text":"digit itself note that we first get a","start":1218.49,"duration":4.919},{"text":"tensor that has a couple extra","start":1221.34,"duration":5.04},{"text":"dimensions of size one at the end so we","start":1223.409,"duration":4.561},{"text":"get rid of them using the squeeze","start":1226.38,"duration":4.59},{"text":"function if we called squeeze without","start":1227.97,"duration":6.089},{"text":"specifying the axes to remove it would","start":1230.97,"duration":6.03},{"text":"remove all dimensions of size 1 this","start":1234.059,"duration":6.091},{"text":"would generally be okay except if the","start":1237.0,"duration":5.58},{"text":"batch size was equal to 1 in which case","start":1240.15,"duration":4.44},{"text":"we would be left with a scalar value","start":1242.58,"duration":4.77},{"text":"rather than an array and we don't want","start":1244.59,"duration":6.38},{"text":"that so it's better to specify the axes","start":1247.35,"duration":6.75},{"text":"great now we have a capsule network that","start":1250.97,"duration":5.319},{"text":"can estimate class probabilities and","start":1254.1,"duration":5.22},{"text":"make predictions we can measure the","start":1256.289,"duration":5.64},{"text":"models accuracy on the batch by simply","start":1259.32,"duration":4.349},{"text":"comparing the predictions and the labels","start":1261.929,"duration":4.11},{"text":"in this case the prediction for the last","start":1263.669,"duration":4.231},{"text":"digit in the batch is wrong it's seven","start":1266.039,"duration":5.13},{"text":"instead of one so we get 80% accuracy","start":1267.9,"duration":5.61},{"text":"now the code is really straightforward","start":1271.169,"duration":4.38},{"text":"we just use the equal function to","start":1273.51,"duration":3.84},{"text":"compare the labels and the predictions","start":1275.549,"duration":4.801},{"text":"wipe red and this gives us an array of","start":1277.35,"duration":5.579},{"text":"boolean z' so we cast these boolean stew","start":1280.35,"duration":4.53},{"text":"floats which gives us a bunch of zeros","start":1282.929,"duration":4.891},{"text":"for bad predictions and ones for good","start":1284.88,"duration":5.88},{"text":"predictions and we compute the mean to","start":1287.82,"duration":6.57},{"text":"get the batch accuracy the labels Y are","start":1290.76,"duration":5.669},{"text":"just a regular placeholder nothing","start":1294.39,"duration":5.25},{"text":"special and that's it we have a full","start":1296.429,"duration":5.551},{"text":"model able to make predictions now let's","start":1299.64,"duration":5.279},{"text":"look at the training code this diagram","start":1301.98,"duration":3.49},{"text":"is about two","start":1304.919,"duration":2.521},{"text":"get pretty crowded so I'll remove the","start":1305.47,"duration":6.18},{"text":"accuracy for clarity and now first we","start":1307.44,"duration":6.91},{"text":"want to compute the margin loss it's","start":1311.65,"duration":5.37},{"text":"given by this equation by the way I made","start":1314.35,"duration":4.53},{"text":"a mistake in my first video I squared","start":1317.02,"duration":4.32},{"text":"norms instead of squaring the max","start":1318.88,"duration":5.31},{"text":"operations sorry about that this here is","start":1321.34,"duration":5.79},{"text":"the correct equation computing it is","start":1324.19,"duration":4.619},{"text":"pretty straightforward so I won't go","start":1327.13,"duration":3.81},{"text":"through it in details the only trick is","start":1328.809,"duration":4.23},{"text":"to understand how you can easily compute","start":1330.94,"duration":5.34},{"text":"all the TK values for a given instance","start":1333.039,"duration":6.151},{"text":"TK is equal to 1 if a digit of cos K is","start":1336.28,"duration":4.769},{"text":"present in the image otherwise it's","start":1339.19,"duration":5.25},{"text":"equal to 0 you can get all the TK values","start":1341.049,"duration":5.61},{"text":"for each instance by simply converting","start":1344.44,"duration":5.099},{"text":"the labels to one hot representation for","start":1346.659,"duration":5.731},{"text":"example if an instance label is three","start":1349.539,"duration":5.311},{"text":"then for this instance T will contain a","start":1352.39,"duration":4.44},{"text":"10 dimensional vector full of zeros","start":1354.85,"duration":6.0},{"text":"except for a 1 its index 3 okay next we","start":1356.83,"duration":5.94},{"text":"want to compute the reconstruction loss","start":1360.85,"duration":5.04},{"text":"so first we must send the outputs of the","start":1362.77,"duration":5.49},{"text":"digit capsules to a decoder that will","start":1365.89,"duration":4.35},{"text":"try to use them to reconstruct the input","start":1368.26,"duration":5.13},{"text":"images this decoder is just a regular","start":1370.24,"duration":5.22},{"text":"feed-forward neural net composed of","start":1373.39,"duration":4.89},{"text":"three fully connected layers it's really","start":1375.46,"duration":4.949},{"text":"simple code so you can pause the video","start":1378.28,"duration":4.019},{"text":"if you want and take a close look at it","start":1380.409,"duration":5.25},{"text":"it outputs an array containing 784","start":1382.299,"duration":6.171},{"text":"values from 0 to 1 for each instance","start":1385.659,"duration":6.0},{"text":"representing the pixel intensities of 28","start":1388.47,"duration":7.18},{"text":"by 28 pixel images and that's it we have","start":1391.659,"duration":6.601},{"text":"our reconstructed images we can now","start":1395.65,"duration":5.46},{"text":"compute the reconstruction loss this is","start":1398.26,"duration":4.62},{"text":"just the squared difference between the","start":1401.11,"duration":3.98},{"text":"input images and the reconstructions","start":1402.88,"duration":6.029},{"text":"since the input images are 28 by 28 by 1","start":1405.09,"duration":6.49},{"text":"we first reshape them to one dimension","start":1408.909,"duration":6.091},{"text":"per instance with 784 values each then","start":1411.58,"duration":6.27},{"text":"we compute the squared difference now we","start":1415.0,"duration":6.21},{"text":"can compute the final loss it's just the","start":1417.85,"duration":5.49},{"text":"sum of the margin loss and the","start":1421.21,"duration":4.56},{"text":"reconstruction loss scale down to let","start":1423.34,"duration":4.98},{"text":"the margin loss dominate training pretty","start":1425.77,"duration":4.529},{"text":"simple as you can see now let's add the","start":1428.32,"duration":4.41},{"text":"training operation the paper mentions","start":1430.299,"duration":4.771},{"text":"they use tensor flows implementation of","start":1432.73,"duration":4.38},{"text":"the atom optimizer using the default","start":1435.07,"duration":3.75},{"text":"parameters so let's do that","start":1437.11,"duration":3.99},{"text":"we create the optimizer and call it's","start":1438.82,"duration":3.9},{"text":"minimized method to get the training","start":1441.1,"duration":3.3},{"text":"operation that will tweak the model","start":1442.72,"duration":4.32},{"text":"parameters to minimize the loss we're","start":1444.4,"duration":4.74},{"text":"almost done but there's one last detail","start":1447.04,"duration":4.17},{"text":"I didn't mention in the first video the","start":1449.14,"duration":4.65},{"text":"paper indicates that the outputs of the","start":1451.21,"duration":5.16},{"text":"digit capsules should all be masked out","start":1453.79,"duration":4.98},{"text":"except for the ones corresponding to the","start":1456.37,"duration":5.01},{"text":"target digit so instead of sending the","start":1458.77,"duration":5.13},{"text":"digit capsules outputs directly to the","start":1461.38,"duration":5.37},{"text":"decoder we want to apply a mask first","start":1463.9,"duration":6.15},{"text":"like this the mask will have the same","start":1466.75,"duration":5.94},{"text":"shape as the digit capsules output array","start":1470.05,"duration":5.55},{"text":"and it will be equal to zero everywhere","start":1472.69,"duration":5.91},{"text":"except for once at the location of the","start":1475.6,"duration":6.3},{"text":"target digits by multiplying the digit","start":1478.6,"duration":5.97},{"text":"capsules output in the mask we get the","start":1481.9,"duration":5.79},{"text":"input to the decoder but there's one","start":1484.57,"duration":4.83},{"text":"catch this picture is good for training","start":1487.69,"duration":4.5},{"text":"but at test time we won't have the","start":1489.4,"duration":5.76},{"text":"labels so instead we will master the","start":1492.19,"duration":5.07},{"text":"output vectors using the predicted","start":1495.16,"duration":6.39},{"text":"classes rather than labels like this now","start":1497.26,"duration":5.82},{"text":"we could build a different graph for","start":1501.55,"duration":4.32},{"text":"training and for testing but it wouldn't","start":1503.08,"duration":5.22},{"text":"be very convenient so instead let's","start":1505.87,"duration":4.89},{"text":"build a conditioned operation we will","start":1508.3,"duration":4.68},{"text":"add a boolean a placeholder called mask","start":1510.76,"duration":5.19},{"text":"with labels if it is true then we use","start":1512.98,"duration":5.7},{"text":"the labels to build the mask if it is","start":1515.95,"duration":4.68},{"text":"false then we use the prediction note","start":1518.68,"duration":5.7},{"text":"the difference okay and here's the code","start":1520.63,"duration":6.3},{"text":"we build the mask of labels placeholder","start":1524.38,"duration":4.77},{"text":"which will default to false so that we","start":1526.93,"duration":4.26},{"text":"only need to set it during training and","start":1529.15,"duration":3.99},{"text":"then we define the reconstruction","start":1531.19,"duration":4.14},{"text":"targets using tensorflow scon function","start":1533.14,"duration":5.58},{"text":"it takes three arguments the first one","start":1535.33,"duration":6.42},{"text":"is a tensor representing the condition","start":1538.72,"duration":5.04},{"text":"in this case simply the mask with labels","start":1541.75,"duration":4.47},{"text":"placeholder the second parameter is a","start":1543.76,"duration":4.38},{"text":"function that returns the tensor to use","start":1546.22,"duration":4.44},{"text":"if the condition is true and the third","start":1548.14,"duration":4.77},{"text":"parameter is a function that returns the","start":1550.66,"duration":3.75},{"text":"tensor to use that the condition is","start":1552.91,"duration":4.56},{"text":"false then to build the mask we simply","start":1554.41,"duration":5.88},{"text":"use the one-hot function now there's","start":1557.47,"duration":4.92},{"text":"actually one slight problem with this","start":1560.29,"duration":5.13},{"text":"implementation and to explain it I need","start":1562.39,"duration":5.07},{"text":"to step back for a second and talk about","start":1565.42,"duration":4.71},{"text":"how tensorflow evaluates","start":1567.46,"duration":6.31},{"text":"suppose we built this graph these are","start":1570.13,"duration":4.51},{"text":"all tensorflow","start":1573.77,"duration":3.42},{"text":"operations and we want to evaluate the","start":1574.64,"duration":5.46},{"text":"output of operation a the first thing","start":1577.19,"duration":5.22},{"text":"tensorflow will do is resolve the","start":1580.1,"duration":5.64},{"text":"dependencies it will find all the","start":1582.41,"duration":5.97},{"text":"operations that a depends on directly or","start":1585.74,"duration":4.89},{"text":"indirectly by traversing the graph","start":1588.38,"duration":5.19},{"text":"backwards in this case we'll find C D","start":1590.63,"duration":6.27},{"text":"and F next it will run any of these","start":1593.57,"duration":5.91},{"text":"operations that has no inputs these are","start":1596.9,"duration":6.06},{"text":"called root nodes in this case F once F","start":1599.48,"duration":6.42},{"text":"is evaluated operation C and D now have","start":1602.96,"duration":5.1},{"text":"all the inputs they need so they can be","start":1605.9,"duration":4.92},{"text":"evaluated and tensorflow will actually","start":1608.06,"duration":5.73},{"text":"try to run them in parallel say D","start":1610.82,"duration":6.0},{"text":"finishes first a still has one unev","start":1613.79,"duration":6.48},{"text":"alyou ated input so it can't run yet but","start":1616.82,"duration":5.76},{"text":"as soon as C is finished a can be","start":1620.27,"duration":5.04},{"text":"evaluated and once it's done the eval","start":1622.58,"duration":4.47},{"text":"method returns the result in we're good","start":1625.31,"duration":4.47},{"text":"you can actually evaluate multiple","start":1627.05,"duration":4.98},{"text":"operations at once for example a and E","start":1629.78,"duration":4.62},{"text":"and the process is really the same it","start":1632.03,"duration":5.31},{"text":"finds all the dependencies runs the root","start":1634.4,"duration":5.25},{"text":"operations and then you know goes upward","start":1637.34,"duration":6.18},{"text":"running every operation whose inputs are","start":1639.65,"duration":6.3},{"text":"satisfied and once it's got both the","start":1643.52,"duration":4.77},{"text":"values for a and E it returns the","start":1645.95,"duration":5.58},{"text":"results so let's apply this to our","start":1648.29,"duration":6.06},{"text":"reconstruction targets this tensor is","start":1651.53,"duration":6.3},{"text":"the output of the cond operation which","start":1654.35,"duration":6.45},{"text":"has three parameters mask with labels a","start":1657.83,"duration":6.39},{"text":"function that returns Y and a function","start":1660.8,"duration":6.36},{"text":"that returns wipe red and when we","start":1664.22,"duration":5.16},{"text":"evaluate the reconstruction targets or","start":1667.16,"duration":4.32},{"text":"any tensor that depends on it such as","start":1669.38,"duration":4.62},{"text":"the final loss which depends on the","start":1671.48,"duration":4.05},{"text":"reconstruction loss which eventually","start":1674.0,"duration":3.39},{"text":"depends on the reconstruction targets","start":1675.53,"duration":4.05},{"text":"well what happens is as earlier","start":1677.39,"duration":3.99},{"text":"tensorflow starts by resolving the","start":1679.58,"duration":4.8},{"text":"dependencies it finds all three bottom","start":1681.38,"duration":6.69},{"text":"nodes and it evaluates them all so Y","start":1684.38,"duration":4.98},{"text":"pred may finish first","start":1688.07,"duration":2.97},{"text":"since these operations are run in","start":1689.36,"duration":3.72},{"text":"parallel there's no way to know in which","start":1691.04,"duration":5.31},{"text":"order they will finish so you know Y may","start":1693.08,"duration":6.36},{"text":"finish next and finally mask what label","start":1696.35,"duration":5.7},{"text":"finishes so suppose it evaluates to true","start":1699.44,"duration":4.32},{"text":"now the reconstruct","start":1702.05,"duration":3.87},{"text":"target's has all the inputs it needs so","start":1703.76,"duration":4.65},{"text":"it can it can be evaluated and of course","start":1705.92,"duration":3.54},{"text":"it does the right thing","start":1708.41,"duration":2.91},{"text":"since masks with labels is true it","start":1709.46,"duration":5.97},{"text":"returns the value of y which is good but","start":1711.32,"duration":6.15},{"text":"notice that y pred was evaluated for","start":1715.43,"duration":4.74},{"text":"nothing we're not using its output it's","start":1717.47,"duration":4.59},{"text":"not a big deal since during training we","start":1720.17,"duration":3.96},{"text":"need to evaluate the margin loss which","start":1722.06,"duration":3.45},{"text":"depends on the estimated class","start":1724.13,"duration":3.84},{"text":"probabilities which is just one step","start":1725.51,"duration":4.17},{"text":"away from the prediction so computing","start":1727.97,"duration":4.11},{"text":"the predictions won't add much overhead","start":1729.68,"duration":4.94},{"text":"but still it's a bit unfortunate","start":1732.08,"duration":5.37},{"text":"now suppose mask what labels evaluates","start":1734.62,"duration":6.16},{"text":"to false then again the reconstruction","start":1737.45,"duration":5.07},{"text":"targets will do the right thing it will","start":1740.78,"duration":5.07},{"text":"output the value of y pred but this time","start":1742.52,"duration":6.33},{"text":"how we evaluated y for nothing it's just","start":1745.85,"duration":4.65},{"text":"a placeholder so it won't add much","start":1748.85,"duration":3.36},{"text":"computation time but it means that we","start":1750.5,"duration":4.32},{"text":"must feed why even if mask with labels","start":1752.21,"duration":3.24},{"text":"is false","start":1754.82,"duration":3.06},{"text":"well actually we can just pass an empty","start":1755.45,"duration":5.46},{"text":"array and that's fine so it will work","start":1757.88,"duration":4.85},{"text":"but it's kind of ugly","start":1760.91,"duration":4.59},{"text":"so this unfortunate situation is due to","start":1762.73,"duration":5.08},{"text":"the fact that the functions we passed to","start":1765.5,"duration":4.92},{"text":"the con function do not actually create","start":1767.81,"duration":5.43},{"text":"any tensor they just return tensors that","start":1770.42,"duration":5.43},{"text":"were created outside of these functions","start":1773.24,"duration":5.37},{"text":"so if you build tensors within these","start":1775.85,"duration":5.01},{"text":"functions then tensorflow will do what","start":1778.61,"duration":4.5},{"text":"you expect it will stitch together the","start":1780.86,"duration":4.2},{"text":"partial graphs created within these","start":1783.11,"duration":3.81},{"text":"functions into a graph that will","start":1785.06,"duration":5.22},{"text":"properly handle the dependencies so you","start":1786.92,"duration":5.52},{"text":"might be able to modify my code and fix","start":1790.28,"duration":4.68},{"text":"this ugliness I tried but I ended up","start":1792.44,"duration":4.5},{"text":"with pretty convoluted code so I decided","start":1794.96,"duration":4.47},{"text":"to stick with this implementation I hope","start":1796.94,"duration":5.18},{"text":"it won't keep you awake at night so","start":1799.43,"duration":5.37},{"text":"we've actually finished here's the full","start":1802.12,"duration":5.05},{"text":"picture again the construction phase is","start":1804.8,"duration":5.79},{"text":"over our graph is built now on to the","start":1807.17,"duration":5.91},{"text":"execution phase let's run this graph","start":1810.59,"duration":5.15},{"text":"first let's look at the training code","start":1813.08,"duration":5.55},{"text":"it's really really completely standard","start":1815.74,"duration":6.19},{"text":"you create a session if a check point","start":1818.63,"duration":7.08},{"text":"file exists load it or else initialize","start":1821.93,"duration":6.3},{"text":"all the variables then run the main","start":1825.71,"duration":4.74},{"text":"training loop for a number of epochs and","start":1828.23,"duration":5.34},{"text":"for each epoch run enough iterations to","start":1830.45,"duration":4.56},{"text":"go through the full training set and","start":1833.57,"duration":3.1},{"text":"inside this loop","start":1835.01,"duration":4.36},{"text":"we simply evaluate the training","start":1836.67,"duration":6.51},{"text":"operation and the loss on the next batch","start":1839.37,"duration":7.29},{"text":"we feed the images and the labels of the","start":1843.18,"duration":5.76},{"text":"current training batch and we set masks","start":1846.66,"duration":4.95},{"text":"with labels to true that's pretty much","start":1848.94,"duration":4.53},{"text":"all there is to it and the note book I","start":1851.61,"duration":3.84},{"text":"also added a simple implementation of","start":1853.47,"duration":3.96},{"text":"early stopping and I print out the","start":1855.45,"duration":4.47},{"text":"progress plus I evaluate the model on","start":1857.43,"duration":4.11},{"text":"the validation set at the end of each","start":1859.92,"duration":4.26},{"text":"epoch but the most important part is","start":1861.54,"duration":6.21},{"text":"here after training I just run a few","start":1864.18,"duration":5.61},{"text":"tests images through the network and get","start":1867.75,"duration":3.69},{"text":"the predictions and reconstructions as","start":1869.79,"duration":3.9},{"text":"you can see the predictions are all","start":1871.44,"duration":4.38},{"text":"correct and the reconstructions are","start":1873.69,"duration":3.78},{"text":"pretty good they're pretty close to the","start":1875.82,"duration":3.81},{"text":"original images except that they're","start":1877.47,"duration":4.97},{"text":"slightly fuzzier and as you can see","start":1879.63,"duration":5.43},{"text":"here's the code there's really nothing","start":1882.44,"duration":5.32},{"text":"special about it I take a few images I","start":1885.06,"duration":5.82},{"text":"start a session and load the model then","start":1887.76,"duration":5.49},{"text":"I just evaluate predictions the","start":1890.88,"duration":4.65},{"text":"predictions and the decoders output and","start":1893.25,"duration":5.43},{"text":"I also get the capsules output so I can","start":1895.53,"duration":6.75},{"text":"tweak them later the ugliness I","start":1898.68,"duration":5.73},{"text":"mentioned earlier is right here right","start":1902.28,"duration":5.61},{"text":"I'm forced to pass an empty array this","start":1904.41,"duration":6.93},{"text":"value will be ignored anyway and finally","start":1907.89,"duration":5.67},{"text":"the code tweaks the output vectors and","start":1911.34,"duration":5.19},{"text":"passes the results to the decoder so we","start":1913.56,"duration":5.07},{"text":"can see what each of the sixteen","start":1916.53,"duration":4.2},{"text":"dimensions represent and the digit","start":1918.63,"duration":4.23},{"text":"capsules output vector for example this","start":1920.73,"duration":4.26},{"text":"image shows the reconstructions we get","start":1922.86,"duration":4.95},{"text":"by tweaking the first parameter so the","start":1924.99,"duration":4.59},{"text":"notebook produces one such image for","start":1927.81,"duration":4.2},{"text":"each of the sixteen parameters and as","start":1929.58,"duration":5.79},{"text":"you can see in the first second and last","start":1932.01,"duration":5.88},{"text":"rows we see that the digits become","start":1935.37,"duration":4.17},{"text":"thinner and thinner we're going to the","start":1937.89,"duration":3.32},{"text":"right or thicker and thicker to the left","start":1939.54,"duration":4.47},{"text":"so it holds information about thickness","start":1941.21,"duration":5.47},{"text":"and in the middle row you can see that","start":1944.01,"duration":5.91},{"text":"the bottom part of the number five gets","start":1946.68,"duration":6.39},{"text":"lifted towards the top so probably","start":1949.92,"duration":5.67},{"text":"that's what this parameter does for this","start":1953.07,"duration":5.61},{"text":"digit before I finish I'd like to thank","start":1955.59,"duration":5.04},{"text":"everyone who shared and commented on my","start":1958.68,"duration":5.25},{"text":"first video I really had no idea it","start":1960.63,"duration":4.62},{"text":"would receive such an enthusiastic","start":1963.93,"duration":4.17},{"text":"response and I'm very very grateful to","start":1965.25,"duration":3.27},{"text":"all of you","start":1968.1,"duration":2.31},{"text":"it definitely motivates me to","start":1968.52,"duration":4.17},{"text":"more videos if you want to learn more","start":1970.41,"duration":4.08},{"text":"about machine learning and support this","start":1972.69,"duration":3.96},{"text":"channel check out my O'Reilly book hands","start":1974.49,"duration":3.36},{"text":"on machine learning with scikit-learn","start":1976.65,"duration":2.04},{"text":"and tensorflow","start":1977.85,"duration":2.31},{"text":"I leave the links in the video","start":1978.69,"duration":3.39},{"text":"description if you speak German there's","start":1980.16,"duration":3.51},{"text":"actually a German translation coming up","start":1982.08,"duration":3.72},{"text":"for Christmas and if you speak French","start":1983.67,"duration":4.41},{"text":"the translation is already available it","start":1985.8,"duration":4.29},{"text":"was split in two books but it's really","start":1988.08,"duration":4.56},{"text":"the same content and that's all I have","start":1990.09,"duration":4.11},{"text":"for today I hope you enjoyed this video","start":1992.64,"duration":3.09},{"text":"and that you learned a thing or two","start":1994.2,"duration":3.87},{"text":"about tensorflow and capsule networks if","start":1995.73,"duration":4.56},{"text":"you did please like share comment","start":1998.07,"duration":3.96},{"text":"subscribe and click on the bell icon","start":2000.29,"duration":3.48},{"text":"next to the subscribe button to receive","start":2002.03,"duration":3.59},{"text":"notifications when I upload new videos","start":2003.77,"duration":5.33},{"text":"see you next time","start":2005.62,"duration":3.48}]},"manual":{"pPN8d0E3900":[{"text":"Hey! I\u2019m Aur\u00e9lien G\u00e9ron, and in this video\nI\u2019ll tell you all about Capsule Networks,","start":0.14,"duration":4.06},{"text":"a hot new architecture for neural nets. Geoffrey\nHinton had the idea of Capsule Networks several","start":4.2,"duration":5.36},{"text":"years ago, and he published a paper in 2011\nthat introduced many of the key ideas, but","start":9.56,"duration":5.431},{"text":"he had a hard time making them work properly,\nuntil now.","start":14.991,"duration":3.669},{"text":"A few weeks ago, in October 2017, a paper\ncalled \u201cDynamic Routing Between Capsules\u201d","start":18.66,"duration":5.839},{"text":"was published by Sara Sabour, Nicholas Frosst\nand of course Geoffrey Hinton. They managed","start":24.499,"duration":4.66},{"text":"to reach state of the art performance on the\nMNIST dataset, and demonstrated considerably","start":29.159,"duration":5.391},{"text":"better results than convolutional neural nets\non highly overlapping digits. So what are","start":34.55,"duration":6.12},{"text":"capsule networks exactly?","start":40.67,"duration":1.76},{"text":"Well, in computer graphics, you start with\nan abstract representation of a scene, for","start":42.43,"duration":4.91},{"text":"example a rectangle at position x=20 and y=30,\nrotated by 16\u00b0, and so on. Each object type","start":47.34,"duration":8.249},{"text":"has various instantiation parameters. Then\nyou call some rendering function, and boom,","start":55.589,"duration":5.86},{"text":"you get an image.","start":61.449,"duration":2.311},{"text":"Inverse graphics, is just the reverse process.\nYou start with an image, and you try to find","start":63.76,"duration":5.249},{"text":"what objects it contains, and what their instantiation\nparameters are. A capsule network is basically","start":69.009,"duration":7.021},{"text":"a neural network that tries to perform inverse\ngraphics.","start":76.03,"duration":4.57},{"text":"It is composed of many capsules. A capsule\nis any function that tries to predict the","start":80.6,"duration":5.64},{"text":"presence and the instantiation parameters\nof a particular object at a given location.","start":86.24,"duration":6.449},{"text":"For example, the network above contains 50\ncapsules. The arrows represent the output","start":92.689,"duration":5.901},{"text":"vectors of these capsules. The capsules output\nvectors. The black arrows correspond to capsules","start":98.59,"duration":6.45},{"text":"that try to find rectangles, while the blue\narrows represent the output of capsules looking","start":105.04,"duration":5.49},{"text":"for triangles. The length of an activation\nvector represents the estimated probability","start":110.53,"duration":7.03},{"text":"that the object the capsule is looking for\nis indeed present. You can see that most arrows","start":117.56,"duration":5.379},{"text":"are tiny, meaning the capsules didn\u2019t detect\nanything, but two arrows are quite long. This","start":122.939,"duration":6.19},{"text":"means that the capsules at these locations\nare pretty confident that they found what","start":129.129,"duration":4.241},{"text":"they were looking for, in this case a rectangle,\nand a triangle.","start":133.37,"duration":4.72},{"text":"Next, the orientation of the activation vector\nencodes the instantiation parameters of the","start":138.09,"duration":6.46},{"text":"object, for example in this case the object\u2019s\nrotation, but it could be also its thickness,","start":144.55,"duration":4.71},{"text":"how stretched or skewed it is, its exact position\n(there might be slight translations), and","start":149.26,"duration":4.97},{"text":"so on. For simplicity, I\u2019ll just focus on\nthe rotation parameter, but in a real capsule","start":154.23,"duration":5.7},{"text":"network, the activation vectors may have 5,\n10 dimensions or more.","start":159.93,"duration":5.22},{"text":"In practice, a good way to implement this\nis to first apply a couple convolutional layers,","start":165.15,"duration":6.449},{"text":"just like in a regular convolutional neural\nnet. This will output an array containing","start":171.599,"duration":5.03},{"text":"a bunch of feature maps. You can then reshape\nthis array to get a set of vectors for each","start":176.629,"duration":6.071},{"text":"location. For example, suppose the convolutional\nlayers output an array containing, say, 18","start":182.7,"duration":5.86},{"text":"feature maps (2 times 9), you can easily reshape\nthis array to get 2 vectors of 9 dimensions","start":188.56,"duration":6.649},{"text":"each, for every location. You could also get\n3 vectors of 6 dimensions each, and so on.","start":195.209,"duration":6.981},{"text":"Something that would look like the capsule\nnetwork represented here with two vectors","start":202.19,"duration":3.85},{"text":"at each location. The last step is to ensure\nthat no vector is longer than 1, since the","start":206.04,"duration":7.16},{"text":"vector\u2019s length is meant to represent a\nprobability, it cannot be greater than 1.","start":213.2,"duration":4.239},{"text":"To do this, we apply a squashing function.\nIt preserves the vector\u2019s orientation, but","start":217.439,"duration":5.181},{"text":"it squashes it to ensure that its length is\nbetween 0 and 1.","start":222.62,"duration":5.47},{"text":"One key feature of Capsule Networks is that\nthey preserve detailed information about the","start":228.09,"duration":5.83},{"text":"object\u2019s location and its pose, throughout\nthe network. For example, if I rotate the","start":233.92,"duration":5.71},{"text":"image slightly, notice that the activation\nvectors also change slightly. Right? This","start":239.63,"duration":6.15},{"text":"is called equivariance. In a regular convolutional\nneural net, there are generally several pooling","start":245.78,"duration":6.55},{"text":"layers, and unfortunately these pooling layers\ntend to lose information, such as the precise","start":252.33,"duration":6.37},{"text":"location and pose of the objects. It\u2019s really\nnot a big deal if you just want to classify","start":258.7,"duration":5.21},{"text":"the whole image, but it makes it challenging\nto perform accurate image segmentation or","start":263.91,"duration":5.47},{"text":"object detection (which require precise location\nand pose). The fact that capsules are equivariant","start":269.38,"duration":8.44},{"text":"makes them very promising for these applications.","start":277.82,"duration":2.59},{"text":"All right, so now let\u2019s see how capsule\nnetworks can handle objects that are composed","start":280.41,"duration":5.32},{"text":"of a hierarchy of parts. For example, consider\na boat centered at position x=22 and y=28,","start":285.73,"duration":7.24},{"text":"and rotated by 16\u00b0. This boat is composed\nof parts. In this case one rectangle and one","start":292.97,"duration":7.56},{"text":"triangle. So this is how it would be rendered.\nNow we want to do the reverse, we want inverse","start":300.53,"duration":6.1},{"text":"graphics, so we want to go from the image\nto this whole hierarchy of parts with their","start":306.63,"duration":4.92},{"text":"instantiation parameters.","start":311.55,"duration":1.87},{"text":"Similarly, we could also draw a house, using\nthe same parts, a rectangle and a triangle,","start":313.42,"duration":5.68},{"text":"but this time organized in a different way.\nSo the trick will be to try to go from this","start":319.1,"duration":6.1},{"text":"image containing a rectangle and a triangle,\nand figure out, not only that the rectangle","start":325.2,"duration":5.83},{"text":"and triangle are at this location and this\norientation, but also that they are part of","start":331.03,"duration":5.24},{"text":"a boat, not a house. So, yeah, let\u2019s figure\nout how it would do this.","start":336.27,"duration":4.73},{"text":"The first step we have already seen: we run\na couple convolutional layers, we reshape","start":341.0,"duration":4.57},{"text":"the output to get vectors, and we squash them.\nThis gives us the output of the primary capsules.","start":345.57,"duration":5.89},{"text":"We\u2019ve got the first layer already. The next\nstep is where most of the magic and complexity","start":351.46,"duration":5.42},{"text":"of capsule networks takes place. Every capsule\nin the first layer tries to predict the output","start":356.88,"duration":7.15},{"text":"of every capsule in the next layer. You might\nwant to pause to think about what this means.","start":364.03,"duration":5.4},{"text":"The capsules in the first layer try to predict\nwhat the second layer capsules will output.","start":369.43,"duration":7.67},{"text":"For example, let\u2019s consider the capsule\nthat detected the rectangle. I\u2019ll call it","start":377.1,"duration":4.06},{"text":"the rectangle-capsule.","start":381.16,"duration":1.99},{"text":"Let\u2019s suppose that there are just two capsules\nin the next layer, the house-capsule and the","start":383.15,"duration":6.78},{"text":"boat-capsule. Since the rectangle-capsule\ndetected a rectangle rotated by 16\u00b0, it predicts","start":389.93,"duration":7.09},{"text":"that the house-capsule will detect a house\nrotated by 16\u00b0, that makes sense, and the","start":397.02,"duration":5.48},{"text":"boat-capsule will detect a boat rotated by\n16\u00b0 as well. That\u2019s what would be consistent","start":402.5,"duration":5.69},{"text":"with the orientation of the rectangle.","start":408.19,"duration":3.53},{"text":"So, to make this prediction, what the rectangle-capsule\ndoes is it simply computes the dot product","start":411.72,"duration":7.21},{"text":"of a transformation matrix W_i,j with its\nown activation vector u_i. During training,","start":418.93,"duration":8.12},{"text":"the network will gradually learn a transformation\nmatrix for each pair of capsules in the first","start":427.05,"duration":4.95},{"text":"and second layer. In other words, it will\nlearn all the part-whole relationships, for","start":432.0,"duration":4.85},{"text":"example the angle between the wall and the\nroof of a house, and so on.","start":436.85,"duration":5.33},{"text":"Now let\u2019s see what the triangle-capsule\npredicts.","start":442.18,"duration":4.15},{"text":"This time, it\u2019s a bit more interesting:\ngiven the rotation angle of the triangle,","start":446.33,"duration":4.69},{"text":"it predicts that the house-capsule will detect\nan upside-down house, and that the boat-capsule","start":451.02,"duration":5.66},{"text":"will detect a boat rotated by 16\u00b0. These\nare the positions that would be consistent","start":456.68,"duration":5.91},{"text":"with the rotation angle of the triangle.","start":462.59,"duration":5.109},{"text":"Now we have a bunch of predicted outputs,\nwhat do we do with them?","start":467.699,"duration":4.821},{"text":"As you can see, the rectangle-capsule and\nthe triangle-capsule strongly agree on what","start":472.52,"duration":5.6},{"text":"the boat-capsule will output. In other words,\nthey agree that a boat positioned in this","start":478.12,"duration":4.72},{"text":"way would explain their own positions and\nrotations. And they totally disagree on what","start":482.84,"duration":5.32},{"text":"the house-capsule will output. Therefore,\nit makes sense to assume that the rectangle","start":488.16,"duration":4.9},{"text":"and triangle are part of a boat, not a house.","start":493.06,"duration":4.6},{"text":"Now that we know that the rectangle and triangle\nare part of a boat, the outputs of the rectangle","start":497.66,"duration":5.43},{"text":"capsule and the triangle capsule really concern\nonly the boat capsule, there\u2019s no need to","start":503.09,"duration":4.6},{"text":"send these outputs to any other capsule, this\nwould just add noise. They should be sent","start":507.69,"duration":5.05},{"text":"only to the boat capsule.","start":512.74,"duration":2.48},{"text":"This is called routing by agreement. There\nare several benefits: first, since capsule","start":515.22,"duration":5.41},{"text":"outputs are only routed to the appropriate\ncapsule in the next layer, these capsules","start":520.63,"duration":5.14},{"text":"will get a cleaner input signal and will more\naccurately determine the pose of the object.","start":525.77,"duration":6.37},{"text":"Second, by looking at the paths of the activations,\nyou can easily navigate the hierarchy of parts,","start":532.14,"duration":6.36},{"text":"and know exactly which part belongs to which\nobject (like, the rectangle belongs to the","start":538.5,"duration":4.75},{"text":"boat, or the triangle belongs to the boat,\nand so on). Lastly, routing by agreement helps","start":543.25,"duration":6.81},{"text":"parse crowded scenes with overlapping objects\n(we will see this in a few slides). But first,","start":550.06,"duration":5.75},{"text":"let\u2019s look at how routing by agreement is\nimplemented in Capsule Networks.","start":555.81,"duration":5.4},{"text":"Here, I have represented the various poses\nof the boat, as predicted by the lower-level","start":561.21,"duration":6.41},{"text":"capsules. For example, one of these circles\nmay represent what the rectangle-capsule thinks","start":567.62,"duration":5.2},{"text":"about the most likely pose of the boat, and\nanother circle may represent what the triangle-capsule","start":572.82,"duration":5.82},{"text":"thinks, and if we suppose that there are many\nother low-level capsules, then we might get","start":578.64,"duration":5.12},{"text":"a cloud of prediction vectors, for the boat\ncapsule, like this. In this example, there","start":583.76,"duration":5.351},{"text":"are two pose parameters: one represents the\nrotation angle, and the other represents the","start":589.111,"duration":4.149},{"text":"size of the boat. As I mentioned earlier,\npose parameters may capture many different","start":593.26,"duration":4.46},{"text":"kinds of visual features, like skew, thickness,\nand so on. Or precise location. So the first","start":597.72,"duration":5.48},{"text":"thing we do, is we compute the mean of all\nthese predictions. This gives us this vector.","start":603.2,"duration":6.37},{"text":"The next step is to measure the distance between\neach predicted vector and the mean vector.","start":609.57,"duration":6.76},{"text":"I will use here the euclidian distance here,\nbut capsule networks actually use the scalar","start":616.33,"duration":5.34},{"text":"product. Basically, we want to measure how\nmuch each predicted vector agrees with the","start":621.67,"duration":5.38},{"text":"mean predicted vector. Using this agreement\nmeasure, we can update the weight of every","start":627.05,"duration":6.03},{"text":"predicted vector accordingly.","start":633.08,"duration":3.32},{"text":"Note that the predicted vectors that are far\nfrom the mean now have a very small weight,","start":636.4,"duration":5.72},{"text":"and the ones closest to the mean have a much\nstronger weight. I\u2019ve represented them in","start":642.12,"duration":6.15},{"text":"black. Now we can just compute the mean once\nagain (or I should say, the weighted mean),","start":648.27,"duration":5.56},{"text":"and you\u2019ll notice that it moves slightly\ntowards the cluster, towards the center of","start":653.83,"duration":5.01},{"text":"the cluster.","start":658.84,"duration":1.0},{"text":"So next, we can once again update the weights.\nAnd now most of the vectors within the cluster","start":659.84,"duration":7.9},{"text":"have turned black.\nAnd again, we can update the mean.","start":667.74,"duration":4.23},{"text":"And we can repeat this process a few times.\nIn practice 3 to 5 iterations are generally","start":671.97,"duration":5.95},{"text":"sufficient. This might remind you, I suppose,\nof the k-means clustering algorithm if you","start":677.92,"duration":5.46},{"text":"know it. Okay, so this is how we find clusters\nof agreement. Now let\u2019s see how the whole","start":683.38,"duration":6.31},{"text":"algorithm works in a bit more details.","start":689.69,"duration":3.66},{"text":"First, for every predicted output, we start\nby setting a raw routing weight b_i,j equal","start":693.35,"duration":7.49},{"text":"to 0.","start":700.84,"duration":1.29},{"text":"Next, we apply the softmax function to these\nraw weights, for each primary capsule. This","start":702.13,"duration":8.491},{"text":"gives the actual routing weights for each\npredicted output, in this example 0.5 each.","start":710.621,"duration":9.399},{"text":"Next we compute a weighted sum of the predictions,\nfor each capsule in the next layer. This might","start":720.02,"duration":7.11},{"text":"give vectors longer than 1, so as usual we\napply the squash function.","start":727.13,"duration":7.5},{"text":"And voil\u00e0! We now have the actual outputs\nof the house-capsule and boat-capsule. But","start":734.63,"duration":5.72},{"text":"this is not the final output, it\u2019s just\nthe end of the first round, the first iteration.","start":740.35,"duration":6.07},{"text":"Now we can see which predictions were most\naccurate. For example, the rectangle-capsule","start":746.42,"duration":4.98},{"text":"made a great prediction for the boat-capsule\u2019s\noutput. It really matches it pretty closely.","start":751.4,"duration":6.41},{"text":"This is estimated by computing the scalar\nproduct of the predicted output vector \u00fb_j|i","start":757.81,"duration":5.94},{"text":"and the actual product vector v_j. This scalar\nproduct is simply added to the predicted output\u2019s","start":763.75,"duration":7.78},{"text":"raw routing weight, b_i,j. So the weight of\nthis particular predicted output is increased.","start":771.53,"duration":10.62},{"text":"When there is a strong agreement, this scalar\nproduct is large, so good predictions will","start":782.15,"duration":5.16},{"text":"have a higher weight.","start":787.31,"duration":2.34},{"text":"On the other hand, the rectangle-capsule made\na pretty bad prediction for the house-capsule\u2019s","start":789.65,"duration":4.75},{"text":"output, so the scalar product in this case\nwill be quite small, and the raw routing weight","start":794.4,"duration":6.5},{"text":"of this predicted vector will not grow much.","start":800.9,"duration":2.99},{"text":"Next, we update the routing weights by computing\nthe softmax of the raw weights, once again.","start":803.89,"duration":6.5},{"text":"And as you can see, the rectangle-capsule\u2019s\npredicted vector for the boat-capsule now","start":810.39,"duration":4.53},{"text":"has a weight of 0.8, while it\u2019s predicted\nvector for the house-capsule dropped down","start":814.92,"duration":5.89},{"text":"to 0.2. So most of its output is now going\nto go to the boat capsule, not the house capsule.","start":820.81,"duration":8.89},{"text":"Once again we compute the weighted sum of\nall the predicted output vectors for each","start":829.7,"duration":4.15},{"text":"capsule in the next layer, that is the house-capsule\nand the boat-capsule. And this time, the house-capsule","start":833.85,"duration":5.61},{"text":"gets so little input that its output is a\ntiny vector. On the other hand the boat-capsule","start":839.46,"duration":5.44},{"text":"gets so much input that it outputs a vector\nmuch longer than 1. So again we squash it.","start":844.9,"duration":8.05},{"text":"And that\u2019s the end of round #2. And as you\ncan see, in just a couple iterations, we have","start":852.95,"duration":3.75},{"text":"already ruled out the house and clearly chosen\nthe boat. After perhaps one or two more rounds,","start":856.7,"duration":6.91},{"text":"we can stop and proceed to the next capsule\nlayer in exactly the same way.","start":863.61,"duration":5.58},{"text":"So as I mentioned earlier, routing by agreement\nis really great to handle crowded scenes,","start":869.19,"duration":5.08},{"text":"such as the one represented in this image.\nOne way to interpret this image (as you can","start":874.27,"duration":5.66},{"text":"see there is a bit of ambiguity), you can\nsee a house upside down in the middle. However,","start":879.93,"duration":5.99},{"text":"if this was the case, then there would be\nno explanation for the bottom rectangle or","start":885.92,"duration":5.18},{"text":"the top triangle, no reason for them to be\nwhere they are.","start":891.1,"duration":4.58},{"text":"The best way to interpret the image is that\nthere is a house at the top and a boat at","start":895.68,"duration":5.01},{"text":"the bottom. And routing by agreement will\ntend to choose this solution, since it makes","start":900.69,"duration":5.1},{"text":"all the capsules perfectly happy, each of\nthem making perfect predictions for the capsules","start":905.79,"duration":4.51},{"text":"in the next layer. The ambiguity is explained\naway.","start":910.3,"duration":4.51},{"text":"Okay, so what can you do with a capsule network\nnow that you know how it works.","start":914.81,"duration":5.43},{"text":"Well for one, you can create a nice image\nclassifier of course. Just have one capsule","start":920.24,"duration":5.62},{"text":"per class in the top layer and that\u2019s almost\nall there is to it. All you need to add is","start":925.86,"duration":5.67},{"text":"a layer that computes the length of the top-layer\nactivation vectors, and this gives you the","start":931.53,"duration":5.07},{"text":"estimated class probabilities. You could then\njust train the network by minimizing the cross-entropy","start":936.6,"duration":6.14},{"text":"loss, as in a regular classification neural\nnetwork, and you would be done.","start":942.74,"duration":4.84},{"text":"However, in the paper they use a margin loss\nthat makes it possible to detect multiple","start":947.58,"duration":5.14},{"text":"classes in the image. So without going into\ntoo much details, this margin loss is such","start":952.72,"duration":6.72},{"text":"that if an object of class k is present in\nthe image, then the corresponding top-level","start":959.44,"duration":6.149},{"text":"capsule should output a vector whose\nlength is at least 0.9. It should be long.","start":965.589,"duration":6.721},{"text":"Conversely, if an object of class k is not\npresent in the image, then the capsule should","start":972.31,"duration":4.87},{"text":"output a short vector, one whose length\nis shorter than 0.1. So the total loss is","start":977.18,"duration":6.44},{"text":"the sum of losses for all classes.","start":983.62,"duration":3.719},{"text":"In the paper, they also add a decoder network\non top of the capsule network. It\u2019s just","start":987.339,"duration":4.841},{"text":"3 fully connected layers with a sigmoid activation\nfunction in the output layer. It learns to","start":992.18,"duration":7.54},{"text":"reconstruct the input image by minimizing\nthe squared difference between the reconstructed","start":999.72,"duration":5.48},{"text":"image and the input image.","start":1005.2,"duration":3.6},{"text":"The full loss is the margin loss we discussed\nearlier, plus the reconstruction loss (scaled","start":1008.8,"duration":5.92},{"text":"down considerably so as to ensure that the\nmargin loss dominates training). The benefit","start":1014.72,"duration":5.7},{"text":"of applying this reconstruction loss is that\nit forces the network to preserve all the","start":1020.42,"duration":4.41},{"text":"information required to reconstruct the image,\nup to the top layer of the capsule network,","start":1024.83,"duration":6.86},{"text":"its output layer. This constraint acts a bit\nlike a regularizer: it reduces the risk of","start":1031.69,"duration":6.499},{"text":"overfitting and helps generalize to new examples.","start":1038.189,"duration":5.521},{"text":"And that\u2019s it, you know how a capsule network\nworks, and how to train it. Let\u2019s look a","start":1043.71,"duration":5.62},{"text":"little bit at some of the figures in the paper,\nwhich I find interesting.","start":1049.33,"duration":3.55},{"text":"This is figure 1 from the paper, showing a\nfull capsule network for MNIST. You can see","start":1052.88,"duration":5.87},{"text":"the first two regular convolutional layers,\nwhose output is reshaped and squashed to get","start":1058.75,"duration":5.72},{"text":"the activation vectors of the primary capsules.\nAnd these primary capsules are organized in","start":1064.47,"duration":6.27},{"text":"a 6 by 6 grid, with 32 primary capsules in\neach cell of this grid, and each primary capsule","start":1070.74,"duration":7.53},{"text":"outputs an 8-dimensional vector. So this first\nlayer of capsules is fully connected to the","start":1078.27,"duration":6.671},{"text":"10 output capsules, which output 16 dimensional\nvectors. The length of these vectors is used","start":1084.941,"duration":7.669},{"text":"to compute the margin loss, as explained earlier.","start":1092.61,"duration":3.74},{"text":"Now this is figure 2 from the paper. It shows\nthe decoder sitting on top of the capsnet.","start":1096.35,"duration":5.4},{"text":"It is composed of 2 fully connected ReLU layers\nplus a fully connected sigmoid layer which","start":1101.75,"duration":5.059},{"text":"outputs 784 numbers that correspond to the\npixel intensities of the reconstructed image","start":1106.809,"duration":6.861},{"text":"(which is a 28 by 28 pixel image). The squared\ndifference between this reconstructed image","start":1113.67,"duration":5.99},{"text":"and the input image gives the reconstruction\nloss.","start":1119.66,"duration":3.99},{"text":"Right, and this is figure 4 from the paper.\nOne nice thing about capsule networks is that","start":1123.65,"duration":6.63},{"text":"the activation vectors are often interpretable.\nFor example, this image shows the reconstructions","start":1130.28,"duration":6.32},{"text":"that you get when you gradually modify one\nof the 16 dimensions of the top layer capsules\u2019","start":1136.6,"duration":5.29},{"text":"output. You can see that the first dimension\nseems to represent scale and thickness. The","start":1141.89,"duration":6.53},{"text":"fourth dimension represents a localized skew.\nThe fifth represents the width of the digit","start":1148.42,"duration":11.08},{"text":"plus a slight translation to get the exact\nposition. So as you can see, it\u2019s rather","start":1159.5,"duration":4.24},{"text":"clear what most of these parameters do.","start":1163.74,"duration":1.991},{"text":"Okay, to conclude, let\u2019s summarize the pros\nand cons. Capsule networks have reached state","start":1165.731,"duration":6.389},{"text":"of the art accuracy on MNIST. On CIFAR10,\nthey got a bit over 10% error, which is far","start":1172.12,"duration":5.439},{"text":"from state of the art, but it\u2019s similar\nto what was first obtained with other techniques","start":1177.559,"duration":5.031},{"text":"before years of efforts were put into them,\nso it\u2019s still a good start. Capsule networks","start":1182.59,"duration":5.16},{"text":"require less training data. They offer equivariance,\nwhich means that position and pose information","start":1187.75,"duration":5.48},{"text":"are preserved. And this is very promising\nfor image segmentation and object detection.","start":1193.23,"duration":5.53},{"text":"The routing by agreement algorithm is great\nfor crowded scenes. The routing tree also","start":1198.76,"duration":4.73},{"text":"maps the hierarchy of objects parts, so every\npart is assigned to a whole. And it\u2019s rather","start":1203.49,"duration":5.78},{"text":"robust to rotations, translations and other\naffine transformations. The activation vectors","start":1209.27,"duration":6.71},{"text":"somewhat are interpretable. And finally, obviously,\nit\u2019s Hinton\u2019s idea, so don\u2019t bet against","start":1215.98,"duration":5.37},{"text":"it.","start":1221.35,"duration":1.0},{"text":"However, there are a few cons: first, as I\nmentioned the results are not yet state of","start":1222.35,"duration":4.64},{"text":"the art on CIFAR10, even though it\u2019s a good\nstart. Plus, it\u2019s still unclear whether","start":1226.99,"duration":4.14},{"text":"capsule networks can scale to larger images,\nsuch as the ImageNet dataset. What will the","start":1231.13,"duration":5.67},{"text":"accuracy be? Capsule networks are also quite\nslow to train, in large part because of the","start":1236.8,"duration":6.55},{"text":"routing by agreement algorithm which has an\ninner loop, as you saw earlier. Finally, there","start":1243.35,"duration":5.371},{"text":"is only one capsule of any given type in a\ngiven location, so it\u2019s impossible for a","start":1248.721,"duration":5.06},{"text":"capsule network to detect two objects of the\nsame type if they are too close to one another.","start":1253.781,"duration":5.389},{"text":"This is called crowding, and it has been observed\nin human vision as well, so it\u2019s probably","start":1259.17,"duration":4.38},{"text":"not a show-stopper.","start":1263.55,"duration":1.0},{"text":"All right! I highly recommend you take a look\nat the code of a CapsNet implementation, such","start":1264.55,"duration":6.31},{"text":"as the ones listed here (I\u2019ll leave the\nlinks in the video description below). If","start":1270.86,"duration":4.471},{"text":"you take your time, you should have no problem\nunderstanding everything the code is doing.","start":1275.331,"duration":3.939},{"text":"The main difficulty in implementing CapsNets\nis that it contains an inner loop for the","start":1279.27,"duration":4.63},{"text":"routing by agreement algorithm. Implementing\nloops in Keras and TensorFlow can be a little","start":1283.9,"duration":4.99},{"text":"bit trickier than in PyTorch, but it can be\ndone. If you don\u2019t have a particular preference,","start":1288.89,"duration":6.26},{"text":"then I would say that the PyTorch code is\nthe easiest to understand.","start":1295.15,"duration":4.85},{"text":"And that\u2019s all I had, I hope you enjoyed\nthis video. If you did, please thumbs up,","start":1300.0,"duration":4.33},{"text":"share, comment, subscribe, blablabla. It\u2019s\nmy first real YouTube video, and if people","start":1304.33,"duration":4.53},{"text":"find it useful, I might make some more. If\nyou want to learn more about Machine Learning,","start":1308.86,"duration":5.04},{"text":"Deep Learning and Deep Reinforcement Learning,\nyou may want to read my O\u2019Reilly book Hands-on","start":1313.9,"duration":4.57},{"text":"Machine Learning with Scikit-Learn and TensorFlow.\nIt covers a ton of topics, with many code","start":1318.47,"duration":5.64},{"text":"examples that you will find on my github account,\nso I\u2019ll leave the links in the video description.","start":1324.11,"duration":5.97},{"text":"That\u2019s all for today, have fun and see you\nnext time!","start":1330.08,"duration":3.1}],"2Kawrd5szHE":[{"text":"Hi, I\u2019m Aur\u00e9lien G\u00e9ron, and today I\u2019m\ngoing to show you how to implement a capsule","start":1.18,"duration":4.239},{"text":"network using TensorFlow.","start":5.419,"duration":2.06},{"text":"In my previous video, I presented the key\nideas behind capsule networks, a recently","start":7.479,"duration":4.381},{"text":"published neural net architecture. If you\nhaven\u2019t seen this video, I encourage you","start":11.86,"duration":3.92},{"text":"to do so now, today I will focus on the TensorFlow\nimplementation.","start":15.78,"duration":4.98},{"text":"I wrote a Jupyter notebook containing all\nthe code and detailed explanations, and I","start":20.76,"duration":4.86},{"text":"published it on my github account (as always\nI\u2019ll put all the links in the video description","start":25.62,"duration":5.159},{"text":"below), so I encourage you to clone it, and\nplay with it.","start":30.779,"duration":3.291},{"text":"So, it reaches over 99.4% accuracy on the\ntest set, which is pretty good, considering","start":34.07,"duration":5.76},{"text":"it\u2019s a shallow network with just two capsule\nlayers and a total of about 1,200 capsules.","start":39.83,"duration":5.7},{"text":"There\u2019s a lot of code in this notebook,\nso I won\u2019t go through every single line","start":45.53,"duration":5.17},{"text":"in this video, but I\u2019ll explain the main\ndifficulties I came across, and hopefully","start":50.7,"duration":3.96},{"text":"this will be useful to you for other TensorFlow\nimplementations, not just CapsNets.","start":54.66,"duration":4.489},{"text":"Okay, let\u2019s build the network. First, we\nneed to feed the input images to the network.","start":59.149,"duration":6.451},{"text":"And that\u2019s our input layer.","start":65.6,"duration":1.9},{"text":"We implement it using a simple TensorFlow\nplaceholder. The batch size is unspecified,","start":67.5,"duration":5.61},{"text":"so that we can pass any number of images in\neach batch, in this example, 5. Note that","start":73.11,"duration":5.9},{"text":"we directly send 28x28 pixel images, with\na single channel, since the images are greyscale.","start":79.01,"duration":6.35},{"text":"Color images would typically have 3 channels,\nfor red, green and blue.","start":85.36,"duration":4.15},{"text":"And that\u2019s it for the input layer.","start":89.51,"duration":1.84},{"text":"Next, let\u2019s build the primary capsule layer.\nFor each digit in the batch it will output","start":91.35,"duration":6.33},{"text":"32 maps, each containing a 6x6 grid of 8 dimensional\nvectors.","start":97.68,"duration":5.56},{"text":"The capsules in this particular map seem to\ndetect the start of a line segment. You can","start":103.24,"duration":5.29},{"text":"see that the output vectors are long in the\nlocations where there\u2019s a start of a line.","start":108.53,"duration":5.5},{"text":"And the orientation of the 8D vector gives\nthe pose parameters, in this case, I\u2019ve","start":114.03,"duration":5.92},{"text":"represented the rotation angle, but the vector\u2019s\n8 dimensional orientation would also capture","start":119.95,"duration":5.09},{"text":"things like the thickness of the line, the\nprecise location of the start of the line","start":125.04,"duration":4.789},{"text":"relative to the cell in the 6x6 grid, and\nso on.","start":129.829,"duration":4.811},{"text":"The implementation is really straightforward.\nFirst, we define two regular convolutional","start":134.64,"duration":4.819},{"text":"layers. The input of the first layer is X,\nthe placeholder that will contain the input","start":139.459,"duration":5.86},{"text":"images we will feed at runtime. The second\nlayer takes the output of the first layer.","start":145.319,"duration":6.78},{"text":"And we use the parameters specified in the\npaper. The second layer is configured to output","start":152.099,"duration":5.081},{"text":"256 feature maps. And each feature map contains\na 6x6 grid of scalars. We want a 6x6 grid","start":157.18,"duration":8.429},{"text":"of vectors instead, so we use TensorFlow\u2019s\nreshape() function to get 32 maps of 8 dimensional","start":165.609,"duration":6.16},{"text":"vectors, instead of 256 maps of scalars. In\nfact, since the primary capsules will be fully","start":171.769,"duration":6.23},{"text":"connected to the digit capsules, we can simply\nreshape to one long list of 1,152 output vectors","start":177.999,"duration":8.32},{"text":"(that\u2019s 32*6*6), for each instance in the\nbatch. And the last step is to squash the","start":186.319,"duration":7.18},{"text":"vectors to ensure that their length is always\nbetween 0 and 1. For this, we use a home made","start":193.499,"duration":6.931},{"text":"squash function. There it is.","start":200.43,"duration":2.059},{"text":"This function implements the squash equation\ngiven in the paper.","start":202.489,"duration":4.661},{"text":"It squashes every vector in an array, along\nthe specified dimension, by default the last","start":207.15,"duration":4.8},{"text":"one.","start":211.95,"duration":1.0},{"text":"So, as you can see, it involves a division\nby the norm of the vector, so there\u2019s a","start":212.95,"duration":4.719},{"text":"risk of a division by zero if at least one\nof the vectors is a zero vector.","start":217.669,"duration":4.91},{"text":"So you could just add a tiny epsilon value\nin the denominator, and it would fix the division","start":222.579,"duration":5.0},{"text":"by zero problem. However you would still run\ninto another issue. The norm of a vector has","start":227.579,"duration":6.48},{"text":"no defined gradients when the vector is zero.\nSo if you just use tensorflow\u2019s norm() function","start":234.059,"duration":5.951},{"text":"to compute the norm in this equation, then\nif at least one of the vectors is zero, the","start":240.01,"duration":5.14},{"text":"gradients will be undefined (it will return\nn-a-n, nan, not a number). So, as a result,","start":245.15,"duration":6.69},{"text":"when gradient descent updates the weights\nof our model, the weights will end up being","start":251.84,"duration":5.03},{"text":"undefined as well. The model would effectively\nbe dead. You don\u2019t want that.","start":256.87,"duration":4.829},{"text":"So the trick is to compute a safe approximation\nof the norm, shown in the equation on the","start":261.699,"duration":5.571},{"text":"right.","start":267.27,"duration":1.03},{"text":"And, that's about it, that\u2019s all for the\nprimary capsules. Apart for computing the","start":268.3,"duration":5.149},{"text":"norm safely, it was pretty straightforward.","start":273.449,"duration":4.211},{"text":"On to the next layer where all the complexity\nis: the digit capsules. There are just 10","start":277.66,"duration":5.539},{"text":"of them, one for each digit, 0 to 9, and they\noutput 16 dimensional vectors. In this particular","start":283.199,"duration":6.981},{"text":"example, you can see that the longest output\nvector is the one for digit 4. And again its","start":290.18,"duration":6.15},{"text":"orientation in the 16 dimensional space gives\ninformation about the pose of this digit,","start":296.33,"duration":5.39},{"text":"its rotation, its thickness, its skew, its\nposition, and so on. By the way, note that","start":301.72,"duration":4.969},{"text":"most of the position information in the first\nlayer was encoded in the location of the active","start":306.689,"duration":5.91},{"text":"capsules in the 6x6 grid. So, for example,\nif I shift the digit 4 slightly to the left","start":312.599,"duration":6.79},{"text":"in the input image then different capsules\nin the first layer get activated. See? So,","start":319.389,"duration":7.731},{"text":"the output of these first layer capsules only\ncontain local shift information, relative","start":327.12,"duration":4.84},{"text":"to the position of the capsule in the 6x6\ngrid. But in the second capsule layer, the","start":331.96,"duration":5.979},{"text":"full position information is now encoded in\nthe orientation of the output vector in 16","start":337.939,"duration":6.16},{"text":"dimensional space.","start":344.099,"duration":1.081},{"text":"Okay, now let\u2019s see how to implement this\nlayer. The first step is to compute the predicted","start":345.18,"duration":5.609},{"text":"output vectors. Since this second layer is\nfully connected to the first layer, we will","start":350.789,"duration":5.72},{"text":"compute one predicted output for each pair\nof first and second layer capsules.","start":356.509,"duration":5.571},{"text":"For example, using the output of the first\nprimary capsule, we can predict the output","start":362.08,"duration":5.329},{"text":"vector of the first digit capsule.","start":367.409,"duration":2.421},{"text":"For this, we just use a transformation matrix\nW_1,1, which will gradually be learned during","start":369.83,"duration":6.11},{"text":"training, and we multiply it by the output\nof the first layer capsule. This gives us","start":375.94,"duration":5.79},{"text":"\u00fb_1|1, which is the predicted output of the\nfirst digit capsule, based on the output of","start":381.73,"duration":6.399},{"text":"the first primary capsule. Since the primary\ncapsules output 8 dimensional vectors, and","start":388.129,"duration":5.921},{"text":"the digit capsules output 16 dimensional vectors,\nthe transformation matrix W_1,1 must be a","start":394.05,"duration":7.17},{"text":"16x8 matrix.","start":401.22,"duration":2.11},{"text":"Next, we try to predict the output of the\nsecond digit capsule, still based on the output","start":403.33,"duration":5.88},{"text":"of the first primary capsule. Note that we\nare using a different transformation matrix,","start":409.21,"duration":6.459},{"text":"W_1,2.","start":415.669,"duration":1.571},{"text":"And we do the same for the third digit capsule,\nusing W_1,3.","start":417.24,"duration":4.549},{"text":"And so on for all the digit capsules.","start":421.789,"duration":3.271},{"text":"Then we move on to the second primary capsule,\nand we use its output to predict the output","start":425.06,"duration":5.099},{"text":"of the first digit capsule.","start":430.159,"duration":2.271},{"text":"And so on for all the digit capsules.","start":432.43,"duration":3.109},{"text":"Then we move on to the third primary capsule,\nwe make 10 predictions.","start":435.539,"duration":3.81},{"text":"And so on, you get the picture. There are\n1,152 primary capsules (multiply 6 * 6 * 32),","start":439.349,"duration":9.611},{"text":"and 10 digit capsules, so we end up with 11,520\npredicted output vectors. Now we could just","start":448.96,"duration":7.4},{"text":"compute them one by one, but it would be terribly\ninefficient. Let\u2019s see how we can get all","start":456.36,"duration":5.429},{"text":"the predicted output vectors in just one matmul()\noperation. Now you know that TensorFlow\u2019s","start":461.789,"duration":6.011},{"text":"matmul() function lets you multiply two matrices,\nbut you may not know that you can also use","start":467.8,"duration":5.32},{"text":"it to multiply many matrices in one shot.\nThis will be incredibly efficient, especially","start":473.12,"duration":4.93},{"text":"if you are using a GPU card, because it will\nperform all the matrix multiplications in","start":478.05,"duration":5.549},{"text":"parallel in many different GPU threads.","start":483.599,"duration":2.811},{"text":"So here\u2019s how it works. Suppose A, B, C,\nD, E, F and G, H, I, J, K, L, are all matrices.","start":486.41,"duration":7.409},{"text":"You can put these matrices in two arrays,\neach with 2 rows and 3 columns, for example.","start":493.819,"duration":5.991},{"text":"So we have 2 dimensions for this 2x3 grid\nof matrices, and each matrix is 2 dimensional,","start":499.81,"duration":6.669},{"text":"so these arrays are 2+2=4 dimensional arrays.\nIf you pass these arrays, these 4D arrays,","start":506.479,"duration":6.761},{"text":"to matmul(), it will perform an elementwise\nmatrix multiplication, so the result will","start":513.24,"duration":6.02},{"text":"be this 4 dimensional array containing A multiplied\nby G, here, B multiplied by H, here, and so","start":519.26,"duration":9.819},{"text":"on. So let\u2019s use this to compute all the\npredicted output vectors.","start":529.079,"duration":5.801},{"text":"We can create a first 4D array containing\nall the transformation matrices: there\u2019s","start":534.88,"duration":5.97},{"text":"one row per primary capsule, and one column\nper digit capsule. The second array must contain","start":540.85,"duration":6.88},{"text":"the output vectors of each primary capsule.\nThen we just pass these two arrays to the","start":547.73,"duration":6.59},{"text":"matmul() function, and it gives us the predicted\noutput vectors for all the pairs of primary","start":554.32,"duration":5.43},{"text":"and digit capsules.","start":559.75,"duration":3.12},{"text":"Since we need to predict the outputs of all\n10 digit capsules for each primary capsule,","start":562.87,"duration":5.63},{"text":"this array must contain 10 copies of the primary\ncapsules\u2019 outputs. We will use the tile","start":568.5,"duration":6.29},{"text":"function to replicate the first column of\noutput vectors, 10 times.","start":574.79,"duration":5.81},{"text":"But there\u2019s one additional catch. We want\nto make these predictions for all the instances","start":580.6,"duration":5.72},{"text":"in the batch, not just one instance. So there\u2019s\nan additional dimension for the batch size.","start":586.32,"duration":5.89},{"text":"It turns out that the primary output vectors\nwere already computed for every single instance,","start":592.21,"duration":5.97},{"text":"so the second array is fine. It already has\nthis dimension.","start":598.18,"duration":5.19},{"text":"But we need to replicate the 4D array containing\nall the transformation matrices, so that we","start":603.37,"duration":5.68},{"text":"end up with one copy per instance in the batch.\nNow if you understand this, then the code","start":609.05,"duration":6.089},{"text":"should be pretty clear.","start":615.139,"duration":1.711},{"text":"First, we create a variable containing all\nthe transformation matrices. It has one row","start":616.85,"duration":5.95},{"text":"per primary capsule, one column per digit\ncapsule, and it contains 16 by 8 matrices.","start":622.8,"duration":8.18},{"text":"That\u2019s 4 dimensions, and we add another\ndimension at the beginning of size one at","start":630.98,"duration":6.21},{"text":"the beginning to make it easy to tile this\narray for each instance in the batch. The","start":637.19,"duration":4.5},{"text":"variable is initialized randomly, using a\nnormal distribution of standard deviation","start":641.69,"duration":5.19},{"text":"0.01 (that\u2019s a hyperparameter you can tweak).\nAnd that's about it. Create this variable!","start":646.88,"duration":8.519},{"text":"Next we want to tile this array for each instance,\nso first we need to know the batch size. We","start":655.399,"duration":5.751},{"text":"don\u2019t actually know it at graph construction\ntime, it will only be known when we run the","start":661.15,"duration":4.65},{"text":"graph. But we can use TensorFlow\u2019s shape()\nfunction: it creates a tensor that *will*","start":665.8,"duration":4.83},{"text":"know the shape at runtime, and we grab its\nfirst dimension, which is the batch size.","start":670.63,"duration":6.519},{"text":"Then we simply tile our big W array, along\nthe first dimension, to get one copy per instance.","start":677.149,"duration":6.411},{"text":"Now, recall that the output of the primary\ncapsules was a 3 dimensional array: the first","start":683.56,"duration":6.26},{"text":"dimension is the batch size, that we will\nknow at runtime, then there\u2019s one row per","start":689.82,"duration":5.629},{"text":"capsule, and each capsule has 8 dimensions.\nSo we need to reshape this array a bit to","start":695.449,"duration":4.981},{"text":"get the shape that we are looking for, to\ndo the big matmul() operation.","start":700.43,"duration":6.55},{"text":"First we add an extra dimension at the end,\nusing TensorFlow\u2019s expand_dims() function.","start":706.98,"duration":5.45},{"text":"The vectors are now represented as column\nvectors, instead of 1 dimensional arrays.","start":712.43,"duration":5.48},{"text":"Each of these is a column vector. A column\nvector is a matrix, a 2D array, with a single","start":717.91,"duration":5.05},{"text":"column.","start":722.96,"duration":1.71},{"text":"Then we add another dimension, for the digit\ncapsules.","start":724.67,"duration":4.38},{"text":"And we replicate all the output vectors 10\ntimes across this new dimension, once per","start":729.05,"duration":4.83},{"text":"digit capsule.","start":733.88,"duration":3.22},{"text":"And lastly, we just use matmul to multiply\nthe transformation matrices with the primary","start":737.1,"duration":5.02},{"text":"capsules\u2019 output vectors, and we get all\nthe digit capsule\u2019s predicted outputs for","start":742.12,"duration":4.93},{"text":"each pair of primary and digit capsules, and\nfor each instance in the batch. In one shot.","start":747.05,"duration":6.59},{"text":"And that\u2019s the end of the first step for\ncomputing the digit capsules\u2019 outputs, we","start":753.64,"duration":4.569},{"text":"now have a bunch of predicted output vectors.\nThe second step is the routing by agreement","start":758.209,"duration":5.771},{"text":"algorithm.","start":763.98,"duration":1.469},{"text":"So first, we set all the raw routing weights\nto 0. For this, we just use tf.zeros(). There\u2019s","start":765.449,"duration":6.531},{"text":"one weight for each pair of primary and digits\ncapsules, for each instance. The last two","start":771.98,"duration":5.85},{"text":"dimensions here are equal to 1, they will\nbe useful in a minute.","start":777.83,"duration":5.369},{"text":"Next we compute the softmax of each primary\ncapsule\u2019s 10 raw routing weights. Okay?","start":783.199,"duration":6.281},{"text":"So softmax happens along this dimension.","start":789.48,"duration":3.979},{"text":"Next, we compute the weighted sum of all the\npredicted output vectors, for each digit capsule,","start":793.459,"duration":7.12},{"text":"using the routing weights. The weighted sum\nis along this dimension.","start":800.579,"duration":4.161},{"text":"This is pretty straightforward TensorFlow\ncode: first multiply the routing weights and","start":804.74,"duration":5.339},{"text":"the predicted vectors (this is an elementwise\nmultiplication, not a matrix multiplication),","start":810.079,"duration":5.831},{"text":"then just compute the sum over the primary\ncapsule dimension. And the two dimensions","start":815.91,"duration":4.78},{"text":"we added earlier for the routing weights are\nuseful in the multiplication step, so that","start":820.69,"duration":4.66},{"text":"the two arrays have the same number of dimensions,\nthe same rank. They don\u2019t have the exact","start":825.35,"duration":5.76},{"text":"same shape, but they have compatible shapes\nso TensorFlow will perform broadcasting.","start":831.11,"duration":5.969},{"text":"Now if you don't know what broadcasting is,\nthis should make it clear. I\u2019m multiplying","start":837.079,"duration":5.291},{"text":"two matrices, but one of them just has one\nrow, so TensorFlow will act as if this row","start":842.37,"duration":5.139},{"text":"were repeated the appropriate number of times.\nYou could achieve the same thing using tiling,","start":847.509,"duration":5.401},{"text":"as we did earlier, but this is more efficient.\nYou may wonder why we didn\u2019t use broadcasting","start":852.91,"duration":5.88},{"text":"earlier, but the reason is it does not work\nfor matrix multiplication. Here we are doing","start":858.79,"duration":4.56},{"text":"elementwise multiplication.","start":863.35,"duration":2.37},{"text":"Okay, back to the digit capsules. We computed\na weighted sum of the predicted vectors, for","start":865.72,"duration":6.0},{"text":"each digit capsule, and we just run the squash\nfunction, and we get the outputs of the digit","start":871.72,"duration":7.08},{"text":"capsules. Hurray! But wait, this is just the\nend of round 1 of the routing by agreement","start":878.8,"duration":6.45},{"text":"algorithm. Now, on to round #2.","start":885.25,"duration":3.34},{"text":"So first, we need to measure how good each\nprediction was, and use this to update the","start":888.59,"duration":5.29},{"text":"routing weights.","start":893.88,"duration":1.0},{"text":"For example, look at the predictions that\nwe made using this primary capsule\u2019s output.","start":894.88,"duration":6.49},{"text":"Notice that, for example, the prediction for\ndigit 4, is excellent.","start":901.37,"duration":4.579},{"text":"And this is measured using the scalar product\nof the predicted output vector and the actual","start":905.949,"duration":4.63},{"text":"output vector.","start":910.579,"duration":2.01},{"text":"These two vectors are actually represented\nas column vectors, meaning a matrix with a","start":912.589,"duration":4.411},{"text":"single column. So to compute the scalar product,\nwe must transpose the predicted column vector","start":917.0,"duration":6.44},{"text":"\u00fb_j|i to get a row vector, and multiply this\nrow vector and the actual output vector v_j,","start":923.44,"duration":9.069},{"text":"which is a column vector. We will get a 1x1\nmatrix containing the scalar product of the","start":932.509,"duration":4.841},{"text":"vectors. Now of course we need to do this\nfor each predicted vector, so once again,","start":937.35,"duration":5.679},{"text":"we can use the matmul() function to perform\nall the matrix multiplications in just one","start":943.029,"duration":4.93},{"text":"shot.","start":947.959,"duration":1.511},{"text":"First we must use the tile() function to get\none copy of the actual output vectors v_j,","start":949.47,"duration":5.45},{"text":"for each primary capsule. Then we use matmul(),\ntelling it to transpose each matrix in the","start":954.92,"duration":7.22},{"text":"first array, on the fly, and lo-and-behold,\nwe get all the scalar products at once. So","start":962.14,"duration":6.77},{"text":"now, we have a measure of the agreement between\neach predicted vector and the actual output","start":968.91,"duration":5.989},{"text":"vector.","start":974.899,"duration":1.06},{"text":"We can then add these scalar products to the\nraw weights, using a simple addition.","start":975.959,"duration":6.07},{"text":"And the rest of round 2 is exactly the same\nas the end of round 1. The code is really","start":982.029,"duration":4.5},{"text":"identical, except we\u2019re now using the raw\nrouting weights of round 2. We compute their","start":986.529,"duration":5.06},{"text":"softmax to get the actual routing weights\nfor round 2, then we compute the weighted","start":991.589,"duration":4.31},{"text":"sum of all the predicted vectors for each\ndigit capsule, and finally we squash the result.","start":995.899,"duration":6.071},{"text":"And now we have the new digit capsule outputs,\nand we\u2019ve finished round 2. We could do","start":1001.97,"duration":4.38},{"text":"a few more rounds exactly like this one, but\nI\u2019ll stop now, and use the current output","start":1006.35,"duration":5.96},{"text":"vectors at the end of round 2 as the output\nof the digit capsules.","start":1012.31,"duration":5.269},{"text":"Now you probably noticed that I implemented\nthe routing algorithm\u2019s loop without an","start":1017.579,"duration":4.031},{"text":"actual loop. It\u2019s a bit like computing the\nsum of squares from 1 to 100, with this code.","start":1021.61,"duration":5.51},{"text":"Of course, this will build a very big TensorFlow\ngraph. But it works. You can think of it as","start":1027.12,"duration":6.09},{"text":"an unrolled loop. Now, a cleaner way to do\nthis would be to write a for loop in Python,","start":1033.21,"duration":5.73},{"text":"like this.","start":1038.94,"duration":1.0},{"text":"Ah, much better. However, it\u2019s important\nto understand that the resulting TensorFlow","start":1039.94,"duration":4.77},{"text":"graph will be absolutely identical to the\none produced by the previous code. All we","start":1044.71,"duration":5.29},{"text":"are doing here is constructing a graph, and\nTensorFlow will not even know that we used","start":1050.0,"duration":4.75},{"text":"a loop to build the graph. Again, this works\nfine, it\u2019s just that you end up with a very","start":1054.75,"duration":5.12},{"text":"large graph. So you can think of this loop\nas a static loop, that only runs at graph","start":1059.87,"duration":5.48},{"text":"construction time. If you want a dynamic loop,\none that TensorFlow itself will run, then","start":1065.35,"duration":5.501},{"text":"you must use TensorFlow\u2019s while_loop() function\nlike this.","start":1070.851,"duration":4.129},{"text":"The while_loop() function takes 3 parameters:\nthe first one is a function that must return","start":1074.98,"duration":6.68},{"text":"a tensor that will determine whether the loop\nshould go on or not, at each iteration. The","start":1081.66,"duration":6.541},{"text":"second parameter is also a function that must\nbuild the body of the loop, that will also","start":1088.201,"duration":6.069},{"text":"be evaluated at each iteration. And finally,\nthe third parameter contains a list of tensors","start":1094.27,"duration":5.65},{"text":"that will be sent to both the condition()\nand loop_body() functions at the first iteration.","start":1099.92,"duration":6.35},{"text":"For the following iterations, these functions\nwill receive the output of the loop_body()","start":1106.27,"duration":4.74},{"text":"function. So you can pause the video if you\nneed to take a closer look at this code. Once","start":1111.01,"duration":4.981},{"text":"you get it, you can try modifying my capsnet\nimplementation to use a dynamic loop rather","start":1115.991,"duration":4.849},{"text":"than a static unrolled loop. Apart from making\nthe code cleaner and the graph smaller, using","start":1120.84,"duration":6.46},{"text":"a dynamic loop allows you to change the number\nof iterations using the exact same model.","start":1127.3,"duration":5.56},{"text":"Also, if you set the swap_memory parameter\nof the while_loop() function, if you set it","start":1132.86,"duration":5.91},{"text":"to True, TensorFlow will automatically swap\nthe GPU memory to CPU memory to save GPU memory.","start":1138.77,"duration":8.07},{"text":"Since CPU RAM is much cheaper and abundant,\nthis can really be useful.","start":1146.84,"duration":6.0},{"text":"And that\u2019s it! We\u2019ve computed the output\nof the digit capsules. Cool! Now the length","start":1152.84,"duration":5.4},{"text":"of each output vector represents the probability\nthat a digit of that class is present in the","start":1158.24,"duration":6.54},{"text":"image.","start":1164.78,"duration":1.0},{"text":"So, let\u2019s compute these probabilities. For\nthis, we can\u2019t use tensorflow\u2019s norm()","start":1165.78,"duration":6.42},{"text":"function because training will explode if\nthere\u2019s a zero vector at any point, as I","start":1172.2,"duration":4.72},{"text":"mentionned earlier. So instead we use a home-made\nsafe_norm() function, similar to what we did","start":1176.92,"duration":6.36},{"text":"with the squash() function. And note that\nthe sum of the probabilities don\u2019t necessarily","start":1183.28,"duration":5.77},{"text":"add up to 1, because we are not using a softmax\nlayer. This makes it possible to detect multiple","start":1189.05,"duration":6.76},{"text":"different digits in the same image (but they\nall have to be different digits: you can detect","start":1195.81,"duration":5.69},{"text":"a 5 and 3, but you can\u2019t detect, say, two\n5s).","start":1201.5,"duration":4.74},{"text":"Next, let\u2019s predict the most likely digit.","start":1206.24,"duration":4.28},{"text":"We just use the argmax() function that gives\nuse the index of the highest probability.","start":1210.52,"duration":5.5},{"text":"The index happens to be the number, the digit\nitself. Note that we first get a tensor that","start":1216.02,"duration":6.41},{"text":"has a couple extra dimensions of size 1 at\nthe end, so we get rid of them using the squeeze()","start":1222.43,"duration":6.13},{"text":"function. If we called squeeze() without specifying\nthe axes to remove, it would remove all dimensions","start":1228.56,"duration":7.27},{"text":"of size 1. This would generally be okay, except\nif the batch size was equal to one, in which","start":1235.83,"duration":6.85},{"text":"case, we would be left with a scalar value,\nrather than an array, and we don\u2019t want","start":1242.68,"duration":4.74},{"text":"that. So it's better to specify the axes.","start":1247.42,"duration":4.0},{"text":"Great, now we have a capsule network that\ncan estimate class probabilities and make","start":1251.42,"duration":5.59},{"text":"predictions.","start":1257.01,"duration":1.37},{"text":"We can measure the model\u2019s accuracy on the\nbatch by simply comparing the predictions","start":1258.38,"duration":4.68},{"text":"and the labels. In this case the prediction\nfor the last digit in the batch is wrong,","start":1263.06,"duration":4.28},{"text":"it\u2019s 7 instead of 1. So we get 80% accuracy.","start":1267.34,"duration":5.26},{"text":"The code is really straightforward: we just\nuse the equal() function to compare the labels","start":1272.6,"duration":3.82},{"text":"and the predictions y_pred, and this gives\nus an array of booleans, so we cast these","start":1276.42,"duration":5.87},{"text":"booleans to floats, which gives us a bunch\nof 0s (for bad predictions) and 1s (for good","start":1282.29,"duration":5.94},{"text":"predictions), and we compute the mean to get\nthe batch\u2019s accuracy. The labels y are just","start":1288.23,"duration":6.45},{"text":"a regular placeholder. Nothing special.","start":1294.68,"duration":3.71},{"text":"And that\u2019s it, we have a full model, able\nto make predictions. Now let\u2019s look at the","start":1298.39,"duration":4.07},{"text":"training code. This diagram is about to get\npretty crowded so I\u2019ll remove the accuracy","start":1302.46,"duration":6.11},{"text":"for clarity.","start":1308.57,"duration":1.98},{"text":"And now, first, we want to compute the margin\nloss.","start":1310.55,"duration":3.12},{"text":"It\u2019s given by this equation. By the way,\nI made a mistake in my first video: I squared","start":1313.67,"duration":5.19},{"text":"the norms instead of the max operations. Sorry\nabout that. This here is the correct equation.","start":1318.86,"duration":7.94},{"text":"Computing it is pretty straightforward, so\nI won\u2019t go through it in details. The only","start":1326.8,"duration":3.83},{"text":"trick is to understand how you can easily\ncompute all the T_k values. For a given instance,","start":1330.63,"duration":5.88},{"text":"T_k is equal to 1 if a digit of class k is\npresent in the image, otherwise it\u2019s equal","start":1336.51,"duration":4.9},{"text":"to 0.","start":1341.41,"duration":1.0},{"text":"So, you can get all the T_k values for each\ninstance by simply converting the labels to","start":1342.41,"duration":5.26},{"text":"a one-hot representation. For example, if\nan instance\u2019s label is 3, then for this","start":1347.67,"duration":6.08},{"text":"instance, T will contain a 10 dimensional\nvector full of zeros except for a 1 at index","start":1353.75,"duration":5.05},{"text":"3. Okay! Next, we want to compute the reconstruction\nloss.","start":1358.8,"duration":4.73},{"text":"So first, we must send the outputs of the\ndigit capsules to a decoder that will try","start":1363.53,"duration":4.87},{"text":"to use them to reconstruct the input images.","start":1368.4,"duration":3.57},{"text":"This decoder is just a regular feedforward\nneural net composed of 3 fully connected layers.","start":1371.97,"duration":5.43},{"text":"It\u2019s really simple code, so you can pause\nthe video if you want to take a close look","start":1377.4,"duration":4.72},{"text":"at it. It outputs an array containing 784\nvalues from 0 to 1, for each instance, representing","start":1382.12,"duration":7.3},{"text":"the pixel intensities of 28x28 pixel images.","start":1389.42,"duration":5.55},{"text":"And that\u2019s it, we have our reconstructed\nimages! We can now compute the reconstruction","start":1394.97,"duration":4.8},{"text":"loss.","start":1399.77,"duration":1.15},{"text":"This is just the squared difference between\nthe input images and their reconstructions.","start":1400.92,"duration":5.16},{"text":"Since the input images are 28x28x1, we first\nreshape them to one dimension per instance","start":1406.08,"duration":6.03},{"text":"with 784 values each. Then we compute the\nsquared difference.","start":1412.11,"duration":5.65},{"text":"Now we can compute the final loss!","start":1417.76,"duration":2.28},{"text":"It\u2019s just the sum of the margin loss and\nthe reconstruction loss, scaled down to let","start":1420.04,"duration":5.71},{"text":"the margin loss dominate training. Pretty\nsimple, as you can see.","start":1425.75,"duration":3.56},{"text":"Now let\u2019s add the training operation.","start":1429.31,"duration":2.66},{"text":"The paper mentions they used TensorFlow\u2019s\nimplementation of the Adam optimizer, using","start":1431.97,"duration":4.71},{"text":"the default parameters, so let\u2019s do that.","start":1436.68,"duration":2.41},{"text":"We create the optimizer, and call its minimize()\nmethod to get the training operation that","start":1439.09,"duration":4.66},{"text":"will tweak the model parameters to minimize\nthe loss. We\u2019re almost done, but there\u2019s","start":1443.75,"duration":4.89},{"text":"one last detail I didn\u2019t mention in the\nfirst video. The paper indicates that the","start":1448.64,"duration":4.84},{"text":"outputs of the digit capsules should all be\nmasked out except for the ones corresponding","start":1453.48,"duration":4.94},{"text":"to the target digit.","start":1458.42,"duration":1.55},{"text":"So instead of sending the digit capsules\u2019\noutputs directly to the decoder, we want to","start":1459.97,"duration":5.56},{"text":"apply a mask first, like this.","start":1465.53,"duration":3.13},{"text":"The mask will have the same shape as the digit\ncapsules output array, and it will be equal","start":1468.66,"duration":5.75},{"text":"to 0 everywhere except for 1s at the location\nof the target digits.","start":1474.41,"duration":6.09},{"text":"By multiplying the digit capsules\u2019 output\nand the mask, we get the input to the decoder.","start":1480.5,"duration":5.6},{"text":"But there\u2019s one catch. This picture is good\nfor training, but at test time, we won\u2019t","start":1486.1,"duration":5.89},{"text":"have the labels. So instead, we will mask\nthe output vectors using the predicted classes","start":1491.99,"duration":6.19},{"text":"rather than the labels, like this.","start":1498.18,"duration":3.32},{"text":"Now we could build a different graph for training\nand for testing, but it wouldn\u2019t be very","start":1501.5,"duration":5.27},{"text":"convenient. So instead, let\u2019s build a condition\noperation.","start":1506.77,"duration":3.81},{"text":"We will add a boolean placeholder called mask_with_labels.\nIf it is true, then we use the labels to build","start":1510.58,"duration":6.58},{"text":"the mask.","start":1517.16,"duration":1.0},{"text":"If it is False, then we use the prediction.\nNote the difference. Okay.","start":1518.16,"duration":5.65},{"text":"And here\u2019s the code. We build the mask_with_labels\nplaceholder, which will default to False so","start":1523.81,"duration":4.97},{"text":"that we only need to set it during training.\nAnd then we define the reconstruction targets","start":1528.78,"duration":4.73},{"text":"using TensorFlow\u2019s cond() function. It takes\n3 arguments: the first one is a tensor representing","start":1533.51,"duration":7.53},{"text":"the condition, in this case simply the mask_with_labels\nplaceholder. The second parameter is a function","start":1541.04,"duration":5.61},{"text":"that returns the tensor to use if the condition\nis True, and the third parameter is a function","start":1546.65,"duration":5.59},{"text":"that returns the tensor to use if the condition\nif False. Then to build the mask, we simply","start":1552.24,"duration":5.22},{"text":"use the one_hot() function. Now there actually\none slight problem with this implementation,","start":1557.46,"duration":6.26},{"text":"and to explain it, I need to step back for\na second and talk about how TensorFlow evaluates","start":1563.72,"duration":5.45},{"text":"a tensor.","start":1569.17,"duration":2.08},{"text":"Suppose we built this graph, these are all\ntensorflow operations, and we want to evaluate","start":1571.25,"duration":5.77},{"text":"the output of operation A.","start":1577.02,"duration":2.58},{"text":"The first thing TensorFlow will do is resolve\nthe dependencies.","start":1579.6,"duration":4.32},{"text":"It will find all the operations that A depends\non, directly or indirectly, by traversing","start":1583.92,"duration":6.43},{"text":"the graph backwards. In this case it will\nfind C, D and F.","start":1590.35,"duration":4.85},{"text":"Next, it will run any operation that has no\ninputs. These are called root nodes. In this","start":1595.2,"duration":5.58},{"text":"case F.","start":1600.78,"duration":1.94},{"text":"Once F is evaluated, operations C and D now\nhave all the inputs they need, so they can","start":1602.72,"duration":5.17},{"text":"be evaluated, and TensorFlow will actually\ntry to run them in parallel.","start":1607.89,"duration":5.45},{"text":"Say D finishes first, A still has one unevaluated\ninput so it can\u2019t run yet.","start":1613.34,"duration":6.91},{"text":"But as soon as C is finished, A can be evaluated.","start":1620.25,"duration":3.75},{"text":"And once it\u2019s done, the eval() method returns\nthe result, and we\u2019re good.","start":1624.0,"duration":3.85},{"text":"You can actually evaluate multiple operations\nat once, for example A and E, and the process","start":1627.85,"duration":5.22},{"text":"is really the same.\nIt finds all the dependencies, runs the root","start":1633.07,"duration":4.8},{"text":"operations, and then, you know, goes upward\nrunning every operation whose inputs are satisfied.","start":1637.87,"duration":6.86},{"text":"And once it's got both the values for both\nA and E, it returns the results.","start":1644.73,"duration":4.44},{"text":"So let\u2019s apply this to our reconstruction\ntargets. This tensor is the output of the","start":1649.17,"duration":6.69},{"text":"cond() operation, which has 3 parameters,\nmask_with_labels, a function that returns","start":1655.86,"duration":7.27},{"text":"y, and a function that returns y_pred.","start":1663.13,"duration":3.25},{"text":"And when we evaluate the reconstruction_targets,\nor any tensor that depends on it, such as","start":1666.38,"duration":5.3},{"text":"the final loss, which depends on the reconstruction\nloss, which eventually depends on the reconstruction_targets.","start":1671.68,"duration":5.88},{"text":"Well, what happens it, as earlier, TensorFlow\nstarts by resolving the dependencies. It finds","start":1677.56,"duration":5.83},{"text":"all three bottom nodes.","start":1683.39,"duration":2.51},{"text":"And it evaluates them all!","start":1685.9,"duration":1.8},{"text":"So y_pred may finish first. Since these operations\nare run in parallel, there\u2019s no way to know","start":1687.7,"duration":4.95},{"text":"in which order they will finish.","start":1692.65,"duration":2.08},{"text":"So, you know, y may finish next.\nAnd finally mask_with_labels finishes. So","start":1694.73,"duration":5.84},{"text":"suppose it evaluates to True. Now the reconstruction_targets\nhas all the inputs it needs, so it can be","start":1700.57,"duration":6.18},{"text":"evaluated.","start":1706.75,"duration":1.37},{"text":"And of course it does the right thing, since\nmask_with_labels is True, it returns the value","start":1708.12,"duration":4.49},{"text":"of y. Which is good.\nBut notice that y_pred was evaluated for nothing,","start":1712.61,"duration":5.35},{"text":"we\u2019re not using its output. It\u2019s not a\nbig deal since during training we need to","start":1717.96,"duration":4.34},{"text":"evaluate the margin loss, which depends on\nthe estimated class probabilities, which is","start":1722.3,"duration":4.83},{"text":"just one step away from the predictions, so\ncomputing the predictions won\u2019t add much","start":1727.13,"duration":4.36},{"text":"overhead. But still, it\u2019s a bit unfortunate.","start":1731.49,"duration":4.1},{"text":"Now suppose mask_with_labels evaluates to\nFalse.","start":1735.59,"duration":2.83},{"text":"Then, again, the reconstruction_targets will\ndo the right thing, it will output the value","start":1738.42,"duration":5.22},{"text":"of y_pred.","start":1743.64,"duration":1.0},{"text":"But this time, we evaluated y for nothing.\nIt\u2019s just a placeholder, so it won\u2019t add","start":1744.64,"duration":5.671},{"text":"much computation time, but it means that we\nmust feed y, even if mask_with_labels is False.","start":1750.311,"duration":5.759},{"text":"Well actually we can just pass an empty array,\nthat's fine. So it will work, but it's kind","start":1756.07,"duration":5.74},{"text":"of ugly.","start":1761.81,"duration":1.9},{"text":"So this unfortunate situation is due to the\nfact that the functions we passed to the cond()","start":1763.71,"duration":5.06},{"text":"function, do not actually create any tensor,\nthey just return tensors that were created","start":1768.77,"duration":5.81},{"text":"outside of these functions. If you build tensors\nwithin these functions, then TensorFlow will","start":1774.58,"duration":5.87},{"text":"do what you expect. It will stitch together\nthe partial graphs created by these functions","start":1780.45,"duration":5.5},{"text":"into a graph that will properly handle the\ndependencies. So you might be able to modify","start":1785.95,"duration":5.59},{"text":"my code and fix this ugliness. I tried, but\nI ended up with pretty convoluted code, so","start":1791.54,"duration":4.941},{"text":"I decided to stick with this implementation.\nI hope it won\u2019t keep you awake at night.","start":1796.481,"duration":5.119},{"text":"Sooo! We\u2019ve actually finished! Here\u2019s\nthe full picture again. The construction phase","start":1801.6,"duration":5.38},{"text":"is over, our graph is built, now on to the\nexecution phase, let\u2019s run this graph. First,","start":1806.98,"duration":7.23},{"text":"let\u2019s look at the training code.","start":1814.21,"duration":2.11},{"text":"It\u2019s really really completely standard.\nWe create a session, if a checkpoint file","start":1816.32,"duration":6.11},{"text":"exists, load it, or else initialize all the\nvariables. Then run the main training loop,","start":1822.43,"duration":6.97},{"text":"for a number of epochs, and for each epoch,\nrun enough iterations to go through the full","start":1829.4,"duration":4.8},{"text":"training set. And inside this loop, we simply\nevaluate the training operation, and the loss,","start":1834.2,"duration":7.92},{"text":"on the next batch. We feed the images and\nthe labels of the current training batch,","start":1842.12,"duration":5.69},{"text":"and we set mask_with_labels to True. That\u2019s\npretty much all there is to it. In the notebook,","start":1847.81,"duration":5.58},{"text":"I also added a simple implementation of early\nstopping, and I print out the progress, plus","start":1853.39,"duration":5.06},{"text":"I evaluate the model on the validation set\nat the end of each epoch. But the most important","start":1858.45,"duration":5.28},{"text":"part is here.","start":1863.73,"duration":1.44},{"text":"After training, I just run a few test images\nthrough the network and get the predictions","start":1865.17,"duration":5.2},{"text":"and reconstructions. As you can see, the predictions\nare all correct, and the reconstructions are","start":1870.37,"duration":5.65},{"text":"pretty good, they're pretty close to the original\nimages, except that they\u2019re slightly fuzzier,","start":1876.02,"duration":4.57},{"text":"as you can see.","start":1880.59,"duration":1.82},{"text":"Here\u2019s the code, there\u2019s really nothing\nspecial about it: I take a few test images,","start":1882.41,"duration":5.54},{"text":"I start a session and load the model, then\nI just evaluate the predictions and the decoder\u2019s","start":1887.95,"duration":7.07},{"text":"output (and I also get the capsules\u2019 outputs\nso I can tweak them later). The ugliness I","start":1895.02,"duration":7.19},{"text":"mentionned earlier is right here. I'm forced\nto pass an empty array. This value will be","start":1902.21,"duration":6.83},{"text":"ignored anyway.","start":1909.04,"duration":1.0},{"text":"And finally, the code tweaks the output vectors\nand passes the result to the decoder. So we","start":1910.04,"duration":6.54},{"text":"can see what each of of the 16 dimensions\nrepresent in the digit capsule\u2019s output","start":1916.58,"duration":5.28},{"text":"vector. For example, this image shows the\nreconstructions we get by tweaking the first","start":1921.86,"duration":4.48},{"text":"parameter (the notebook produces one such\nimage for each of the 16 parameters). And","start":1926.34,"duration":5.59},{"text":"as you can see, in the first, second and last\nrows, we see that the digits become thinner","start":1931.93,"duration":6.26},{"text":"and thinner, going to the right, or thicker\nand thicker to the left, so it holds information","start":1938.19,"duration":5.26},{"text":"about thickness. And in the middle row, you\ncan see that the bottom part of the number","start":1943.45,"duration":5.48},{"text":"5 gets lifted towards the top, so probably\nthat's what this parameter does for this digit.","start":1948.93,"duration":7.98},{"text":"Before I finish, I\u2019d like to thank everyone\nwho shared and commented on my first video,","start":1956.91,"duration":5.11},{"text":"I really had no idea it would receive such\nan enthusiastic response, and I\u2019m very very","start":1962.02,"duration":5.48},{"text":"grateful to all of you, it definitely motivates\nme to create more videos. If you want to learn","start":1967.5,"duration":5.0},{"text":"more about Machine Learning and support this\nchannel, check out my O\u2019Reilly book Hands-on","start":1972.5,"duration":4.28},{"text":"Machine Learning with Scikit-Learn and TensorFlow,\nI\u2019ll leave the links in the video description.","start":1976.78,"duration":4.41},{"text":"If you speak German, there\u2019s actually a\nGerman translation coming up for Christmas.","start":1981.19,"duration":3.77},{"text":"And if you speak French, the translation is\nalready available. It was split in two books","start":1984.96,"duration":4.61},{"text":"but it\u2019s really the same content.\nAnd that\u2019s all I had for today, I hope you","start":1989.57,"duration":4.04},{"text":"enjoyed this video and that you learned a\nthing or two about TensorFlow and capsule","start":1993.61,"duration":3.68},{"text":"networks. If you did, please, like, share,\ncomment, subscribe, and click on the bell","start":1997.29,"duration":4.59},{"text":"icon next to the subscribe button, to receive\nnotifications when I upload new videos. See","start":2001.88,"duration":4.711},{"text":"you next time!","start":2006.591,"duration":0.519}]},"autogen_text":{"pPN8d0E3900":"hey I'm overly Asian and in this video I'll tell you all about capsule networks a hot new architecture for neural nets Jeffrey Henson had the idea of capsule Network several years ago and he published a paper in 2011 that introduced many of the key ideas but he had a hard time making them work properly until now a few weeks ago in October 2017 a paper called dynamic routing between capsules was published by Sarah saber Nicholas frost and of course Geoffrey Hinton they managed to reach state-of-the-art performance on the MS dataset and demonstrated considerably better results than convolutional neural nets on highly overlapping digits so what are capsule networks exactly well in computer graphics you start with an abstract representation of a scene for example a rectangle at position X 20 and y equals 30 rotated by 16 degrees and and so on each object type has various instantiation parameters then you call some rendering function and boom you get an image inverse graphics is just the reverse process you start with an image and you try to find what objects it contains and what their instantiation parameters are a capsule Network is basically a neural network that tries to perform inverse graphics it is composed of many capsules a capsule is any function that tries to predict the presence and the instantiation parameters of a particular object at a given location for example the network above contains 50 capsules the arrows represent the output vectors of these capsules capsules output vectors the black arrows correspond to capsules that try to find rectangles while the blue arrows represent the output of capsules looking for triangles the length of an activation vector represents the estimated probability that the object the capsule is looking for is indeed present so you can see that most arrows are tiny meaning the capsules didn't detect anything but two arrows are quite long this means that the capsules at these locations are pretty confident that they found what they were looking for in this case a rectangle and the triangle so the orientation of the activation vector encodes the instantiation parameters of the object for example in this case the object's rotation but it could be also its thickness how stretched or skewed it is its exact location it might be slight translations and so on for simplicity I'll just focus on the rotation parameter but in real capsule Network the activation vectors may have five ten dimensions or more in practice a good way to implement this is to apply a couple convolutional layers just like in a regular convolutional neural net this will output an array containing a bunch of feature maps you can then reshape this array to get a set of vectors for each location for example suppose the convolutional layers output an array containing say 18 feature maps 2 times 9 you can easily reshape this array to get two vectors of nine dimensions each for every location you could also get three vectors of six dimensions each and so on something that would look like the capsule network represented here with two vectors at each location the last step is to ensure that no vector is longer than one since the vectors length is meant to represent a probability it cannot be greater than one to do this we apply a squashing function it preserves the vectors orientation but it squashes it to ensure that its length is between zero and one one key feature of capsule networks is that they preserve detailed information about the object's location and its pose throughout the network for example if I rotate the image slightly notice that the activation vectors also change slightly right this is called equivariance in a regular convolutional neural net there are generally several pooling layers and unfortunately these pooling layers tend to lose information such as the precise location and pose of the objects it's really not a big deal if you just want to classify the whole image but it makes it challenging to perform accurate image segmentation or object detection which require precise you know location and pose the fact that capsules are equivariance makes them very promising for these applications alright so now let's see how capsule networks can handle objects that are composed of a hierarchy of parts for example consider a boat centered at position X equals 22 and y equals 28 and rotated by 16 degrees this boat is compotes is composed of parts in this case one rectangle and one triangle so this is how would be rent rendered now we want to do the reverse we want universe graphics so we want to go from the image to this whole hierarchy of parts with their instantiation parameters similarly we could also draw a draw a house using the same parts a rectangle in a triangle that this arm organized in a different way so the trick will be to try to come to go from this image containing a rectangle in a triangle and figure out not only that the rectangle and triangle are at this location and this orientation but also that they are part of a boat not a house so yeah let's let's figure out how it would do this the first step we have already seen we run a couple of convolutional layers we reshape the output to get vectors and we squash them this gives us the output of the primary capsules we've got the first layer already the next step is where most of the magic and complexity of capsule networks takes place every capsule in the first layer tries to predict the output of every capsule in the next layer you might want to pause to think about what this means that the capsules at the primary the first layer tried to predict what the second layer capsules will output for example let's consider the capsule that detected the rectangle I'll call it the rectangle capsule let's suppose that there are two capsules in the next layer the house capsule and the boat capsule since the rectangle capsule detected a rectangle rotated by 16 degrees it predicts that the house capsule will detect a house rotated by 16 degrees that makes sense and the boat capsule will detect a boat rotated by 16 degrees as well that's what would be consistent with the orientation of the rectangle so to make this prediction what the rectangle capsule does is simply computes the dot product of a transformation matrix W IJ with its own activation vector at U at UI during training the network will gradually learn a transformation matrix for each pair of capsules in the first and second layer in other words it will learn all the part hole relationships for example the angle between the wall and the roof of a house and so on now let's see what the triangle capsule predicts right this time it's a bit more interesting given the rotation angle of the triangle it predicts that the house capsule will detect an upside-down house and the boat capsule will detect about rotated by 16 degrees these are the positions that would be consistent with the you know rotation angle of the triangle now we've got a bunch of predicted outputs what do we do with them well as you can see the rectangle capsule and the triangle capsules strongly agree on what the boat capsule will output in other words they agree that a boat positioned in this way would explain their own positions and rotations and they totally disagree on what the house capsule will output therefore it makes sense to assume that the rectangle and triangle are part of a boat not a house now that we know that the rectangle and triangle are part of the boat the outputs of the rectangle capsule and the triangle capsule really concern only the boat capsule this there's no need to send these outputs to any other capsule this would just add noise they should be sent only to the boat capsule this is called routing by agreements there are several benefits first since capsule outputs are only routed to the appropriate capsule in the next layer these capsules will get a cleaner input signal more accurate accurately determine the pose of the object second by looking at the paths of the activations you can easily navigate the hierarchy of parts and know exactly which part belongs to which objects like the rectangle biller belongs to the boats or the triangle belongs to the boat and so on lastly routing this by agreement helps parse crowded scenes with overlapping objects we will see this in a few slides but first let's look at how running by agreements is implemented in capsule networks here I have represented the various poses of the boat as predicted by the lower level capsules for example one of these circles may represent what the rectangle capsule thinks about the most likely pose of the boat and another circle may represent what the triangle capsule thinks and if we suppose that there are many other low level capsules then we might get a cloud of prediction vectors for the boat capsule like this in this example there are two post parameters one represents the rotation angle and the other represents the size of the boat as I mentioned earlier post parameters may capture many different kinds of visual features like skew thickness and so on or location a precise location so the first thing we do is we compute the mean of these predictions this gives us this vector the next step is to measure the distance between each predicted vector and the meet vector so I will use here the Euclidean distance but capsule networks actually use the scalar product basically we want to measure how much each predicted vector agrees with the mean predicted vector use using this agreement measure we can update the weights of every predicted vector accordingly note that the predicted vectors that are far from the mean now have a very small weight and the ones closest to the mean have a much stronger weight I've represent them represented them in black now we can compute the mean once again or I should say the weighted mean and notice that it moves slightly towards the cluster towards the center of the cluster so next we can once again update the weights and now most of the vectors within the cluster have turned black and again we can update the mean and we can repeat this process a few times in practice you know three to five iterations are generally sufficient this might remind you I suppose of the k-means clustering algorithm if you know it okay so this is how we find clusters of agreement now let's see how the whole algorithm works in a bit more details first for every predicted output we start by setting a raw routing weight bij equal to zero next we apply the softmax function softmax function to these raw weights for each primary capsule this gives the actual routing weights for each predicted output in this example 0.5 each equal weights next we compute a weighted sum of the predictions for each capsule in the next layer this might give vectors longer than one so as usual we apply the squash function and voila we now have the actual outputs of the house capsule and boat capsule but this is not the final output is just the end of the first round the first iteration now we can see which predictions were most accurate for example the rectangle capsule made a great prediction for the boat capsules output it really matches it pretty closely this is estimated by computing the scalar product of the predicted output vector U hat j-jake I and the actual product vector V J this scalar product is simply added to the predicted outputs raw routing weight bij so the weight of this particular predicted output is increased when there is a strong agreement the scalar product is going to be large so good predictions will have a higher weight on the other hand the rectangle capsule made a pretty bad prediction for the house capsules output so the scalar product in this case will be quite small and the raw routing weights of these predicted vector will not grow much next we update the routing weights by computing the softmax of the raw weights once again and as you can see the rectangle capsules predicted vector for the boat capsule now has a weight of 0.8 while it's predicted vector for the house capsule dropped down to 0.2 so most of its output is not going to go to the boat capsule not the house capsule once again we compute the weighted sum of all the predicted outputs vectors for each capsule in the next layer that is the house capsule in the boat capsule and this time the house capsule gets so little input that its output is a tiny vector on the other hand the boat capsule gets so much input that its output that it outputs a vector much longer than 1 so again we squash it and that's the end of round 2 and as you can see in just a couple iterations we have already ruled out the house and clearly chosen the boat after perhaps one or two more rounds we can stop and proceed to the next capsule layer in exactly the same way so as I mentioned earlier routing by agreement is really great to handle crowded scenes such as the one represented in this image one way to interpret this image as you can see there's a little bit of ambiguity you can see a house upside down in the middle however if this was the case then there would be no explanation for the bottom rectangle or the top triangle no reason for them to be where they are the best way to interpret the image is that there is a house at the top and a boat at the bottom and routing by agreement will tend to choose this solution since it makes all the capsules perfectly happy each of them making perfect predictions for the capsules in the next layer the ambiguity is explained away ok so what can you do with a capsule network now that you know how it works well for one you can create a nice image classifier of course just have one capsule per class in the top capsule layer and that's almost all there is to it all you need to add is a layer that computes the length of the top layer activation vectors and this gives you the estimated class probabilities you could then just train the network by minimizing the cross entropy loss has an irregular classification neural network and you know you'd be done however in the paper they use a margin loss that makes it possible to detect multiple classes in the image so without going into too much detail this margin loss is such that if an object of class K is present in the image then the corresponding top-level capsule should help put a vector whose squared length is at least 0.9 it should be long conversely if an object of class K is not present in the image then the capsule should output a short vector one whose squared length is shorter than 0.1 so the total loss is the sum of losses for all classes in the paper they also add a decoder network on top of the capsule network it's just a three you know three connected layers fully connected with a sigmoid activation function in the output layer and it learns to reconstruct the input image by minimizing the squared difference between the reconstructed image and the input image the full loss is margin loss we discussed earlier plus the reconstruction loss scaled down considerably so as to ensure that the margin loss dominates training the benefit of applying this reconstruction loss is that it forces the network to preserve all the information required to construct to reconstruct the image up to the top layer of the capsule network its output layer this constraint acts a bit like a regularizer it reduces the risk of overfitting and helps generalize to new examples and that's it you know how a capsule Network works and how to train it let's look a little bit at some of the figures in the paper which I find interesting so this is figure one showing a full capsule network for mmm mist you can see the the first two regular convolutional layers whose output is reshaped and squashed to get the activation vectors of the primary capsules and these primary capsules are organized in a six by six grid with 32 primary capsules in each cell of this grid and each primary capsule outputs an eight dimensional vector so this first layer of capsules is fully connected to the ten output capsules which output sixteen dimensional lectures the length of these vectors is yoots is used to compute the margin loss as explained earlier now this is figure two from the paper it shows the decoder sitting on top of the caps net it is composed of two fully connected relu layers plus a fully connected sigmoid layer which outputs 784 numbers that correspond to the pixel intensities of the richens reconstructed image which is 28 by 28 pixel the squared difference between this reconstructed image and the input image gives the reconstruction loss right and this is figure four from the paper also interesting one nice thing about capsule networks is that the activation vectors are often quite interpretable for example this image shows the reconstructions that you get when you gradually modify one of the 16 dimensions of the top layer capsules output you can see that the first dimension seems to represent you know scale and thickness the fourth dimension represents a localized skew if you look at how it that number 4 is modified from the left to the right the fifth represents the width of the digit plus a slight translation to get the exact position so as you can see it's rather clear what most of these parameters do ok to conclude let's summarize the pros and cons capsule networks have reached state-of-the-art accuracy on a missed on so far 10 they they got a bit over 10 percent error which is far from a state-of-the-art but it's - what was first obtained with other techniques before you know years of efforts were put into them so it's still a good start capsule networks require less training data they offer equivariance which means that position and pose information are preserved and this is very promising for image segmentation and object detection the routing by agreement algorithm is great for crowded scenes the routing tree also maps the hierarchy of objects parts so every part is associated to a whole and it's rather robust to rotations translations and other FIM transformations the activation vectors are somewhat interpretable and finally obviously its sentence idea so don't bet against it however there are a few cons first as I mentioned the results are not yet state-of-the-art on cipher 10 even though it's a good start plus it's still unclear whether capsule networks can scale to larger images such as the image net data set you know what will the accuracy be capsule networks are also quite slow to Train in large part because of the routing by agreement algorithm which has an inner loop as you saw earlier finally there is only one capsule of any type in a given location so it's impossible for a capsule network to detect two objects of the same type if they are too close to one another this is called crowding and it's been observed in human vision as well so it's probably not a showstopper alright I highly recommend you take a look at the code of a caps net implementation such as the ones listed here I'll leave the links in the video description below if you take your time you should have no problem understanding everything the code is doing the main difficulty in implementing caps nets is that it contains an inner loop for the routing by agreement algorithm implementing loops in Kerris and tensorflow can be a little bit trickier than in pi torch but it can be done so if you don't have a particular preference then I would say that the PI torch code is probably the easiest to understand and that's all I had I hope you enjoyed this video if you did please thumbs up share comment subscribe blah blah blah it's my first real YouTube video and if people find it useful I'm make some more if you want to learn more about machine learning deep learning and deep reinforcement learning you may want to read my Holly book hands on machine learning with scikit-learn and tensorflow it covers a ton of topics with many code examples that you will find on my github account so I'll leave the links in the video description that's all for today have fun and see you next time","2Kawrd5szHE":"hi I'm over LaVon and today I'm going to show you how to implement a capsule network using tensor flow in my previous video I presented the key ideas behind capsule networks a recently published neural net architecture if you haven't seen this video I encourage you to do so now today I will focus on the tensor flow implementation I wrote a jupiter notebook containing all the code and detailed explanations and i published it on my github account as always I'll put all the links in the video description below so I encourage you to clone it and play with it so it reaches over ninety-nine point four accuracy on the test set which is pretty good considering it's a shallow network with just two capsule layers and a total of about 1200 capsules there's a lot of code in the in this notebook so I won't go through every single line in this video but I'll explain the main difficulties I came across and hopefully this will be useful to you for other tensor flow implementations not just caps nets okay let's build a network first we need to feed the input images to the network and that's our input layer we implement it using a simple tensor flow placeholder the batch size is unspecified so that we can pass any number of images in each batch in this example 5 note that we directly send Tony 8 by 28 pixel images with a single channel since the images are grayscale color images would typically have 3 channels for red green and blue and that's it for the input layer next let's build the primary capsule layer for each digit in the batch it will output 32 maps each containing a 6 by 6 grid of 8 dimensional vectors the capsules in this particular map seem to detect the start of a line segment that you can see that the output vectors are long in the locations where there's a start of a line and the orientation of the ad vector gives the pose parameters in this case I've represented the rotation angle but the vectors a dimensional orientation would also capture things like the thickness of the line the precise location of the start of the line relative to the cell in the 6 by 6 grid and so the implementation is really straightforward first we define two regular convolutional layers the input of the first layer is X the placeholder that will contain the input images we will feed at runtime the second layer takes the output of the first layer of course and we use the parameters specified in the paper the second layer is configured to output 256 feature maps each feature map contains 6x6 grid of scalars we want a 6x6 a grid of vectors instead so we use tensor flows reshape function to get 32 Maps of eight dimensional vectors instead of 256 maps of scalars in fact since the primary capsules will be fully connected to the digit capsules we can simply reshape to one long list of 1152 output vectors that's a thirty-two times six times six for each instance in the batch and the last step is to squash the vectors to ensure that their length is always between zero and one for this we use a homemade squash function here it is this function implements the squash equation given in the paper it squashes every vector in an array along the specified dimension by default the last one so as you can see it involves a division by the norm of the vector so there's a risk of a division by zero if at least one of the vectors is a zero vector so you could just add a tiny epsilon value in the denominator and it would fix the division by zero problem however you would still run into another issue the norm of a vector has no defined gradients when the vector is zero so if you just use tensor flows norm function to compute the norm in this equation then if at least one of the vectors is zero the gradients will be undefined it will return n a n and not so as a result when greed in descent updates the weights of our model the weights will end up being undefined as well the model would effectively be dead then you don't want that so the trick is to compute a safe approximation of the norm shown in the equation on the right and that's about it that's all for the primary capsules apart for computing the norm safely it was pretty straight forward on to the next layer where all the complexity is the digit capsules there are just ten of them one for each digits 0 to 9 and the output 16 dimensional vectors in this particular example you can see that the longest output vector is the one for digit 4 and again it's orientation in the 16 dimensional space gives information about the pose of this digit its rotation its thickness its Q its position and so on by the way note that most of the position information in the first layer was encoded in the location of the active capsules in the 6x6 grid so for example if I shift the digit 4 slightly to the left in the input image they then different capsules in the first layer get activated see so the output of these first layer capsules only contains a local shift information relative to the position of the capsule in the 6x6 grid but in the second capsule layer the full position information is now encoded in the orientation of the output vector in 16 dimensional space ok now let's see how to implement this layer the first step is to compute the predicted output vectors since this second layer is fully connected to the first layer we will compute one predicted output for each pair of first and second layer capsules for example using the output of the first primary capsule we can predict the output vector of the first digit capsule for this we just use a transformation matrix w11 which will gradually be learned during training and we multiply it by the output of the first layer capsule this gives us u hat 1 1 which is the predicted output of the first digit capsule based on the output of the first primary capsule since the primary capsules output eight dimensional vectors and the digit capsules output sixteen dimensional vectors the transformation matrix w11 must be a sixteen by eight matrix next we try to predict the output of the second digit capsule still based on the output of the first primary capsule note that we are using a different transformation matrix w12 and we do the same for the third capsule using W one three and so on for all the digit capsules then we move on to the second primary capsule and we use its output to predict the output of the first digit capsule and so on for all the digit capsules then we move on to the third primary capsule we make ten predictions and so on you get the picture there are 1152 primary capsules multiply six by six by 32 and ten digit capsules so we end up with eleven thousand five hundred and twenty predicted output vectors now we could just compute them one by one but it would be terribly inefficient so let's see how we can get all the predicted output vectors in just one matte small operation now you know that tensorflow mat small function lets you multiply two matrices but you may not know that you can also use it to multiply many matrices in one shot this will be incredibly efficient especially if you are using a GPU card because it will perform all the matrix multiplications in parallel in many different GPU threads so here's how it works suppose a b c d e f and g h i j k l are all matrices you can put these matrices in two arrays each with two rows and three columns for example so we have two dimensions for this 2x3 grid of matrices and each matrix is two-dimensional so these arrays are two plus two equals four dimensional arrays if you pass these arrays these four d arrays two matmo it will perform an element-wise matrix application so the result will be this four dimensional array containing a x G here and B multiplied by H here and so on so let's use this to compute all the predicted output vectors we can create first 4d array containing all the transformation matrices there's one row per primary capsule and one column per digit capsule the second array must contain the output vectors of each primary capsule then we just pass these two arrays to the mole function and it gives us the predicted output vectors for all the pairs of primary and digit capsules since we need to predict the outputs of all ten-digit capsules for each primary capsule this array must contain ten copies of the primary capsules outputs we will use the tile function to replicate the first column of output vectors ten times but there's one additional catch we want to make these predictions for all the instances in the batch not just one instance so there's an additional dimension for the batch size it turns out that the primary output vectors were already computed for every single instance so the second array is fine it already has this dimension but we need to replicate the 4d array containing all the transformation matrices so that we end up with one copy per instance in the batch now if you understand this then the code should be pretty clear first we create a variable containing all the transformation matrices it has a one row per primary capsule one column per digit castle capsule and it contains 16 by 8 matrices that's four dimensions and we add another dimension at the beginning of size one to make it easy to tile this array for each instance in the batch now the variable is initialized randomly using a normal distribution of standard deviation 0.01 that's a hyper parameter you can tweet and that's about it we create this variable next we want to tile this array for each instance so first we need to know the batch size we don't actually know it at graph construction time it will only be known when we run the graph but we can use tensor flows shape function it creates a tensor that will know the shape at run time and we grab its first dimension which is the batch size then we simply tile our big W array along the first dimension to get one copy per instance now recall that the output of the primary capsules was a three dimensional array the first dimension is the batch size that we will know at runtime then there's one row per capsule and each capsule has eight dimensions so we need to reshape this array a bit to get the shape that we are looking for to to do the big mat Mulla operation first we add an extra dimension at the end using tensor flows expand dims function and vectors are now represented as column vectors instead of one dimensional array each of these is a column vector column vector is a matrix a 2d array with a single column then we add another dimension for the digit capsules and we replicate all the output vectors ten times across this new dimension once per digit capsule and lastly we just use map Moll to multiply the transformation matrices with the primary capsules output vectors and we get all the digit capsules predicted outputs for each pair of primary and digit capsules and for each instance in the batch in one shot and that's the end of the first step for computing the digit capsules outputs we now have a bunch of predicted output vectors the second step is the routing by agreement algorithm so first we set all the routing weights to 0 for this we just use TF zeros there is one weight for each pair of primary and digit capsules and for each instance the last two dimensions here are equal to 1 they will be useful in a minute next we compute the softmax of each primary capsules 10 raw routing weights okay so softmax happens along this dimension next we compute the weighted sum of all the predicted output vectors for each digit capsule using the routing weights so the weighted sum is along this dimension this is pretty straightforward tensorflow code first multiply the routing weights and the predicted vectors this is a element-wise multiplication not a matrix multiplication then just compute the sum over the primary capsule dimension and the two dimensions we added earlier for the routing weights are useful in the multiplication step so that the two arrays have the same number of dimensions the same rank they don't have the exact same shape but they have compatible shapes so tensorflow will perform broadcasting now if you don't know what broadcasting is this should make it clear I'm multiplying two matrices but one of them just has one row so tensorflow will act as if this row were repeated the appropriate number of times you could achieve the same thing using tiling as we did earlier but this is more efficient and you may wonder why we didn't use broadcasting earlier but the reason is it does not work for matrix multiplication here we're doing element wise multiplication okay back to the digit capsules we computed a weighted sum of the predicted vectors for each digit capsule and we just run the squash function and we get the outputs of the digit capsules all right but wait this is just the end of round one of the routing by agreement algorithm now on to round two so first we need to measure how good each prediction was and use this to update the routing weights for example look at the predictions that we made using this primary capsules output notice that for example the prediction for digit four is excellent and this is measured using the scalar product of the predicted output vector and the actual output vector these two vectors are actually represented as column vectors meaning a matrix with a single column so to compute the scalar product we must transpose the predicted column vector you had J I to get a row vector and multiply this this row vector and the actual output vector V J which is a column vector we will get a one by one matrix containing the scalar product of the vectors and now of course we need to do this for each predicted vector so once again we can use the map mol function to perform all the matrix multiplications in just one shot first we must use the tile function to get one copy of the actual output vectors V J for each primary capsule then we use map mo telling it to transpose each metric matrix in the first array on the fly and lo and behold we get all the scalar products at once so now we have a measure of agreements between each predicted vector and the actual output vector we can then add these scalar products to the raw whites using a simple addition and the rest of round 2 is exactly the same as the end of round 1 the code is really identical except we're now using the raw routing ways of round 2 we compute their softmax to get the actual routing weights for round 2 then we compute the weighted sum of all the predicted vectors for each digit capsule and finally we squash the results and now we have the new digit capsule outputs and we finish round 2 we could do a few more rounds exactly like this one but I'll stop now and use the current output vectors at the end of round 2 as the output of the digit capsules now you probably noticed that I implemented the routing algorithms loop without an actual loop it's a bit like computing the sum of squares from 1 to 100 with this code of course this will build a very big tensor flow graph but it works you can think of it as an unrolled loop now cleaner a way to do this would be to write a for loop in Python like this much better however it's important to understand that the resulting tensor photograph will be absolutely identical to the one produced by the previous code all we're doing here is constructing a graph and tensorflow will not even know that we use the loop to build a graph again this works fine it's just that you end up with a very large graph so you can think of this loop as a static loop that only runs at graph construction time if you want a dynamic loop one that tensorflow itself will run then you must use tensor flows while loop function like this the while loop function takes three parameters the first one is a function that must return a tensor that will determine whether the loop should go on or not at each iteration the second parameter is also a function that must build the body of the loop and that will also be evaluated at each iteration and finally third parameter contains a list of tensors that will be sent to both the condition and loop body function at the first iteration for the following iterations these functions will receive the output of the loop body function so you can pause the video if you need to take a closer look at this code once you get it you can try modifying my caps net implementation to use a dynamic loop rather than a static unrolled loop apart from making the code cleaner and the graph smaller using a dynamic loop allows you to change the number of iterations using the exact same model also if you set the swap memory parameter of the while loop function if you set it to true tensorflow will automatically swap the GPU memory to CPU memory when it can to save GPU memory since CPU Ram is much cheaper and abundant this can really be useful and that's it we've computed the output of the digit capsules cool now the length of each output vector represents the probability that a digit of that class is present in the image so let's compute these probabilities for this we cannot use tensor flows norm function because training will explode if there's a zero vector at any point as I mentioned earlier so instead we used a homemade safe norm function similar to what we did with the squash function and note that the sum of the probabilities don't necessarily add up to one because we are not using a soft max layer this makes it possible to detect multiple different digits in the same image but they all have to be different digits view you can detect a 5 and a 3 but you cannot detect say two fives next let's predict the most likely digit we just used the Arg max function that gives us the index of the highest probability the index happens to be the number of the digit itself note that we first get a tensor that has a couple extra dimensions of size one at the end so we get rid of them using the squeeze function if we called squeeze without specifying the axes to remove it would remove all dimensions of size 1 this would generally be okay except if the batch size was equal to 1 in which case we would be left with a scalar value rather than an array and we don't want that so it's better to specify the axes great now we have a capsule network that can estimate class probabilities and make predictions we can measure the models accuracy on the batch by simply comparing the predictions and the labels in this case the prediction for the last digit in the batch is wrong it's seven instead of one so we get 80% accuracy now the code is really straightforward we just use the equal function to compare the labels and the predictions wipe red and this gives us an array of boolean z' so we cast these boolean stew floats which gives us a bunch of zeros for bad predictions and ones for good predictions and we compute the mean to get the batch accuracy the labels Y are just a regular placeholder nothing special and that's it we have a full model able to make predictions now let's look at the training code this diagram is about two get pretty crowded so I'll remove the accuracy for clarity and now first we want to compute the margin loss it's given by this equation by the way I made a mistake in my first video I squared norms instead of squaring the max operations sorry about that this here is the correct equation computing it is pretty straightforward so I won't go through it in details the only trick is to understand how you can easily compute all the TK values for a given instance TK is equal to 1 if a digit of cos K is present in the image otherwise it's equal to 0 you can get all the TK values for each instance by simply converting the labels to one hot representation for example if an instance label is three then for this instance T will contain a 10 dimensional vector full of zeros except for a 1 its index 3 okay next we want to compute the reconstruction loss so first we must send the outputs of the digit capsules to a decoder that will try to use them to reconstruct the input images this decoder is just a regular feed-forward neural net composed of three fully connected layers it's really simple code so you can pause the video if you want and take a close look at it it outputs an array containing 784 values from 0 to 1 for each instance representing the pixel intensities of 28 by 28 pixel images and that's it we have our reconstructed images we can now compute the reconstruction loss this is just the squared difference between the input images and the reconstructions since the input images are 28 by 28 by 1 we first reshape them to one dimension per instance with 784 values each then we compute the squared difference now we can compute the final loss it's just the sum of the margin loss and the reconstruction loss scale down to let the margin loss dominate training pretty simple as you can see now let's add the training operation the paper mentions they use tensor flows implementation of the atom optimizer using the default parameters so let's do that we create the optimizer and call it's minimized method to get the training operation that will tweak the model parameters to minimize the loss we're almost done but there's one last detail I didn't mention in the first video the paper indicates that the outputs of the digit capsules should all be masked out except for the ones corresponding to the target digit so instead of sending the digit capsules outputs directly to the decoder we want to apply a mask first like this the mask will have the same shape as the digit capsules output array and it will be equal to zero everywhere except for once at the location of the target digits by multiplying the digit capsules output in the mask we get the input to the decoder but there's one catch this picture is good for training but at test time we won't have the labels so instead we will master the output vectors using the predicted classes rather than labels like this now we could build a different graph for training and for testing but it wouldn't be very convenient so instead let's build a conditioned operation we will add a boolean a placeholder called mask with labels if it is true then we use the labels to build the mask if it is false then we use the prediction note the difference okay and here's the code we build the mask of labels placeholder which will default to false so that we only need to set it during training and then we define the reconstruction targets using tensorflow scon function it takes three arguments the first one is a tensor representing the condition in this case simply the mask with labels placeholder the second parameter is a function that returns the tensor to use if the condition is true and the third parameter is a function that returns the tensor to use that the condition is false then to build the mask we simply use the one-hot function now there's actually one slight problem with this implementation and to explain it I need to step back for a second and talk about how tensorflow evaluates suppose we built this graph these are all tensorflow operations and we want to evaluate the output of operation a the first thing tensorflow will do is resolve the dependencies it will find all the operations that a depends on directly or indirectly by traversing the graph backwards in this case we'll find C D and F next it will run any of these operations that has no inputs these are called root nodes in this case F once F is evaluated operation C and D now have all the inputs they need so they can be evaluated and tensorflow will actually try to run them in parallel say D finishes first a still has one unev alyou ated input so it can't run yet but as soon as C is finished a can be evaluated and once it's done the eval method returns the result in we're good you can actually evaluate multiple operations at once for example a and E and the process is really the same it finds all the dependencies runs the root operations and then you know goes upward running every operation whose inputs are satisfied and once it's got both the values for a and E it returns the results so let's apply this to our reconstruction targets this tensor is the output of the cond operation which has three parameters mask with labels a function that returns Y and a function that returns wipe red and when we evaluate the reconstruction targets or any tensor that depends on it such as the final loss which depends on the reconstruction loss which eventually depends on the reconstruction targets well what happens is as earlier tensorflow starts by resolving the dependencies it finds all three bottom nodes and it evaluates them all so Y pred may finish first since these operations are run in parallel there's no way to know in which order they will finish so you know Y may finish next and finally mask what label finishes so suppose it evaluates to true now the reconstruct target's has all the inputs it needs so it can it can be evaluated and of course it does the right thing since masks with labels is true it returns the value of y which is good but notice that y pred was evaluated for nothing we're not using its output it's not a big deal since during training we need to evaluate the margin loss which depends on the estimated class probabilities which is just one step away from the prediction so computing the predictions won't add much overhead but still it's a bit unfortunate now suppose mask what labels evaluates to false then again the reconstruction targets will do the right thing it will output the value of y pred but this time how we evaluated y for nothing it's just a placeholder so it won't add much computation time but it means that we must feed why even if mask with labels is false well actually we can just pass an empty array and that's fine so it will work but it's kind of ugly so this unfortunate situation is due to the fact that the functions we passed to the con function do not actually create any tensor they just return tensors that were created outside of these functions so if you build tensors within these functions then tensorflow will do what you expect it will stitch together the partial graphs created within these functions into a graph that will properly handle the dependencies so you might be able to modify my code and fix this ugliness I tried but I ended up with pretty convoluted code so I decided to stick with this implementation I hope it won't keep you awake at night so we've actually finished here's the full picture again the construction phase is over our graph is built now on to the execution phase let's run this graph first let's look at the training code it's really really completely standard you create a session if a check point file exists load it or else initialize all the variables then run the main training loop for a number of epochs and for each epoch run enough iterations to go through the full training set and inside this loop we simply evaluate the training operation and the loss on the next batch we feed the images and the labels of the current training batch and we set masks with labels to true that's pretty much all there is to it and the note book I also added a simple implementation of early stopping and I print out the progress plus I evaluate the model on the validation set at the end of each epoch but the most important part is here after training I just run a few tests images through the network and get the predictions and reconstructions as you can see the predictions are all correct and the reconstructions are pretty good they're pretty close to the original images except that they're slightly fuzzier and as you can see here's the code there's really nothing special about it I take a few images I start a session and load the model then I just evaluate predictions the predictions and the decoders output and I also get the capsules output so I can tweak them later the ugliness I mentioned earlier is right here right I'm forced to pass an empty array this value will be ignored anyway and finally the code tweaks the output vectors and passes the results to the decoder so we can see what each of the sixteen dimensions represent and the digit capsules output vector for example this image shows the reconstructions we get by tweaking the first parameter so the notebook produces one such image for each of the sixteen parameters and as you can see in the first second and last rows we see that the digits become thinner and thinner we're going to the right or thicker and thicker to the left so it holds information about thickness and in the middle row you can see that the bottom part of the number five gets lifted towards the top so probably that's what this parameter does for this digit before I finish I'd like to thank everyone who shared and commented on my first video I really had no idea it would receive such an enthusiastic response and I'm very very grateful to all of you it definitely motivates me to more videos if you want to learn more about machine learning and support this channel check out my O'Reilly book hands on machine learning with scikit-learn and tensorflow I leave the links in the video description if you speak German there's actually a German translation coming up for Christmas and if you speak French the translation is already available it was split in two books but it's really the same content and that's all I have for today I hope you enjoyed this video and that you learned a thing or two about tensorflow and capsule networks if you did please like share comment subscribe and click on the bell icon next to the subscribe button to receive notifications when I upload new videos see you next time"},"manual_text":{"pPN8d0E3900":"Hey! I\u2019m Aur\u00e9lien G\u00e9ron, and in this video\nI\u2019ll tell you all about Capsule Networks, a hot new architecture for neural nets. Geoffrey\nHinton had the idea of Capsule Networks several years ago, and he published a paper in 2011\nthat introduced many of the key ideas, but he had a hard time making them work properly,\nuntil now. A few weeks ago, in October 2017, a paper\ncalled \u201cDynamic Routing Between Capsules\u201d was published by Sara Sabour, Nicholas Frosst\nand of course Geoffrey Hinton. They managed to reach state of the art performance on the\nMNIST dataset, and demonstrated considerably better results than convolutional neural nets\non highly overlapping digits. So what are capsule networks exactly? Well, in computer graphics, you start with\nan abstract representation of a scene, for example a rectangle at position x=20 and y=30,\nrotated by 16\u00b0, and so on. Each object type has various instantiation parameters. Then\nyou call some rendering function, and boom, you get an image. Inverse graphics, is just the reverse process.\nYou start with an image, and you try to find what objects it contains, and what their instantiation\nparameters are. A capsule network is basically a neural network that tries to perform inverse\ngraphics. It is composed of many capsules. A capsule\nis any function that tries to predict the presence and the instantiation parameters\nof a particular object at a given location. For example, the network above contains 50\ncapsules. The arrows represent the output vectors of these capsules. The capsules output\nvectors. The black arrows correspond to capsules that try to find rectangles, while the blue\narrows represent the output of capsules looking for triangles. The length of an activation\nvector represents the estimated probability that the object the capsule is looking for\nis indeed present. You can see that most arrows are tiny, meaning the capsules didn\u2019t detect\nanything, but two arrows are quite long. This means that the capsules at these locations\nare pretty confident that they found what they were looking for, in this case a rectangle,\nand a triangle. Next, the orientation of the activation vector\nencodes the instantiation parameters of the object, for example in this case the object\u2019s\nrotation, but it could be also its thickness, how stretched or skewed it is, its exact position\n(there might be slight translations), and so on. For simplicity, I\u2019ll just focus on\nthe rotation parameter, but in a real capsule network, the activation vectors may have 5,\n10 dimensions or more. In practice, a good way to implement this\nis to first apply a couple convolutional layers, just like in a regular convolutional neural\nnet. This will output an array containing a bunch of feature maps. You can then reshape\nthis array to get a set of vectors for each location. For example, suppose the convolutional\nlayers output an array containing, say, 18 feature maps (2 times 9), you can easily reshape\nthis array to get 2 vectors of 9 dimensions each, for every location. You could also get\n3 vectors of 6 dimensions each, and so on. Something that would look like the capsule\nnetwork represented here with two vectors at each location. The last step is to ensure\nthat no vector is longer than 1, since the vector\u2019s length is meant to represent a\nprobability, it cannot be greater than 1. To do this, we apply a squashing function.\nIt preserves the vector\u2019s orientation, but it squashes it to ensure that its length is\nbetween 0 and 1. One key feature of Capsule Networks is that\nthey preserve detailed information about the object\u2019s location and its pose, throughout\nthe network. For example, if I rotate the image slightly, notice that the activation\nvectors also change slightly. Right? This is called equivariance. In a regular convolutional\nneural net, there are generally several pooling layers, and unfortunately these pooling layers\ntend to lose information, such as the precise location and pose of the objects. It\u2019s really\nnot a big deal if you just want to classify the whole image, but it makes it challenging\nto perform accurate image segmentation or object detection (which require precise location\nand pose). The fact that capsules are equivariant makes them very promising for these applications. All right, so now let\u2019s see how capsule\nnetworks can handle objects that are composed of a hierarchy of parts. For example, consider\na boat centered at position x=22 and y=28, and rotated by 16\u00b0. This boat is composed\nof parts. In this case one rectangle and one triangle. So this is how it would be rendered.\nNow we want to do the reverse, we want inverse graphics, so we want to go from the image\nto this whole hierarchy of parts with their instantiation parameters. Similarly, we could also draw a house, using\nthe same parts, a rectangle and a triangle, but this time organized in a different way.\nSo the trick will be to try to go from this image containing a rectangle and a triangle,\nand figure out, not only that the rectangle and triangle are at this location and this\norientation, but also that they are part of a boat, not a house. So, yeah, let\u2019s figure\nout how it would do this. The first step we have already seen: we run\na couple convolutional layers, we reshape the output to get vectors, and we squash them.\nThis gives us the output of the primary capsules. We\u2019ve got the first layer already. The next\nstep is where most of the magic and complexity of capsule networks takes place. Every capsule\nin the first layer tries to predict the output of every capsule in the next layer. You might\nwant to pause to think about what this means. The capsules in the first layer try to predict\nwhat the second layer capsules will output. For example, let\u2019s consider the capsule\nthat detected the rectangle. I\u2019ll call it the rectangle-capsule. Let\u2019s suppose that there are just two capsules\nin the next layer, the house-capsule and the boat-capsule. Since the rectangle-capsule\ndetected a rectangle rotated by 16\u00b0, it predicts that the house-capsule will detect a house\nrotated by 16\u00b0, that makes sense, and the boat-capsule will detect a boat rotated by\n16\u00b0 as well. That\u2019s what would be consistent with the orientation of the rectangle. So, to make this prediction, what the rectangle-capsule\ndoes is it simply computes the dot product of a transformation matrix W_i,j with its\nown activation vector u_i. During training, the network will gradually learn a transformation\nmatrix for each pair of capsules in the first and second layer. In other words, it will\nlearn all the part-whole relationships, for example the angle between the wall and the\nroof of a house, and so on. Now let\u2019s see what the triangle-capsule\npredicts. This time, it\u2019s a bit more interesting:\ngiven the rotation angle of the triangle, it predicts that the house-capsule will detect\nan upside-down house, and that the boat-capsule will detect a boat rotated by 16\u00b0. These\nare the positions that would be consistent with the rotation angle of the triangle. Now we have a bunch of predicted outputs,\nwhat do we do with them? As you can see, the rectangle-capsule and\nthe triangle-capsule strongly agree on what the boat-capsule will output. In other words,\nthey agree that a boat positioned in this way would explain their own positions and\nrotations. And they totally disagree on what the house-capsule will output. Therefore,\nit makes sense to assume that the rectangle and triangle are part of a boat, not a house. Now that we know that the rectangle and triangle\nare part of a boat, the outputs of the rectangle capsule and the triangle capsule really concern\nonly the boat capsule, there\u2019s no need to send these outputs to any other capsule, this\nwould just add noise. They should be sent only to the boat capsule. This is called routing by agreement. There\nare several benefits: first, since capsule outputs are only routed to the appropriate\ncapsule in the next layer, these capsules will get a cleaner input signal and will more\naccurately determine the pose of the object. Second, by looking at the paths of the activations,\nyou can easily navigate the hierarchy of parts, and know exactly which part belongs to which\nobject (like, the rectangle belongs to the boat, or the triangle belongs to the boat,\nand so on). Lastly, routing by agreement helps parse crowded scenes with overlapping objects\n(we will see this in a few slides). But first, let\u2019s look at how routing by agreement is\nimplemented in Capsule Networks. Here, I have represented the various poses\nof the boat, as predicted by the lower-level capsules. For example, one of these circles\nmay represent what the rectangle-capsule thinks about the most likely pose of the boat, and\nanother circle may represent what the triangle-capsule thinks, and if we suppose that there are many\nother low-level capsules, then we might get a cloud of prediction vectors, for the boat\ncapsule, like this. In this example, there are two pose parameters: one represents the\nrotation angle, and the other represents the size of the boat. As I mentioned earlier,\npose parameters may capture many different kinds of visual features, like skew, thickness,\nand so on. Or precise location. So the first thing we do, is we compute the mean of all\nthese predictions. This gives us this vector. The next step is to measure the distance between\neach predicted vector and the mean vector. I will use here the euclidian distance here,\nbut capsule networks actually use the scalar product. Basically, we want to measure how\nmuch each predicted vector agrees with the mean predicted vector. Using this agreement\nmeasure, we can update the weight of every predicted vector accordingly. Note that the predicted vectors that are far\nfrom the mean now have a very small weight, and the ones closest to the mean have a much\nstronger weight. I\u2019ve represented them in black. Now we can just compute the mean once\nagain (or I should say, the weighted mean), and you\u2019ll notice that it moves slightly\ntowards the cluster, towards the center of the cluster. So next, we can once again update the weights.\nAnd now most of the vectors within the cluster have turned black.\nAnd again, we can update the mean. And we can repeat this process a few times.\nIn practice 3 to 5 iterations are generally sufficient. This might remind you, I suppose,\nof the k-means clustering algorithm if you know it. Okay, so this is how we find clusters\nof agreement. Now let\u2019s see how the whole algorithm works in a bit more details. First, for every predicted output, we start\nby setting a raw routing weight b_i,j equal to 0. Next, we apply the softmax function to these\nraw weights, for each primary capsule. This gives the actual routing weights for each\npredicted output, in this example 0.5 each. Next we compute a weighted sum of the predictions,\nfor each capsule in the next layer. This might give vectors longer than 1, so as usual we\napply the squash function. And voil\u00e0! We now have the actual outputs\nof the house-capsule and boat-capsule. But this is not the final output, it\u2019s just\nthe end of the first round, the first iteration. Now we can see which predictions were most\naccurate. For example, the rectangle-capsule made a great prediction for the boat-capsule\u2019s\noutput. It really matches it pretty closely. This is estimated by computing the scalar\nproduct of the predicted output vector \u00fb_j|i and the actual product vector v_j. This scalar\nproduct is simply added to the predicted output\u2019s raw routing weight, b_i,j. So the weight of\nthis particular predicted output is increased. When there is a strong agreement, this scalar\nproduct is large, so good predictions will have a higher weight. On the other hand, the rectangle-capsule made\na pretty bad prediction for the house-capsule\u2019s output, so the scalar product in this case\nwill be quite small, and the raw routing weight of this predicted vector will not grow much. Next, we update the routing weights by computing\nthe softmax of the raw weights, once again. And as you can see, the rectangle-capsule\u2019s\npredicted vector for the boat-capsule now has a weight of 0.8, while it\u2019s predicted\nvector for the house-capsule dropped down to 0.2. So most of its output is now going\nto go to the boat capsule, not the house capsule. Once again we compute the weighted sum of\nall the predicted output vectors for each capsule in the next layer, that is the house-capsule\nand the boat-capsule. And this time, the house-capsule gets so little input that its output is a\ntiny vector. On the other hand the boat-capsule gets so much input that it outputs a vector\nmuch longer than 1. So again we squash it. And that\u2019s the end of round #2. And as you\ncan see, in just a couple iterations, we have already ruled out the house and clearly chosen\nthe boat. After perhaps one or two more rounds, we can stop and proceed to the next capsule\nlayer in exactly the same way. So as I mentioned earlier, routing by agreement\nis really great to handle crowded scenes, such as the one represented in this image.\nOne way to interpret this image (as you can see there is a bit of ambiguity), you can\nsee a house upside down in the middle. However, if this was the case, then there would be\nno explanation for the bottom rectangle or the top triangle, no reason for them to be\nwhere they are. The best way to interpret the image is that\nthere is a house at the top and a boat at the bottom. And routing by agreement will\ntend to choose this solution, since it makes all the capsules perfectly happy, each of\nthem making perfect predictions for the capsules in the next layer. The ambiguity is explained\naway. Okay, so what can you do with a capsule network\nnow that you know how it works. Well for one, you can create a nice image\nclassifier of course. Just have one capsule per class in the top layer and that\u2019s almost\nall there is to it. All you need to add is a layer that computes the length of the top-layer\nactivation vectors, and this gives you the estimated class probabilities. You could then\njust train the network by minimizing the cross-entropy loss, as in a regular classification neural\nnetwork, and you would be done. However, in the paper they use a margin loss\nthat makes it possible to detect multiple classes in the image. So without going into\ntoo much details, this margin loss is such that if an object of class k is present in\nthe image, then the corresponding top-level capsule should output a vector whose\nlength is at least 0.9. It should be long. Conversely, if an object of class k is not\npresent in the image, then the capsule should output a short vector, one whose length\nis shorter than 0.1. So the total loss is the sum of losses for all classes. In the paper, they also add a decoder network\non top of the capsule network. It\u2019s just 3 fully connected layers with a sigmoid activation\nfunction in the output layer. It learns to reconstruct the input image by minimizing\nthe squared difference between the reconstructed image and the input image. The full loss is the margin loss we discussed\nearlier, plus the reconstruction loss (scaled down considerably so as to ensure that the\nmargin loss dominates training). The benefit of applying this reconstruction loss is that\nit forces the network to preserve all the information required to reconstruct the image,\nup to the top layer of the capsule network, its output layer. This constraint acts a bit\nlike a regularizer: it reduces the risk of overfitting and helps generalize to new examples. And that\u2019s it, you know how a capsule network\nworks, and how to train it. Let\u2019s look a little bit at some of the figures in the paper,\nwhich I find interesting. This is figure 1 from the paper, showing a\nfull capsule network for MNIST. You can see the first two regular convolutional layers,\nwhose output is reshaped and squashed to get the activation vectors of the primary capsules.\nAnd these primary capsules are organized in a 6 by 6 grid, with 32 primary capsules in\neach cell of this grid, and each primary capsule outputs an 8-dimensional vector. So this first\nlayer of capsules is fully connected to the 10 output capsules, which output 16 dimensional\nvectors. The length of these vectors is used to compute the margin loss, as explained earlier. Now this is figure 2 from the paper. It shows\nthe decoder sitting on top of the capsnet. It is composed of 2 fully connected ReLU layers\nplus a fully connected sigmoid layer which outputs 784 numbers that correspond to the\npixel intensities of the reconstructed image (which is a 28 by 28 pixel image). The squared\ndifference between this reconstructed image and the input image gives the reconstruction\nloss. Right, and this is figure 4 from the paper.\nOne nice thing about capsule networks is that the activation vectors are often interpretable.\nFor example, this image shows the reconstructions that you get when you gradually modify one\nof the 16 dimensions of the top layer capsules\u2019 output. You can see that the first dimension\nseems to represent scale and thickness. The fourth dimension represents a localized skew.\nThe fifth represents the width of the digit plus a slight translation to get the exact\nposition. So as you can see, it\u2019s rather clear what most of these parameters do. Okay, to conclude, let\u2019s summarize the pros\nand cons. Capsule networks have reached state of the art accuracy on MNIST. On CIFAR10,\nthey got a bit over 10% error, which is far from state of the art, but it\u2019s similar\nto what was first obtained with other techniques before years of efforts were put into them,\nso it\u2019s still a good start. Capsule networks require less training data. They offer equivariance,\nwhich means that position and pose information are preserved. And this is very promising\nfor image segmentation and object detection. The routing by agreement algorithm is great\nfor crowded scenes. The routing tree also maps the hierarchy of objects parts, so every\npart is assigned to a whole. And it\u2019s rather robust to rotations, translations and other\naffine transformations. The activation vectors somewhat are interpretable. And finally, obviously,\nit\u2019s Hinton\u2019s idea, so don\u2019t bet against it. However, there are a few cons: first, as I\nmentioned the results are not yet state of the art on CIFAR10, even though it\u2019s a good\nstart. Plus, it\u2019s still unclear whether capsule networks can scale to larger images,\nsuch as the ImageNet dataset. What will the accuracy be? Capsule networks are also quite\nslow to train, in large part because of the routing by agreement algorithm which has an\ninner loop, as you saw earlier. Finally, there is only one capsule of any given type in a\ngiven location, so it\u2019s impossible for a capsule network to detect two objects of the\nsame type if they are too close to one another. This is called crowding, and it has been observed\nin human vision as well, so it\u2019s probably not a show-stopper. All right! I highly recommend you take a look\nat the code of a CapsNet implementation, such as the ones listed here (I\u2019ll leave the\nlinks in the video description below). If you take your time, you should have no problem\nunderstanding everything the code is doing. The main difficulty in implementing CapsNets\nis that it contains an inner loop for the routing by agreement algorithm. Implementing\nloops in Keras and TensorFlow can be a little bit trickier than in PyTorch, but it can be\ndone. If you don\u2019t have a particular preference, then I would say that the PyTorch code is\nthe easiest to understand. And that\u2019s all I had, I hope you enjoyed\nthis video. If you did, please thumbs up, share, comment, subscribe, blablabla. It\u2019s\nmy first real YouTube video, and if people find it useful, I might make some more. If\nyou want to learn more about Machine Learning, Deep Learning and Deep Reinforcement Learning,\nyou may want to read my O\u2019Reilly book Hands-on Machine Learning with Scikit-Learn and TensorFlow.\nIt covers a ton of topics, with many code examples that you will find on my github account,\nso I\u2019ll leave the links in the video description. That\u2019s all for today, have fun and see you\nnext time!","2Kawrd5szHE":"Hi, I\u2019m Aur\u00e9lien G\u00e9ron, and today I\u2019m\ngoing to show you how to implement a capsule network using TensorFlow. In my previous video, I presented the key\nideas behind capsule networks, a recently published neural net architecture. If you\nhaven\u2019t seen this video, I encourage you to do so now, today I will focus on the TensorFlow\nimplementation. I wrote a Jupyter notebook containing all\nthe code and detailed explanations, and I published it on my github account (as always\nI\u2019ll put all the links in the video description below), so I encourage you to clone it, and\nplay with it. So, it reaches over 99.4% accuracy on the\ntest set, which is pretty good, considering it\u2019s a shallow network with just two capsule\nlayers and a total of about 1,200 capsules. There\u2019s a lot of code in this notebook,\nso I won\u2019t go through every single line in this video, but I\u2019ll explain the main\ndifficulties I came across, and hopefully this will be useful to you for other TensorFlow\nimplementations, not just CapsNets. Okay, let\u2019s build the network. First, we\nneed to feed the input images to the network. And that\u2019s our input layer. We implement it using a simple TensorFlow\nplaceholder. The batch size is unspecified, so that we can pass any number of images in\neach batch, in this example, 5. Note that we directly send 28x28 pixel images, with\na single channel, since the images are greyscale. Color images would typically have 3 channels,\nfor red, green and blue. And that\u2019s it for the input layer. Next, let\u2019s build the primary capsule layer.\nFor each digit in the batch it will output 32 maps, each containing a 6x6 grid of 8 dimensional\nvectors. The capsules in this particular map seem to\ndetect the start of a line segment. You can see that the output vectors are long in the\nlocations where there\u2019s a start of a line. And the orientation of the 8D vector gives\nthe pose parameters, in this case, I\u2019ve represented the rotation angle, but the vector\u2019s\n8 dimensional orientation would also capture things like the thickness of the line, the\nprecise location of the start of the line relative to the cell in the 6x6 grid, and\nso on. The implementation is really straightforward.\nFirst, we define two regular convolutional layers. The input of the first layer is X,\nthe placeholder that will contain the input images we will feed at runtime. The second\nlayer takes the output of the first layer. And we use the parameters specified in the\npaper. The second layer is configured to output 256 feature maps. And each feature map contains\na 6x6 grid of scalars. We want a 6x6 grid of vectors instead, so we use TensorFlow\u2019s\nreshape() function to get 32 maps of 8 dimensional vectors, instead of 256 maps of scalars. In\nfact, since the primary capsules will be fully connected to the digit capsules, we can simply\nreshape to one long list of 1,152 output vectors (that\u2019s 32*6*6), for each instance in the\nbatch. And the last step is to squash the vectors to ensure that their length is always\nbetween 0 and 1. For this, we use a home made squash function. There it is. This function implements the squash equation\ngiven in the paper. It squashes every vector in an array, along\nthe specified dimension, by default the last one. So, as you can see, it involves a division\nby the norm of the vector, so there\u2019s a risk of a division by zero if at least one\nof the vectors is a zero vector. So you could just add a tiny epsilon value\nin the denominator, and it would fix the division by zero problem. However you would still run\ninto another issue. The norm of a vector has no defined gradients when the vector is zero.\nSo if you just use tensorflow\u2019s norm() function to compute the norm in this equation, then\nif at least one of the vectors is zero, the gradients will be undefined (it will return\nn-a-n, nan, not a number). So, as a result, when gradient descent updates the weights\nof our model, the weights will end up being undefined as well. The model would effectively\nbe dead. You don\u2019t want that. So the trick is to compute a safe approximation\nof the norm, shown in the equation on the right. And, that's about it, that\u2019s all for the\nprimary capsules. Apart for computing the norm safely, it was pretty straightforward. On to the next layer where all the complexity\nis: the digit capsules. There are just 10 of them, one for each digit, 0 to 9, and they\noutput 16 dimensional vectors. In this particular example, you can see that the longest output\nvector is the one for digit 4. And again its orientation in the 16 dimensional space gives\ninformation about the pose of this digit, its rotation, its thickness, its skew, its\nposition, and so on. By the way, note that most of the position information in the first\nlayer was encoded in the location of the active capsules in the 6x6 grid. So, for example,\nif I shift the digit 4 slightly to the left in the input image then different capsules\nin the first layer get activated. See? So, the output of these first layer capsules only\ncontain local shift information, relative to the position of the capsule in the 6x6\ngrid. But in the second capsule layer, the full position information is now encoded in\nthe orientation of the output vector in 16 dimensional space. Okay, now let\u2019s see how to implement this\nlayer. The first step is to compute the predicted output vectors. Since this second layer is\nfully connected to the first layer, we will compute one predicted output for each pair\nof first and second layer capsules. For example, using the output of the first\nprimary capsule, we can predict the output vector of the first digit capsule. For this, we just use a transformation matrix\nW_1,1, which will gradually be learned during training, and we multiply it by the output\nof the first layer capsule. This gives us \u00fb_1|1, which is the predicted output of the\nfirst digit capsule, based on the output of the first primary capsule. Since the primary\ncapsules output 8 dimensional vectors, and the digit capsules output 16 dimensional vectors,\nthe transformation matrix W_1,1 must be a 16x8 matrix. Next, we try to predict the output of the\nsecond digit capsule, still based on the output of the first primary capsule. Note that we\nare using a different transformation matrix, W_1,2. And we do the same for the third digit capsule,\nusing W_1,3. And so on for all the digit capsules. Then we move on to the second primary capsule,\nand we use its output to predict the output of the first digit capsule. And so on for all the digit capsules. Then we move on to the third primary capsule,\nwe make 10 predictions. And so on, you get the picture. There are\n1,152 primary capsules (multiply 6 * 6 * 32), and 10 digit capsules, so we end up with 11,520\npredicted output vectors. Now we could just compute them one by one, but it would be terribly\ninefficient. Let\u2019s see how we can get all the predicted output vectors in just one matmul()\noperation. Now you know that TensorFlow\u2019s matmul() function lets you multiply two matrices,\nbut you may not know that you can also use it to multiply many matrices in one shot.\nThis will be incredibly efficient, especially if you are using a GPU card, because it will\nperform all the matrix multiplications in parallel in many different GPU threads. So here\u2019s how it works. Suppose A, B, C,\nD, E, F and G, H, I, J, K, L, are all matrices. You can put these matrices in two arrays,\neach with 2 rows and 3 columns, for example. So we have 2 dimensions for this 2x3 grid\nof matrices, and each matrix is 2 dimensional, so these arrays are 2+2=4 dimensional arrays.\nIf you pass these arrays, these 4D arrays, to matmul(), it will perform an elementwise\nmatrix multiplication, so the result will be this 4 dimensional array containing A multiplied\nby G, here, B multiplied by H, here, and so on. So let\u2019s use this to compute all the\npredicted output vectors. We can create a first 4D array containing\nall the transformation matrices: there\u2019s one row per primary capsule, and one column\nper digit capsule. The second array must contain the output vectors of each primary capsule.\nThen we just pass these two arrays to the matmul() function, and it gives us the predicted\noutput vectors for all the pairs of primary and digit capsules. Since we need to predict the outputs of all\n10 digit capsules for each primary capsule, this array must contain 10 copies of the primary\ncapsules\u2019 outputs. We will use the tile function to replicate the first column of\noutput vectors, 10 times. But there\u2019s one additional catch. We want\nto make these predictions for all the instances in the batch, not just one instance. So there\u2019s\nan additional dimension for the batch size. It turns out that the primary output vectors\nwere already computed for every single instance, so the second array is fine. It already has\nthis dimension. But we need to replicate the 4D array containing\nall the transformation matrices, so that we end up with one copy per instance in the batch.\nNow if you understand this, then the code should be pretty clear. First, we create a variable containing all\nthe transformation matrices. It has one row per primary capsule, one column per digit\ncapsule, and it contains 16 by 8 matrices. That\u2019s 4 dimensions, and we add another\ndimension at the beginning of size one at the beginning to make it easy to tile this\narray for each instance in the batch. The variable is initialized randomly, using a\nnormal distribution of standard deviation 0.01 (that\u2019s a hyperparameter you can tweak).\nAnd that's about it. Create this variable! Next we want to tile this array for each instance,\nso first we need to know the batch size. We don\u2019t actually know it at graph construction\ntime, it will only be known when we run the graph. But we can use TensorFlow\u2019s shape()\nfunction: it creates a tensor that *will* know the shape at runtime, and we grab its\nfirst dimension, which is the batch size. Then we simply tile our big W array, along\nthe first dimension, to get one copy per instance. Now, recall that the output of the primary\ncapsules was a 3 dimensional array: the first dimension is the batch size, that we will\nknow at runtime, then there\u2019s one row per capsule, and each capsule has 8 dimensions.\nSo we need to reshape this array a bit to get the shape that we are looking for, to\ndo the big matmul() operation. First we add an extra dimension at the end,\nusing TensorFlow\u2019s expand_dims() function. The vectors are now represented as column\nvectors, instead of 1 dimensional arrays. Each of these is a column vector. A column\nvector is a matrix, a 2D array, with a single column. Then we add another dimension, for the digit\ncapsules. And we replicate all the output vectors 10\ntimes across this new dimension, once per digit capsule. And lastly, we just use matmul to multiply\nthe transformation matrices with the primary capsules\u2019 output vectors, and we get all\nthe digit capsule\u2019s predicted outputs for each pair of primary and digit capsules, and\nfor each instance in the batch. In one shot. And that\u2019s the end of the first step for\ncomputing the digit capsules\u2019 outputs, we now have a bunch of predicted output vectors.\nThe second step is the routing by agreement algorithm. So first, we set all the raw routing weights\nto 0. For this, we just use tf.zeros(). There\u2019s one weight for each pair of primary and digits\ncapsules, for each instance. The last two dimensions here are equal to 1, they will\nbe useful in a minute. Next we compute the softmax of each primary\ncapsule\u2019s 10 raw routing weights. Okay? So softmax happens along this dimension. Next, we compute the weighted sum of all the\npredicted output vectors, for each digit capsule, using the routing weights. The weighted sum\nis along this dimension. This is pretty straightforward TensorFlow\ncode: first multiply the routing weights and the predicted vectors (this is an elementwise\nmultiplication, not a matrix multiplication), then just compute the sum over the primary\ncapsule dimension. And the two dimensions we added earlier for the routing weights are\nuseful in the multiplication step, so that the two arrays have the same number of dimensions,\nthe same rank. They don\u2019t have the exact same shape, but they have compatible shapes\nso TensorFlow will perform broadcasting. Now if you don't know what broadcasting is,\nthis should make it clear. I\u2019m multiplying two matrices, but one of them just has one\nrow, so TensorFlow will act as if this row were repeated the appropriate number of times.\nYou could achieve the same thing using tiling, as we did earlier, but this is more efficient.\nYou may wonder why we didn\u2019t use broadcasting earlier, but the reason is it does not work\nfor matrix multiplication. Here we are doing elementwise multiplication. Okay, back to the digit capsules. We computed\na weighted sum of the predicted vectors, for each digit capsule, and we just run the squash\nfunction, and we get the outputs of the digit capsules. Hurray! But wait, this is just the\nend of round 1 of the routing by agreement algorithm. Now, on to round #2. So first, we need to measure how good each\nprediction was, and use this to update the routing weights. For example, look at the predictions that\nwe made using this primary capsule\u2019s output. Notice that, for example, the prediction for\ndigit 4, is excellent. And this is measured using the scalar product\nof the predicted output vector and the actual output vector. These two vectors are actually represented\nas column vectors, meaning a matrix with a single column. So to compute the scalar product,\nwe must transpose the predicted column vector \u00fb_j|i to get a row vector, and multiply this\nrow vector and the actual output vector v_j, which is a column vector. We will get a 1x1\nmatrix containing the scalar product of the vectors. Now of course we need to do this\nfor each predicted vector, so once again, we can use the matmul() function to perform\nall the matrix multiplications in just one shot. First we must use the tile() function to get\none copy of the actual output vectors v_j, for each primary capsule. Then we use matmul(),\ntelling it to transpose each matrix in the first array, on the fly, and lo-and-behold,\nwe get all the scalar products at once. So now, we have a measure of the agreement between\neach predicted vector and the actual output vector. We can then add these scalar products to the\nraw weights, using a simple addition. And the rest of round 2 is exactly the same\nas the end of round 1. The code is really identical, except we\u2019re now using the raw\nrouting weights of round 2. We compute their softmax to get the actual routing weights\nfor round 2, then we compute the weighted sum of all the predicted vectors for each\ndigit capsule, and finally we squash the result. And now we have the new digit capsule outputs,\nand we\u2019ve finished round 2. We could do a few more rounds exactly like this one, but\nI\u2019ll stop now, and use the current output vectors at the end of round 2 as the output\nof the digit capsules. Now you probably noticed that I implemented\nthe routing algorithm\u2019s loop without an actual loop. It\u2019s a bit like computing the\nsum of squares from 1 to 100, with this code. Of course, this will build a very big TensorFlow\ngraph. But it works. You can think of it as an unrolled loop. Now, a cleaner way to do\nthis would be to write a for loop in Python, like this. Ah, much better. However, it\u2019s important\nto understand that the resulting TensorFlow graph will be absolutely identical to the\none produced by the previous code. All we are doing here is constructing a graph, and\nTensorFlow will not even know that we used a loop to build the graph. Again, this works\nfine, it\u2019s just that you end up with a very large graph. So you can think of this loop\nas a static loop, that only runs at graph construction time. If you want a dynamic loop,\none that TensorFlow itself will run, then you must use TensorFlow\u2019s while_loop() function\nlike this. The while_loop() function takes 3 parameters:\nthe first one is a function that must return a tensor that will determine whether the loop\nshould go on or not, at each iteration. The second parameter is also a function that must\nbuild the body of the loop, that will also be evaluated at each iteration. And finally,\nthe third parameter contains a list of tensors that will be sent to both the condition()\nand loop_body() functions at the first iteration. For the following iterations, these functions\nwill receive the output of the loop_body() function. So you can pause the video if you\nneed to take a closer look at this code. Once you get it, you can try modifying my capsnet\nimplementation to use a dynamic loop rather than a static unrolled loop. Apart from making\nthe code cleaner and the graph smaller, using a dynamic loop allows you to change the number\nof iterations using the exact same model. Also, if you set the swap_memory parameter\nof the while_loop() function, if you set it to True, TensorFlow will automatically swap\nthe GPU memory to CPU memory to save GPU memory. Since CPU RAM is much cheaper and abundant,\nthis can really be useful. And that\u2019s it! We\u2019ve computed the output\nof the digit capsules. Cool! Now the length of each output vector represents the probability\nthat a digit of that class is present in the image. So, let\u2019s compute these probabilities. For\nthis, we can\u2019t use tensorflow\u2019s norm() function because training will explode if\nthere\u2019s a zero vector at any point, as I mentionned earlier. So instead we use a home-made\nsafe_norm() function, similar to what we did with the squash() function. And note that\nthe sum of the probabilities don\u2019t necessarily add up to 1, because we are not using a softmax\nlayer. This makes it possible to detect multiple different digits in the same image (but they\nall have to be different digits: you can detect a 5 and 3, but you can\u2019t detect, say, two\n5s). Next, let\u2019s predict the most likely digit. We just use the argmax() function that gives\nuse the index of the highest probability. The index happens to be the number, the digit\nitself. Note that we first get a tensor that has a couple extra dimensions of size 1 at\nthe end, so we get rid of them using the squeeze() function. If we called squeeze() without specifying\nthe axes to remove, it would remove all dimensions of size 1. This would generally be okay, except\nif the batch size was equal to one, in which case, we would be left with a scalar value,\nrather than an array, and we don\u2019t want that. So it's better to specify the axes. Great, now we have a capsule network that\ncan estimate class probabilities and make predictions. We can measure the model\u2019s accuracy on the\nbatch by simply comparing the predictions and the labels. In this case the prediction\nfor the last digit in the batch is wrong, it\u2019s 7 instead of 1. So we get 80% accuracy. The code is really straightforward: we just\nuse the equal() function to compare the labels and the predictions y_pred, and this gives\nus an array of booleans, so we cast these booleans to floats, which gives us a bunch\nof 0s (for bad predictions) and 1s (for good predictions), and we compute the mean to get\nthe batch\u2019s accuracy. The labels y are just a regular placeholder. Nothing special. And that\u2019s it, we have a full model, able\nto make predictions. Now let\u2019s look at the training code. This diagram is about to get\npretty crowded so I\u2019ll remove the accuracy for clarity. And now, first, we want to compute the margin\nloss. It\u2019s given by this equation. By the way,\nI made a mistake in my first video: I squared the norms instead of the max operations. Sorry\nabout that. This here is the correct equation. Computing it is pretty straightforward, so\nI won\u2019t go through it in details. The only trick is to understand how you can easily\ncompute all the T_k values. For a given instance, T_k is equal to 1 if a digit of class k is\npresent in the image, otherwise it\u2019s equal to 0. So, you can get all the T_k values for each\ninstance by simply converting the labels to a one-hot representation. For example, if\nan instance\u2019s label is 3, then for this instance, T will contain a 10 dimensional\nvector full of zeros except for a 1 at index 3. Okay! Next, we want to compute the reconstruction\nloss. So first, we must send the outputs of the\ndigit capsules to a decoder that will try to use them to reconstruct the input images. This decoder is just a regular feedforward\nneural net composed of 3 fully connected layers. It\u2019s really simple code, so you can pause\nthe video if you want to take a close look at it. It outputs an array containing 784\nvalues from 0 to 1, for each instance, representing the pixel intensities of 28x28 pixel images. And that\u2019s it, we have our reconstructed\nimages! We can now compute the reconstruction loss. This is just the squared difference between\nthe input images and their reconstructions. Since the input images are 28x28x1, we first\nreshape them to one dimension per instance with 784 values each. Then we compute the\nsquared difference. Now we can compute the final loss! It\u2019s just the sum of the margin loss and\nthe reconstruction loss, scaled down to let the margin loss dominate training. Pretty\nsimple, as you can see. Now let\u2019s add the training operation. The paper mentions they used TensorFlow\u2019s\nimplementation of the Adam optimizer, using the default parameters, so let\u2019s do that. We create the optimizer, and call its minimize()\nmethod to get the training operation that will tweak the model parameters to minimize\nthe loss. We\u2019re almost done, but there\u2019s one last detail I didn\u2019t mention in the\nfirst video. The paper indicates that the outputs of the digit capsules should all be\nmasked out except for the ones corresponding to the target digit. So instead of sending the digit capsules\u2019\noutputs directly to the decoder, we want to apply a mask first, like this. The mask will have the same shape as the digit\ncapsules output array, and it will be equal to 0 everywhere except for 1s at the location\nof the target digits. By multiplying the digit capsules\u2019 output\nand the mask, we get the input to the decoder. But there\u2019s one catch. This picture is good\nfor training, but at test time, we won\u2019t have the labels. So instead, we will mask\nthe output vectors using the predicted classes rather than the labels, like this. Now we could build a different graph for training\nand for testing, but it wouldn\u2019t be very convenient. So instead, let\u2019s build a condition\noperation. We will add a boolean placeholder called mask_with_labels.\nIf it is true, then we use the labels to build the mask. If it is False, then we use the prediction.\nNote the difference. Okay. And here\u2019s the code. We build the mask_with_labels\nplaceholder, which will default to False so that we only need to set it during training.\nAnd then we define the reconstruction targets using TensorFlow\u2019s cond() function. It takes\n3 arguments: the first one is a tensor representing the condition, in this case simply the mask_with_labels\nplaceholder. The second parameter is a function that returns the tensor to use if the condition\nis True, and the third parameter is a function that returns the tensor to use if the condition\nif False. Then to build the mask, we simply use the one_hot() function. Now there actually\none slight problem with this implementation, and to explain it, I need to step back for\na second and talk about how TensorFlow evaluates a tensor. Suppose we built this graph, these are all\ntensorflow operations, and we want to evaluate the output of operation A. The first thing TensorFlow will do is resolve\nthe dependencies. It will find all the operations that A depends\non, directly or indirectly, by traversing the graph backwards. In this case it will\nfind C, D and F. Next, it will run any operation that has no\ninputs. These are called root nodes. In this case F. Once F is evaluated, operations C and D now\nhave all the inputs they need, so they can be evaluated, and TensorFlow will actually\ntry to run them in parallel. Say D finishes first, A still has one unevaluated\ninput so it can\u2019t run yet. But as soon as C is finished, A can be evaluated. And once it\u2019s done, the eval() method returns\nthe result, and we\u2019re good. You can actually evaluate multiple operations\nat once, for example A and E, and the process is really the same.\nIt finds all the dependencies, runs the root operations, and then, you know, goes upward\nrunning every operation whose inputs are satisfied. And once it's got both the values for both\nA and E, it returns the results. So let\u2019s apply this to our reconstruction\ntargets. This tensor is the output of the cond() operation, which has 3 parameters,\nmask_with_labels, a function that returns y, and a function that returns y_pred. And when we evaluate the reconstruction_targets,\nor any tensor that depends on it, such as the final loss, which depends on the reconstruction\nloss, which eventually depends on the reconstruction_targets. Well, what happens it, as earlier, TensorFlow\nstarts by resolving the dependencies. It finds all three bottom nodes. And it evaluates them all! So y_pred may finish first. Since these operations\nare run in parallel, there\u2019s no way to know in which order they will finish. So, you know, y may finish next.\nAnd finally mask_with_labels finishes. So suppose it evaluates to True. Now the reconstruction_targets\nhas all the inputs it needs, so it can be evaluated. And of course it does the right thing, since\nmask_with_labels is True, it returns the value of y. Which is good.\nBut notice that y_pred was evaluated for nothing, we\u2019re not using its output. It\u2019s not a\nbig deal since during training we need to evaluate the margin loss, which depends on\nthe estimated class probabilities, which is just one step away from the predictions, so\ncomputing the predictions won\u2019t add much overhead. But still, it\u2019s a bit unfortunate. Now suppose mask_with_labels evaluates to\nFalse. Then, again, the reconstruction_targets will\ndo the right thing, it will output the value of y_pred. But this time, we evaluated y for nothing.\nIt\u2019s just a placeholder, so it won\u2019t add much computation time, but it means that we\nmust feed y, even if mask_with_labels is False. Well actually we can just pass an empty array,\nthat's fine. So it will work, but it's kind of ugly. So this unfortunate situation is due to the\nfact that the functions we passed to the cond() function, do not actually create any tensor,\nthey just return tensors that were created outside of these functions. If you build tensors\nwithin these functions, then TensorFlow will do what you expect. It will stitch together\nthe partial graphs created by these functions into a graph that will properly handle the\ndependencies. So you might be able to modify my code and fix this ugliness. I tried, but\nI ended up with pretty convoluted code, so I decided to stick with this implementation.\nI hope it won\u2019t keep you awake at night. Sooo! We\u2019ve actually finished! Here\u2019s\nthe full picture again. The construction phase is over, our graph is built, now on to the\nexecution phase, let\u2019s run this graph. First, let\u2019s look at the training code. It\u2019s really really completely standard.\nWe create a session, if a checkpoint file exists, load it, or else initialize all the\nvariables. Then run the main training loop, for a number of epochs, and for each epoch,\nrun enough iterations to go through the full training set. And inside this loop, we simply\nevaluate the training operation, and the loss, on the next batch. We feed the images and\nthe labels of the current training batch, and we set mask_with_labels to True. That\u2019s\npretty much all there is to it. In the notebook, I also added a simple implementation of early\nstopping, and I print out the progress, plus I evaluate the model on the validation set\nat the end of each epoch. But the most important part is here. After training, I just run a few test images\nthrough the network and get the predictions and reconstructions. As you can see, the predictions\nare all correct, and the reconstructions are pretty good, they're pretty close to the original\nimages, except that they\u2019re slightly fuzzier, as you can see. Here\u2019s the code, there\u2019s really nothing\nspecial about it: I take a few test images, I start a session and load the model, then\nI just evaluate the predictions and the decoder\u2019s output (and I also get the capsules\u2019 outputs\nso I can tweak them later). The ugliness I mentionned earlier is right here. I'm forced\nto pass an empty array. This value will be ignored anyway. And finally, the code tweaks the output vectors\nand passes the result to the decoder. So we can see what each of of the 16 dimensions\nrepresent in the digit capsule\u2019s output vector. For example, this image shows the\nreconstructions we get by tweaking the first parameter (the notebook produces one such\nimage for each of the 16 parameters). And as you can see, in the first, second and last\nrows, we see that the digits become thinner and thinner, going to the right, or thicker\nand thicker to the left, so it holds information about thickness. And in the middle row, you\ncan see that the bottom part of the number 5 gets lifted towards the top, so probably\nthat's what this parameter does for this digit. Before I finish, I\u2019d like to thank everyone\nwho shared and commented on my first video, I really had no idea it would receive such\nan enthusiastic response, and I\u2019m very very grateful to all of you, it definitely motivates\nme to create more videos. If you want to learn more about Machine Learning and support this\nchannel, check out my O\u2019Reilly book Hands-on Machine Learning with Scikit-Learn and TensorFlow,\nI\u2019ll leave the links in the video description. If you speak German, there\u2019s actually a\nGerman translation coming up for Christmas. And if you speak French, the translation is\nalready available. It was split in two books but it\u2019s really the same content.\nAnd that\u2019s all I had for today, I hope you enjoyed this video and that you learned a\nthing or two about TensorFlow and capsule networks. If you did, please, like, share,\ncomment, subscribe, and click on the bell icon next to the subscribe button, to receive\nnotifications when I upload new videos. See you next time!"},"diffs":{"pPN8d0E3900":[{"-":["hey","I'm","overly","Asian"],"+":["Hey!","I\u2019m","Aur\u00e9lien","G\u00e9ron,"]},"and","in","this","video",{"-":["I'll"],"+":["I\u2019ll"]},"tell","you","all","about",{"-":["capsule","networks"],"+":["Capsule","Networks,"]},"a","hot","new","architecture","for","neural",{"-":["nets","Jeffrey","Henson"],"+":["nets.","Geoffrey","Hinton"]},"had","the","idea","of",{"-":["capsule","Network"],"+":["Capsule","Networks"]},"several","years",{"-":["ago"],"+":["ago,"]},"and","he","published","a","paper","in","2011","that","introduced","many","of","the","key",{"-":["ideas"],"+":["ideas,"]},"but","he","had","a","hard","time","making","them","work",{"-":["properly"],"+":["properly,"]},"until",{"-":["now","a"],"+":["now.","A"]},"few","weeks",{"-":["ago"],"+":["ago,"]},"in","October",{"-":["2017"],"+":["2017,"]},"a","paper","called",{"-":["dynamic","routing","between","capsules"],"+":["\u201cDynamic","Routing","Between","Capsules\u201d"]},"was","published","by",{"-":["Sarah","saber"],"+":["Sara","Sabour,"]},"Nicholas",{"-":["frost"],"+":["Frosst"]},"and","of","course","Geoffrey",{"-":["Hinton","they"],"+":["Hinton.","They"]},"managed","to","reach",{"-":["state-of-the-art"],"+":["state","of","the","art"]},"performance","on","the",{"-":["MS","dataset"],"+":["MNIST","dataset,"]},"and","demonstrated","considerably","better","results","than","convolutional","neural","nets","on","highly","overlapping",{"-":["digits","so"],"+":["digits.","So"]},"what","are","capsule","networks",{"-":["exactly","well"],"+":["exactly?","Well,"]},"in","computer",{"-":["graphics"],"+":["graphics,"]},"you","start","with","an","abstract","representation","of","a",{"-":["scene"],"+":["scene,"]},"for","example","a","rectangle","at","position",{"-":["X","20"],"+":["x=20"]},"and",{"-":["y","equals","30"],"+":["y=30,"]},"rotated","by",{"-":["16","degrees","and"],"+":["16\u00b0,"]},"and","so",{"-":["on","each"],"+":["on.","Each"]},"object","type","has","various","instantiation",{"-":["parameters","then"],"+":["parameters.","Then"]},"you","call","some","rendering",{"-":["function"],"+":["function,"]},"and",{"-":["boom"],"+":["boom,"]},"you","get","an",{"-":["image","inverse","graphics"],"+":["image.","Inverse","graphics,"]},"is","just","the","reverse",{"-":["process","you"],"+":["process.","You"]},"start","with","an",{"-":["image"],"+":["image,"]},"and","you","try","to","find","what","objects","it",{"-":["contains"],"+":["contains,"]},"and","what","their","instantiation","parameters",{"-":["are","a"],"+":["are.","A"]},"capsule",{"-":["Network"],"+":["network"]},"is","basically","a","neural","network","that","tries","to","perform","inverse",{"-":["graphics","it"],"+":["graphics.","It"]},"is","composed","of","many",{"-":["capsules","a"],"+":["capsules.","A"]},"capsule","is","any","function","that","tries","to","predict","the","presence","and","the","instantiation","parameters","of","a","particular","object","at","a","given",{"-":["location","for","example"],"+":["location.","For","example,"]},"the","network","above","contains","50",{"-":["capsules","the"],"+":["capsules.","The"]},"arrows","represent","the","output","vectors","of","these",{"-":["capsules"],"+":["capsules.","The"]},"capsules","output",{"-":["vectors","the"],"+":["vectors.","The"]},"black","arrows","correspond","to","capsules","that","try","to","find",{"-":["rectangles"],"+":["rectangles,"]},"while","the","blue","arrows","represent","the","output","of","capsules","looking","for",{"-":["triangles","the"],"+":["triangles.","The"]},"length","of","an","activation","vector","represents","the","estimated","probability","that","the","object","the","capsule","is","looking","for","is","indeed",{"-":["present","so","you"],"+":["present.","You"]},"can","see","that","most","arrows","are",{"-":["tiny"],"+":["tiny,"]},"meaning","the","capsules",{"-":["didn't"],"+":["didn\u2019t"]},"detect",{"-":["anything"],"+":["anything,"]},"but","two","arrows","are","quite",{"-":["long","this"],"+":["long.","This"]},"means","that","the","capsules","at","these","locations","are","pretty","confident","that","they","found","what","they","were","looking",{"-":["for"],"+":["for,"]},"in","this","case","a",{"-":["rectangle"],"+":["rectangle,"]},"and",{"-":["the","triangle","so"],"+":["a","triangle.","Next,"]},"the","orientation","of","the","activation","vector","encodes","the","instantiation","parameters","of","the",{"-":["object"],"+":["object,"]},"for","example","in","this","case","the",{"-":["object's","rotation"],"+":["object\u2019s","rotation,"]},"but","it","could","be","also","its",{"-":["thickness"],"+":["thickness,"]},"how","stretched","or","skewed","it",{"-":["is"],"+":["is,"]},"its","exact",{"-":["location","it"],"+":["position","(there"]},"might","be","slight",{"-":["translations"],"+":["translations),"]},"and","so",{"-":["on","for","simplicity","I'll"],"+":["on.","For","simplicity,","I\u2019ll"]},"just","focus","on","the","rotation",{"-":["parameter"],"+":["parameter,"]},"but","in",{"-":[],"+":["a"]},"real","capsule",{"-":["Network"],"+":["network,"]},"the","activation","vectors","may","have",{"-":["five","ten"],"+":["5,","10"]},"dimensions","or",{"-":["more","in","practice"],"+":["more.","In","practice,"]},"a","good","way","to","implement","this","is","to",{"-":[],"+":["first"]},"apply","a","couple","convolutional",{"-":["layers"],"+":["layers,"]},"just","like","in","a","regular","convolutional","neural",{"-":["net","this"],"+":["net.","This"]},"will","output","an","array","containing","a","bunch","of","feature",{"-":["maps","you"],"+":["maps.","You"]},"can","then","reshape","this","array","to","get","a","set","of","vectors","for","each",{"-":["location","for","example"],"+":["location.","For","example,"]},"suppose","the","convolutional","layers","output","an","array",{"-":["containing","say"],"+":["containing,","say,"]},"18","feature","maps",{"-":["2"],"+":["(2"]},"times",{"-":["9"],"+":["9),"]},"you","can","easily","reshape","this","array","to","get",{"-":["two"],"+":["2"]},"vectors","of",{"-":["nine"],"+":["9"]},"dimensions",{"-":["each"],"+":["each,"]},"for","every",{"-":["location","you"],"+":["location.","You"]},"could","also","get",{"-":["three"],"+":["3"]},"vectors","of",{"-":["six"],"+":["6"]},"dimensions",{"-":["each"],"+":["each,"]},"and","so",{"-":["on","something"],"+":["on.","Something"]},"that","would","look","like","the","capsule","network","represented","here","with","two","vectors","at","each",{"-":["location","the"],"+":["location.","The"]},"last","step","is","to","ensure","that","no","vector","is","longer","than",{"-":["one"],"+":["1,"]},"since","the",{"-":["vectors"],"+":["vector\u2019s"]},"length","is","meant","to","represent","a",{"-":["probability"],"+":["probability,"]},"it","cannot","be","greater","than",{"-":["one","to"],"+":["1.","To"]},"do",{"-":["this"],"+":["this,"]},"we","apply","a","squashing",{"-":["function","it"],"+":["function.","It"]},"preserves","the",{"-":["vectors","orientation"],"+":["vector\u2019s","orientation,"]},"but","it","squashes","it","to","ensure","that","its","length","is","between",{"-":["zero"],"+":["0"]},"and",{"-":["one","one"],"+":["1.","One"]},"key","feature","of",{"-":["capsule","networks"],"+":["Capsule","Networks"]},"is","that","they","preserve","detailed","information","about","the",{"-":["object's"],"+":["object\u2019s"]},"location","and","its",{"-":["pose"],"+":["pose,"]},"throughout","the",{"-":["network","for","example"],"+":["network.","For","example,"]},"if","I","rotate","the","image",{"-":["slightly"],"+":["slightly,"]},"notice","that","the","activation","vectors","also","change",{"-":["slightly","right","this"],"+":["slightly.","Right?","This"]},"is","called",{"-":["equivariance","in"],"+":["equivariance.","In"]},"a","regular","convolutional","neural",{"-":["net"],"+":["net,"]},"there","are","generally","several","pooling",{"-":["layers"],"+":["layers,"]},"and","unfortunately","these","pooling","layers","tend","to","lose",{"-":["information"],"+":["information,"]},"such","as","the","precise","location","and","pose","of","the",{"-":["objects","it's"],"+":["objects.","It\u2019s"]},"really","not","a","big","deal","if","you","just","want","to","classify","the","whole",{"-":["image"],"+":["image,"]},"but","it","makes","it","challenging","to","perform","accurate","image","segmentation","or","object","detection",{"-":["which"],"+":["(which"]},"require","precise",{"-":["you","know"],"+":[]},"location","and",{"-":["pose","the"],"+":["pose).","The"]},"fact","that","capsules","are",{"-":["equivariance"],"+":["equivariant"]},"makes","them","very","promising","for","these",{"-":["applications","alright"],"+":["applications.","All","right,"]},"so","now",{"-":["let's"],"+":["let\u2019s"]},"see","how","capsule","networks","can","handle","objects","that","are","composed","of","a","hierarchy","of",{"-":["parts","for","example"],"+":["parts.","For","example,"]},"consider","a","boat","centered","at","position",{"-":["X","equals","22"],"+":["x=22"]},"and",{"-":["y","equals","28"],"+":["y=28,"]},"and","rotated","by",{"-":["16","degrees","this"],"+":["16\u00b0.","This"]},"boat","is",{"-":["compotes","is"],"+":[]},"composed","of",{"-":["parts","in"],"+":["parts.","In"]},"this","case","one","rectangle","and","one",{"-":["triangle","so"],"+":["triangle.","So"]},"this","is","how",{"-":[],"+":["it"]},"would","be",{"-":["rent","rendered","now"],"+":["rendered.","Now"]},"we","want","to","do","the",{"-":["reverse"],"+":["reverse,"]},"we","want",{"-":["universe","graphics"],"+":["inverse","graphics,"]},"so","we","want","to","go","from","the","image","to","this","whole","hierarchy","of","parts","with","their","instantiation",{"-":["parameters","similarly"],"+":["parameters.","Similarly,"]},"we","could","also","draw","a",{"-":["draw","a","house"],"+":["house,"]},"using","the","same",{"-":["parts"],"+":["parts,"]},"a","rectangle",{"-":["in"],"+":["and"]},"a",{"-":["triangle","that"],"+":["triangle,","but"]},"this",{"-":["arm"],"+":["time"]},"organized","in","a","different",{"-":["way","so"],"+":["way.","So"]},"the","trick","will","be","to","try","to",{"-":["come","to"],"+":[]},"go","from","this","image","containing","a","rectangle",{"-":["in"],"+":["and"]},"a",{"-":["triangle"],"+":["triangle,"]},"and","figure",{"-":["out"],"+":["out,"]},"not","only","that","the","rectangle","and","triangle","are","at","this","location","and","this",{"-":["orientation"],"+":["orientation,"]},"but","also","that","they","are","part","of","a",{"-":["boat"],"+":["boat,"]},"not","a",{"-":["house","so","yeah","let's","let's"],"+":["house.","So,","yeah,","let\u2019s"]},"figure","out","how","it","would","do",{"-":["this","the"],"+":["this.","The"]},"first","step","we","have","already",{"-":["seen"],"+":["seen:"]},"we","run","a","couple",{"-":["of"],"+":[]},"convolutional",{"-":["layers"],"+":["layers,"]},"we","reshape","the","output","to","get",{"-":["vectors"],"+":["vectors,"]},"and","we","squash",{"-":["them","this"],"+":["them.","This"]},"gives","us","the","output","of","the","primary",{"-":["capsules","we've"],"+":["capsules.","We\u2019ve"]},"got","the","first","layer",{"-":["already","the"],"+":["already.","The"]},"next","step","is","where","most","of","the","magic","and","complexity","of","capsule","networks","takes",{"-":["place","every"],"+":["place.","Every"]},"capsule","in","the","first","layer","tries","to","predict","the","output","of","every","capsule","in","the","next",{"-":["layer","you"],"+":["layer.","You"]},"might","want","to","pause","to","think","about","what","this",{"-":["means","that","the"],"+":["means.","The"]},"capsules",{"-":["at","the","primary"],"+":["in"]},"the","first","layer",{"-":["tried"],"+":["try"]},"to","predict","what","the","second","layer","capsules","will",{"-":["output","for","example","let's"],"+":["output.","For","example,","let\u2019s"]},"consider","the","capsule","that","detected","the",{"-":["rectangle","I'll"],"+":["rectangle.","I\u2019ll"]},"call","it","the",{"-":["rectangle","capsule","let's"],"+":["rectangle-capsule.","Let\u2019s"]},"suppose","that","there","are",{"-":[],"+":["just"]},"two","capsules","in","the","next",{"-":["layer"],"+":["layer,"]},"the",{"-":["house","capsule"],"+":["house-capsule"]},"and","the",{"-":["boat","capsule","since"],"+":["boat-capsule.","Since"]},"the",{"-":["rectangle","capsule"],"+":["rectangle-capsule"]},"detected","a","rectangle","rotated","by",{"-":["16","degrees"],"+":["16\u00b0,"]},"it","predicts","that","the",{"-":["house","capsule"],"+":["house-capsule"]},"will","detect","a","house","rotated","by",{"-":["16","degrees"],"+":["16\u00b0,"]},"that","makes",{"-":["sense"],"+":["sense,"]},"and","the",{"-":["boat","capsule"],"+":["boat-capsule"]},"will","detect","a","boat","rotated","by",{"-":["16","degrees"],"+":["16\u00b0"]},"as",{"-":["well","that's"],"+":["well.","That\u2019s"]},"what","would","be","consistent","with","the","orientation","of","the",{"-":["rectangle","so"],"+":["rectangle.","So,"]},"to","make","this",{"-":["prediction"],"+":["prediction,"]},"what","the",{"-":["rectangle","capsule"],"+":["rectangle-capsule"]},"does","is",{"-":[],"+":["it"]},"simply","computes","the","dot","product","of","a","transformation","matrix",{"-":["W","IJ"],"+":["W_i,j"]},"with","its","own","activation","vector",{"-":["at","U","at","UI","during","training"],"+":["u_i.","During","training,"]},"the","network","will","gradually","learn","a","transformation","matrix","for","each","pair","of","capsules","in","the","first","and","second",{"-":["layer","in"],"+":["layer.","In"]},"other",{"-":["words"],"+":["words,"]},"it","will","learn","all","the",{"-":["part","hole","relationships"],"+":["part-whole","relationships,"]},"for","example","the","angle","between","the","wall","and","the","roof","of","a",{"-":["house"],"+":["house,"]},"and","so",{"-":["on","now","let's"],"+":["on.","Now","let\u2019s"]},"see","what","the",{"-":["triangle","capsule","predicts","right","this","time","it's"],"+":["triangle-capsule","predicts.","This","time,","it\u2019s"]},"a","bit","more",{"-":["interesting"],"+":["interesting:"]},"given","the","rotation","angle","of","the",{"-":["triangle"],"+":["triangle,"]},"it","predicts","that","the",{"-":["house","capsule"],"+":["house-capsule"]},"will","detect","an","upside-down",{"-":["house"],"+":["house,"]},"and",{"-":[],"+":["that"]},"the",{"-":["boat","capsule"],"+":["boat-capsule"]},"will","detect",{"-":["about"],"+":["a","boat"]},"rotated","by",{"-":["16","degrees","these"],"+":["16\u00b0.","These"]},"are","the","positions","that","would","be","consistent","with","the",{"-":["you","know"],"+":[]},"rotation","angle","of","the",{"-":["triangle","now","we've","got"],"+":["triangle.","Now","we","have"]},"a","bunch","of","predicted",{"-":["outputs"],"+":["outputs,"]},"what","do","we","do","with",{"-":["them","well","as"],"+":["them?","As"]},"you","can",{"-":["see"],"+":["see,"]},"the",{"-":["rectangle","capsule"],"+":["rectangle-capsule"]},"and","the",{"-":["triangle","capsules"],"+":["triangle-capsule"]},"strongly","agree","on","what","the",{"-":["boat","capsule"],"+":["boat-capsule"]},"will",{"-":["output","in"],"+":["output.","In"]},"other",{"-":["words"],"+":["words,"]},"they","agree","that","a","boat","positioned","in","this","way","would","explain","their","own","positions","and",{"-":["rotations","and"],"+":["rotations.","And"]},"they","totally","disagree","on","what","the",{"-":["house","capsule"],"+":["house-capsule"]},"will",{"-":["output","therefore"],"+":["output.","Therefore,"]},"it","makes","sense","to","assume","that","the","rectangle","and","triangle","are","part","of","a",{"-":["boat"],"+":["boat,"]},"not","a",{"-":["house","now"],"+":["house.","Now"]},"that","we","know","that","the","rectangle","and","triangle","are","part","of",{"-":["the","boat"],"+":["a","boat,"]},"the","outputs","of","the","rectangle","capsule","and","the","triangle","capsule","really","concern","only","the","boat",{"-":["capsule","this","there's"],"+":["capsule,","there\u2019s"]},"no","need","to","send","these","outputs","to","any","other",{"-":["capsule"],"+":["capsule,"]},"this","would","just","add",{"-":["noise","they"],"+":["noise.","They"]},"should","be","sent","only","to","the","boat",{"-":["capsule","this"],"+":["capsule.","This"]},"is","called","routing","by",{"-":["agreements","there"],"+":["agreement.","There"]},"are","several",{"-":["benefits","first"],"+":["benefits:","first,"]},"since","capsule","outputs","are","only","routed","to","the","appropriate","capsule","in","the","next",{"-":["layer"],"+":["layer,"]},"these","capsules","will","get","a","cleaner","input","signal",{"-":[],"+":["and","will"]},"more",{"-":["accurate"],"+":[]},"accurately","determine","the","pose","of","the",{"-":["object","second"],"+":["object.","Second,"]},"by","looking","at","the","paths","of","the",{"-":["activations"],"+":["activations,"]},"you","can","easily","navigate","the","hierarchy","of",{"-":["parts"],"+":["parts,"]},"and","know","exactly","which","part","belongs","to","which",{"-":["objects","like"],"+":["object","(like,"]},"the","rectangle",{"-":["biller"],"+":[]},"belongs","to","the",{"-":["boats"],"+":["boat,"]},"or","the","triangle","belongs","to","the",{"-":["boat"],"+":["boat,"]},"and","so",{"-":["on","lastly"],"+":["on).","Lastly,"]},"routing",{"-":["this"],"+":[]},"by","agreement","helps","parse","crowded","scenes","with","overlapping","objects",{"-":["we"],"+":["(we"]},"will","see","this","in","a","few",{"-":["slides","but","first","let's"],"+":["slides).","But","first,","let\u2019s"]},"look","at","how",{"-":["running"],"+":["routing"]},"by",{"-":["agreements"],"+":["agreement"]},"is","implemented","in",{"-":["capsule","networks","here"],"+":["Capsule","Networks.","Here,"]},"I","have","represented","the","various","poses","of","the",{"-":["boat"],"+":["boat,"]},"as","predicted","by","the",{"-":["lower","level","capsules","for","example"],"+":["lower-level","capsules.","For","example,"]},"one","of","these","circles","may","represent","what","the",{"-":["rectangle","capsule"],"+":["rectangle-capsule"]},"thinks","about","the","most","likely","pose","of","the",{"-":["boat"],"+":["boat,"]},"and","another","circle","may","represent","what","the",{"-":["triangle","capsule","thinks"],"+":["triangle-capsule","thinks,"]},"and","if","we","suppose","that","there","are","many","other",{"-":["low","level","capsules"],"+":["low-level","capsules,"]},"then","we","might","get","a","cloud","of","prediction",{"-":["vectors"],"+":["vectors,"]},"for","the","boat",{"-":["capsule"],"+":["capsule,"]},"like",{"-":["this","in"],"+":["this.","In"]},"this",{"-":["example"],"+":["example,"]},"there","are","two",{"-":["post","parameters"],"+":["pose","parameters:"]},"one","represents","the","rotation",{"-":["angle"],"+":["angle,"]},"and","the","other","represents","the","size","of","the",{"-":["boat","as"],"+":["boat.","As"]},"I","mentioned",{"-":["earlier","post"],"+":["earlier,","pose"]},"parameters","may","capture","many","different","kinds","of","visual",{"-":["features"],"+":["features,"]},"like",{"-":["skew","thickness"],"+":["skew,","thickness,"]},"and","so",{"-":["on","or","location","a"],"+":["on.","Or"]},"precise",{"-":["location","so"],"+":["location.","So"]},"the","first","thing","we",{"-":["do"],"+":["do,"]},"is","we","compute","the","mean","of",{"-":[],"+":["all"]},"these",{"-":["predictions","this"],"+":["predictions.","This"]},"gives","us","this",{"-":["vector","the"],"+":["vector.","The"]},"next","step","is","to","measure","the","distance","between","each","predicted","vector","and","the",{"-":["meet","vector","so"],"+":["mean","vector."]},"I","will","use","here","the",{"-":["Euclidean"],"+":["euclidian"]},"distance",{"-":[],"+":["here,"]},"but","capsule","networks","actually","use","the","scalar",{"-":["product","basically"],"+":["product.","Basically,"]},"we","want","to","measure","how","much","each","predicted","vector","agrees","with","the","mean","predicted",{"-":["vector","use","using"],"+":["vector.","Using"]},"this","agreement",{"-":["measure"],"+":["measure,"]},"we","can","update","the",{"-":["weights"],"+":["weight"]},"of","every","predicted","vector",{"-":["accordingly","note"],"+":["accordingly.","Note"]},"that","the","predicted","vectors","that","are","far","from","the","mean","now","have","a","very","small",{"-":["weight"],"+":["weight,"]},"and","the","ones","closest","to","the","mean","have","a","much","stronger",{"-":["weight","I've","represent","them"],"+":["weight.","I\u2019ve"]},"represented","them","in",{"-":["black","now"],"+":["black.","Now"]},"we","can",{"-":[],"+":["just"]},"compute","the","mean","once","again",{"-":["or"],"+":["(or"]},"I","should",{"-":["say"],"+":["say,"]},"the","weighted",{"-":["mean"],"+":["mean),"]},"and",{"-":[],"+":["you\u2019ll"]},"notice","that","it","moves","slightly","towards","the",{"-":["cluster"],"+":["cluster,"]},"towards","the","center","of","the",{"-":["cluster","so","next"],"+":["cluster.","So","next,"]},"we","can","once","again","update","the",{"-":["weights","and"],"+":["weights.","And"]},"now","most","of","the","vectors","within","the","cluster","have","turned",{"-":["black","and","again"],"+":["black.","And","again,"]},"we","can","update","the",{"-":["mean","and"],"+":["mean.","And"]},"we","can","repeat","this","process","a","few",{"-":["times","in"],"+":["times.","In"]},"practice",{"-":["you","know","three"],"+":["3"]},"to",{"-":["five"],"+":["5"]},"iterations","are","generally",{"-":["sufficient","this"],"+":["sufficient.","This"]},"might","remind",{"-":["you"],"+":["you,"]},"I",{"-":["suppose"],"+":["suppose,"]},"of","the","k-means","clustering","algorithm","if","you","know",{"-":["it","okay"],"+":["it.","Okay,"]},"so","this","is","how","we","find","clusters","of",{"-":["agreement","now","let's"],"+":["agreement.","Now","let\u2019s"]},"see","how","the","whole","algorithm","works","in","a","bit","more",{"-":["details","first"],"+":["details.","First,"]},"for","every","predicted",{"-":["output"],"+":["output,"]},"we","start","by","setting","a","raw","routing","weight",{"-":["bij"],"+":["b_i,j"]},"equal","to",{"-":["zero","next"],"+":["0.","Next,"]},"we","apply","the","softmax","function",{"-":["softmax","function"],"+":[]},"to","these","raw",{"-":["weights"],"+":["weights,"]},"for","each","primary",{"-":["capsule","this"],"+":["capsule.","This"]},"gives","the","actual","routing","weights","for","each","predicted",{"-":["output"],"+":["output,"]},"in","this","example","0.5",{"-":["each","equal","weights","next"],"+":["each.","Next"]},"we","compute","a","weighted","sum","of","the",{"-":["predictions"],"+":["predictions,"]},"for","each","capsule","in","the","next",{"-":["layer","this"],"+":["layer.","This"]},"might","give","vectors","longer","than",{"-":["one"],"+":["1,"]},"so","as","usual","we","apply","the","squash",{"-":["function","and","voila","we"],"+":["function.","And","voil\u00e0!","We"]},"now","have","the","actual","outputs","of","the",{"-":["house","capsule"],"+":["house-capsule"]},"and",{"-":["boat","capsule","but"],"+":["boat-capsule.","But"]},"this","is","not","the","final",{"-":["output","is"],"+":["output,","it\u2019s"]},"just","the","end","of","the","first",{"-":["round"],"+":["round,"]},"the","first",{"-":["iteration","now"],"+":["iteration.","Now"]},"we","can","see","which","predictions","were","most",{"-":["accurate","for","example"],"+":["accurate.","For","example,"]},"the",{"-":["rectangle","capsule"],"+":["rectangle-capsule"]},"made","a","great","prediction","for","the",{"-":["boat","capsules","output","it"],"+":["boat-capsule\u2019s","output.","It"]},"really","matches","it","pretty",{"-":["closely","this"],"+":["closely.","This"]},"is","estimated","by","computing","the","scalar","product","of","the","predicted","output","vector",{"-":["U","hat","j-jake","I"],"+":["\u00fb_j|i"]},"and","the","actual","product","vector",{"-":["V","J","this"],"+":["v_j.","This"]},"scalar","product","is","simply","added","to","the","predicted",{"-":["outputs"],"+":["output\u2019s"]},"raw","routing",{"-":["weight","bij","so"],"+":["weight,","b_i,j.","So"]},"the","weight","of","this","particular","predicted","output","is",{"-":["increased","when"],"+":["increased.","When"]},"there","is","a","strong",{"-":["agreement","the"],"+":["agreement,","this"]},"scalar","product","is",{"-":["going","to","be","large"],"+":["large,"]},"so","good","predictions","will","have","a","higher",{"-":["weight","on"],"+":["weight.","On"]},"the","other",{"-":["hand"],"+":["hand,"]},"the",{"-":["rectangle","capsule"],"+":["rectangle-capsule"]},"made","a","pretty","bad","prediction","for","the",{"-":["house","capsules","output"],"+":["house-capsule\u2019s","output,"]},"so","the","scalar","product","in","this","case","will","be","quite",{"-":["small"],"+":["small,"]},"and","the","raw","routing",{"-":["weights"],"+":["weight"]},"of",{"-":["these"],"+":["this"]},"predicted","vector","will","not","grow",{"-":["much","next"],"+":["much.","Next,"]},"we","update","the","routing","weights","by","computing","the","softmax","of","the","raw",{"-":["weights"],"+":["weights,"]},"once",{"-":["again","and"],"+":["again.","And"]},"as","you","can",{"-":["see"],"+":["see,"]},"the",{"-":["rectangle","capsules"],"+":["rectangle-capsule\u2019s"]},"predicted","vector","for","the",{"-":["boat","capsule"],"+":["boat-capsule"]},"now","has","a","weight","of",{"-":["0.8"],"+":["0.8,"]},"while",{"-":["it's"],"+":["it\u2019s"]},"predicted","vector","for","the",{"-":["house","capsule"],"+":["house-capsule"]},"dropped","down","to",{"-":["0.2","so"],"+":["0.2.","So"]},"most","of","its","output","is",{"-":[],"+":["now","going","to","go","to","the","boat","capsule,"]},"not",{"-":["going","to","go","to","the","boat","capsule","not"],"+":[]},"the","house",{"-":["capsule","once"],"+":["capsule.","Once"]},"again","we","compute","the","weighted","sum","of","all","the","predicted",{"-":["outputs"],"+":["output"]},"vectors","for","each","capsule","in","the","next",{"-":["layer"],"+":["layer,"]},"that","is","the",{"-":["house","capsule","in","the","boat","capsule"],"+":["house-capsule"]},"and",{"-":[],"+":["the","boat-capsule.","And"]},"this",{"-":["time"],"+":["time,"]},"the",{"-":["house","capsule"],"+":["house-capsule"]},"gets","so","little","input","that","its","output","is","a","tiny",{"-":["vector","on"],"+":["vector.","On"]},"the","other","hand","the",{"-":["boat","capsule"],"+":["boat-capsule"]},"gets","so","much","input","that",{"-":["its","output","that"],"+":[]},"it","outputs","a","vector","much","longer","than",{"-":["1","so"],"+":["1.","So"]},"again","we","squash",{"-":["it","and","that's"],"+":["it.","And","that\u2019s"]},"the","end","of","round",{"-":["2","and"],"+":["#2.","And"]},"as","you","can",{"-":["see"],"+":["see,"]},"in","just","a","couple",{"-":["iterations"],"+":["iterations,"]},"we","have","already","ruled","out","the","house","and","clearly","chosen","the",{"-":["boat","after"],"+":["boat.","After"]},"perhaps","one","or","two","more",{"-":["rounds"],"+":["rounds,"]},"we","can","stop","and","proceed","to","the","next","capsule","layer","in","exactly","the","same",{"-":["way","so"],"+":["way.","So"]},"as","I","mentioned",{"-":["earlier"],"+":["earlier,"]},"routing","by","agreement","is","really","great","to","handle","crowded",{"-":["scenes"],"+":["scenes,"]},"such","as","the","one","represented","in","this",{"-":[],"+":["image.","One","way","to","interpret","this"]},"image",{"-":["one","way","to","interpret","this","image","as"],"+":["(as"]},"you","can","see",{"-":["there's"],"+":["there","is"]},"a",{"-":["little"],"+":[]},"bit","of",{"-":["ambiguity"],"+":["ambiguity),"]},"you","can","see","a","house","upside","down","in","the",{"-":["middle","however"],"+":["middle.","However,"]},"if","this","was","the",{"-":["case"],"+":["case,"]},"then","there","would","be","no","explanation","for","the","bottom","rectangle","or","the","top",{"-":["triangle"],"+":["triangle,"]},"no","reason","for","them","to","be","where","they",{"-":["are","the"],"+":["are.","The"]},"best","way","to","interpret","the","image","is","that","there","is","a","house","at","the","top","and","a","boat","at","the",{"-":["bottom","and"],"+":["bottom.","And"]},"routing","by","agreement","will","tend","to","choose","this",{"-":["solution"],"+":["solution,"]},"since","it","makes","all","the","capsules","perfectly",{"-":["happy"],"+":["happy,"]},"each","of","them","making","perfect","predictions","for","the","capsules","in","the","next",{"-":["layer","the"],"+":["layer.","The"]},"ambiguity","is","explained",{"-":["away","ok"],"+":["away.","Okay,"]},"so","what","can","you","do","with","a","capsule","network","now","that","you","know","how","it",{"-":["works","well"],"+":["works.","Well"]},"for",{"-":["one"],"+":["one,"]},"you","can","create","a","nice","image","classifier","of",{"-":["course","just"],"+":["course.","Just"]},"have","one","capsule","per","class","in","the","top",{"-":["capsule"],"+":[]},"layer","and",{"-":["that's"],"+":["that\u2019s"]},"almost","all","there","is","to",{"-":["it","all"],"+":["it.","All"]},"you","need","to","add","is","a","layer","that","computes","the","length","of","the",{"-":["top","layer"],"+":["top-layer"]},"activation",{"-":["vectors"],"+":["vectors,"]},"and","this","gives","you","the","estimated","class",{"-":["probabilities","you"],"+":["probabilities.","You"]},"could","then","just","train","the","network","by","minimizing","the",{"-":["cross","entropy","loss","has","an","irregular"],"+":["cross-entropy","loss,","as","in","a","regular"]},"classification","neural",{"-":["network"],"+":["network,"]},"and","you",{"-":["know","you'd"],"+":["would"]},"be",{"-":["done","however"],"+":["done.","However,"]},"in","the","paper","they","use","a","margin","loss","that","makes","it","possible","to","detect","multiple","classes","in","the",{"-":["image","so"],"+":["image.","So"]},"without","going","into","too","much",{"-":["detail"],"+":["details,"]},"this","margin","loss","is","such","that","if","an","object","of","class",{"-":["K"],"+":["k"]},"is","present","in","the",{"-":["image"],"+":["image,"]},"then","the","corresponding","top-level","capsule","should",{"-":["help","put"],"+":["output"]},"a","vector","whose",{"-":["squared"],"+":[]},"length","is","at","least",{"-":["0.9","it"],"+":["0.9.","It"]},"should","be",{"-":["long","conversely"],"+":["long.","Conversely,"]},"if","an","object","of","class",{"-":["K"],"+":["k"]},"is","not","present","in","the",{"-":["image"],"+":["image,"]},"then","the","capsule","should","output","a","short",{"-":["vector"],"+":["vector,"]},"one","whose",{"-":["squared"],"+":[]},"length","is","shorter","than",{"-":["0.1","so"],"+":["0.1.","So"]},"the","total","loss","is","the","sum","of","losses","for","all",{"-":["classes","in"],"+":["classes.","In"]},"the",{"-":["paper"],"+":["paper,"]},"they","also","add","a","decoder","network","on","top","of","the","capsule",{"-":["network","it's"],"+":["network.","It\u2019s"]},"just",{"-":["a","three","you","know","three"],"+":["3","fully"]},"connected","layers",{"-":["fully","connected"],"+":[]},"with","a","sigmoid","activation","function","in","the","output",{"-":["layer","and","it"],"+":["layer.","It"]},"learns","to","reconstruct","the","input","image","by","minimizing","the","squared","difference","between","the","reconstructed","image","and","the","input",{"-":["image","the"],"+":["image.","The"]},"full","loss","is",{"-":[],"+":["the"]},"margin","loss","we","discussed",{"-":["earlier"],"+":["earlier,"]},"plus","the","reconstruction","loss",{"-":["scaled"],"+":["(scaled"]},"down","considerably","so","as","to","ensure","that","the","margin","loss","dominates",{"-":["training","the"],"+":["training).","The"]},"benefit","of","applying","this","reconstruction","loss","is","that","it","forces","the","network","to","preserve","all","the","information","required","to",{"-":["construct","to"],"+":[]},"reconstruct","the",{"-":["image"],"+":["image,"]},"up","to","the","top","layer","of","the","capsule",{"-":["network"],"+":["network,"]},"its","output",{"-":["layer","this"],"+":["layer.","This"]},"constraint","acts","a","bit","like","a",{"-":["regularizer"],"+":["regularizer:"]},"it","reduces","the","risk","of","overfitting","and","helps","generalize","to","new",{"-":["examples","and","that's","it"],"+":["examples.","And","that\u2019s","it,"]},"you","know","how","a","capsule",{"-":["Network","works"],"+":["network","works,"]},"and","how","to","train",{"-":["it","let's"],"+":["it.","Let\u2019s"]},"look","a","little","bit","at","some","of","the","figures","in","the",{"-":["paper"],"+":["paper,"]},"which","I","find",{"-":["interesting","so","this"],"+":["interesting.","This"]},"is","figure",{"-":["one"],"+":["1","from","the","paper,"]},"showing","a","full","capsule","network","for",{"-":["mmm","mist","you"],"+":["MNIST.","You"]},"can","see",{"-":["the"],"+":[]},"the","first","two","regular","convolutional",{"-":["layers"],"+":["layers,"]},"whose","output","is","reshaped","and","squashed","to","get","the","activation","vectors","of","the","primary",{"-":["capsules","and"],"+":["capsules.","And"]},"these","primary","capsules","are","organized","in","a",{"-":["six"],"+":["6"]},"by",{"-":["six","grid"],"+":["6","grid,"]},"with","32","primary","capsules","in","each","cell","of","this",{"-":["grid"],"+":["grid,"]},"and","each","primary","capsule","outputs","an",{"-":["eight","dimensional","vector","so"],"+":["8-dimensional","vector.","So"]},"this","first","layer","of","capsules","is","fully","connected","to","the",{"-":["ten"],"+":["10"]},"output",{"-":["capsules"],"+":["capsules,"]},"which","output",{"-":["sixteen"],"+":["16"]},"dimensional",{"-":["lectures","the"],"+":["vectors.","The"]},"length","of","these","vectors","is",{"-":["yoots","is"],"+":[]},"used","to","compute","the","margin",{"-":["loss"],"+":["loss,"]},"as","explained",{"-":["earlier","now"],"+":["earlier.","Now"]},"this","is","figure",{"-":["two"],"+":["2"]},"from","the",{"-":["paper","it"],"+":["paper.","It"]},"shows","the","decoder","sitting","on","top","of","the",{"-":["caps","net","it"],"+":["capsnet.","It"]},"is","composed","of",{"-":["two"],"+":["2"]},"fully","connected",{"-":["relu"],"+":["ReLU"]},"layers","plus","a","fully","connected","sigmoid","layer","which","outputs","784","numbers","that","correspond","to","the","pixel","intensities","of","the",{"-":["richens"],"+":[]},"reconstructed","image",{"-":["which"],"+":["(which"]},"is",{"-":[],"+":["a"]},"28","by","28","pixel",{"-":["the"],"+":["image).","The"]},"squared","difference","between","this","reconstructed","image","and","the","input","image","gives","the","reconstruction",{"-":["loss","right"],"+":["loss.","Right,"]},"and","this","is","figure",{"-":["four"],"+":["4"]},"from","the",{"-":["paper","also","interesting","one"],"+":["paper.","One"]},"nice","thing","about","capsule","networks","is","that","the","activation","vectors","are","often",{"-":["quite","interpretable","for","example"],"+":["interpretable.","For","example,"]},"this","image","shows","the","reconstructions","that","you","get","when","you","gradually","modify","one","of","the","16","dimensions","of","the","top","layer",{"-":["capsules","output","you"],"+":["capsules\u2019","output.","You"]},"can","see","that","the","first","dimension","seems","to","represent",{"-":["you","know"],"+":[]},"scale","and",{"-":["thickness","the"],"+":["thickness.","The"]},"fourth","dimension","represents","a","localized",{"-":["skew","if","you","look","at","how","it","that","number","4","is","modified","from","the","left","to","the","right","the"],"+":["skew.","The"]},"fifth","represents","the","width","of","the","digit","plus","a","slight","translation","to","get","the","exact",{"-":["position","so"],"+":["position.","So"]},"as","you","can",{"-":["see","it's"],"+":["see,","it\u2019s"]},"rather","clear","what","most","of","these","parameters",{"-":["do","ok"],"+":["do.","Okay,"]},"to",{"-":["conclude","let's"],"+":["conclude,","let\u2019s"]},"summarize","the","pros","and",{"-":["cons","capsule"],"+":["cons.","Capsule"]},"networks","have","reached",{"-":["state-of-the-art"],"+":["state","of","the","art"]},"accuracy","on",{"-":["a","missed","on","so","far","10","they"],"+":["MNIST.","On","CIFAR10,"]},"they","got","a","bit","over",{"-":["10","percent","error"],"+":["10%","error,"]},"which","is","far","from",{"-":["a","state-of-the-art"],"+":["state","of","the","art,"]},"but",{"-":["it's","-"],"+":["it\u2019s","similar","to"]},"what","was","first","obtained","with","other","techniques","before",{"-":["you","know"],"+":[]},"years","of","efforts","were","put","into",{"-":["them"],"+":["them,"]},"so",{"-":["it's"],"+":["it\u2019s"]},"still","a","good",{"-":["start","capsule"],"+":["start.","Capsule"]},"networks","require","less","training",{"-":["data","they"],"+":["data.","They"]},"offer",{"-":["equivariance"],"+":["equivariance,"]},"which","means","that","position","and","pose","information","are",{"-":["preserved","and"],"+":["preserved.","And"]},"this","is","very","promising","for","image","segmentation","and","object",{"-":["detection","the"],"+":["detection.","The"]},"routing","by","agreement","algorithm","is","great","for","crowded",{"-":["scenes","the"],"+":["scenes.","The"]},"routing","tree","also","maps","the","hierarchy","of","objects",{"-":["parts"],"+":["parts,"]},"so","every","part","is",{"-":["associated"],"+":["assigned"]},"to","a",{"-":["whole","and","it's"],"+":["whole.","And","it\u2019s"]},"rather","robust","to",{"-":["rotations"],"+":["rotations,"]},"translations","and","other",{"-":["FIM","transformations","the"],"+":["affine","transformations.","The"]},"activation","vectors",{"-":[],"+":["somewhat"]},"are",{"-":["somewhat","interpretable","and","finally","obviously","its","sentence","idea"],"+":["interpretable.","And","finally,","obviously,","it\u2019s","Hinton\u2019s","idea,"]},"so",{"-":["don't"],"+":["don\u2019t"]},"bet","against",{"-":["it","however"],"+":["it.","However,"]},"there","are","a","few",{"-":["cons","first"],"+":["cons:","first,"]},"as","I","mentioned","the","results","are","not","yet",{"-":["state-of-the-art"],"+":["state","of","the","art"]},"on",{"-":["cipher","10"],"+":["CIFAR10,"]},"even","though",{"-":["it's"],"+":["it\u2019s"]},"a","good",{"-":["start","plus","it's"],"+":["start.","Plus,","it\u2019s"]},"still","unclear","whether","capsule","networks","can","scale","to","larger",{"-":["images"],"+":["images,"]},"such","as","the",{"-":["image","net","data","set","you","know","what"],"+":["ImageNet","dataset.","What"]},"will","the","accuracy",{"-":["be","capsule"],"+":["be?","Capsule"]},"networks","are","also","quite","slow","to",{"-":["Train"],"+":["train,"]},"in","large","part","because","of","the","routing","by","agreement","algorithm","which","has","an","inner",{"-":["loop"],"+":["loop,"]},"as","you","saw",{"-":["earlier","finally"],"+":["earlier.","Finally,"]},"there","is","only","one","capsule","of","any",{"-":[],"+":["given"]},"type","in","a","given",{"-":["location"],"+":["location,"]},"so",{"-":["it's"],"+":["it\u2019s"]},"impossible","for","a","capsule","network","to","detect","two","objects","of","the","same","type","if","they","are","too","close","to","one",{"-":["another","this"],"+":["another.","This"]},"is","called",{"-":["crowding"],"+":["crowding,"]},"and",{"-":["it's"],"+":["it","has"]},"been","observed","in","human","vision","as",{"-":["well"],"+":["well,"]},"so",{"-":["it's"],"+":["it\u2019s"]},"probably","not","a",{"-":["showstopper","alright"],"+":["show-stopper.","All","right!"]},"I","highly","recommend","you","take","a","look","at","the","code","of","a",{"-":["caps","net","implementation"],"+":["CapsNet","implementation,"]},"such","as","the","ones","listed","here",{"-":["I'll"],"+":["(I\u2019ll"]},"leave","the","links","in","the","video","description",{"-":["below","if"],"+":["below).","If"]},"you","take","your",{"-":["time"],"+":["time,"]},"you","should","have","no","problem","understanding","everything","the","code","is",{"-":["doing","the"],"+":["doing.","The"]},"main","difficulty","in","implementing",{"-":["caps","nets"],"+":["CapsNets"]},"is","that","it","contains","an","inner","loop","for","the","routing","by","agreement",{"-":["algorithm","implementing"],"+":["algorithm.","Implementing"]},"loops","in",{"-":["Kerris"],"+":["Keras"]},"and",{"-":["tensorflow"],"+":["TensorFlow"]},"can","be","a","little","bit","trickier","than","in",{"-":["pi","torch"],"+":["PyTorch,"]},"but","it","can","be",{"-":["done","so","if"],"+":["done.","If"]},"you",{"-":["don't"],"+":["don\u2019t"]},"have","a","particular",{"-":["preference"],"+":["preference,"]},"then","I","would","say","that","the",{"-":["PI","torch"],"+":["PyTorch"]},"code","is",{"-":["probably"],"+":[]},"the","easiest","to",{"-":["understand","and","that's"],"+":["understand.","And","that\u2019s"]},"all","I",{"-":["had"],"+":["had,"]},"I","hope","you","enjoyed","this",{"-":["video","if"],"+":["video.","If"]},"you",{"-":["did"],"+":["did,"]},"please","thumbs",{"-":["up","share","comment","subscribe","blah","blah","blah","it's"],"+":["up,","share,","comment,","subscribe,","blablabla.","It\u2019s"]},"my","first","real","YouTube",{"-":["video"],"+":["video,"]},"and","if","people","find","it",{"-":["useful","I'm"],"+":["useful,","I","might"]},"make","some",{"-":["more","if"],"+":["more.","If"]},"you","want","to","learn","more","about",{"-":["machine","learning","deep","learning","and","deep","reinforcement","learning"],"+":["Machine","Learning,","Deep","Learning","and","Deep","Reinforcement","Learning,"]},"you","may","want","to","read","my",{"-":["Holly"],"+":["O\u2019Reilly"]},"book",{"-":["hands","on","machine","learning"],"+":["Hands-on","Machine","Learning"]},"with",{"-":["scikit-learn"],"+":["Scikit-Learn"]},"and",{"-":["tensorflow","it"],"+":["TensorFlow.","It"]},"covers","a","ton","of",{"-":["topics"],"+":["topics,"]},"with","many","code","examples","that","you","will","find","on","my","github",{"-":["account"],"+":["account,"]},"so",{"-":["I'll"],"+":["I\u2019ll"]},"leave","the","links","in","the","video",{"-":["description","that's"],"+":["description.","That\u2019s"]},"all","for",{"-":["today"],"+":["today,"]},"have","fun","and","see","you","next",{"-":["time"],"+":["time!"]}],"2Kawrd5szHE":[{"-":["hi","I'm","over","LaVon"],"+":["Hi,","I\u2019m","Aur\u00e9lien","G\u00e9ron,"]},"and","today",{"-":["I'm"],"+":["I\u2019m"]},"going","to","show","you","how","to","implement","a","capsule","network","using",{"-":["tensor","flow","in"],"+":["TensorFlow.","In"]},"my","previous",{"-":["video"],"+":["video,"]},"I","presented","the","key","ideas","behind","capsule",{"-":["networks"],"+":["networks,"]},"a","recently","published","neural","net",{"-":["architecture","if"],"+":["architecture.","If"]},"you",{"-":["haven't"],"+":["haven\u2019t"]},"seen","this",{"-":["video"],"+":["video,"]},"I","encourage","you","to","do","so",{"-":["now"],"+":["now,"]},"today","I","will","focus","on","the",{"-":["tensor","flow","implementation"],"+":["TensorFlow","implementation."]},"I","wrote","a",{"-":["jupiter"],"+":["Jupyter"]},"notebook","containing","all","the","code","and","detailed",{"-":["explanations"],"+":["explanations,"]},"and",{"-":["i"],"+":["I"]},"published","it","on","my","github","account",{"-":["as"],"+":["(as"]},"always",{"-":["I'll"],"+":["I\u2019ll"]},"put","all","the","links","in","the","video","description",{"-":["below"],"+":["below),"]},"so","I","encourage","you","to","clone",{"-":["it"],"+":["it,"]},"and","play","with",{"-":["it","so"],"+":["it.","So,"]},"it","reaches","over",{"-":["ninety-nine","point","four"],"+":["99.4%"]},"accuracy","on","the","test",{"-":["set"],"+":["set,"]},"which","is","pretty",{"-":["good"],"+":["good,"]},"considering",{"-":["it's"],"+":["it\u2019s"]},"a","shallow","network","with","just","two","capsule","layers","and","a","total","of","about",{"-":["1200","capsules","there's"],"+":["1,200","capsules.","There\u2019s"]},"a","lot","of","code","in",{"-":["the","in"],"+":[]},"this",{"-":["notebook"],"+":["notebook,"]},"so","I",{"-":["won't"],"+":["won\u2019t"]},"go","through","every","single","line","in","this",{"-":["video"],"+":["video,"]},"but",{"-":["I'll"],"+":["I\u2019ll"]},"explain","the","main","difficulties","I","came",{"-":["across"],"+":["across,"]},"and","hopefully","this","will","be","useful","to","you","for","other",{"-":["tensor","flow","implementations"],"+":["TensorFlow","implementations,"]},"not","just",{"-":["caps","nets","okay","let's"],"+":["CapsNets.","Okay,","let\u2019s"]},"build",{"-":["a","network","first"],"+":["the","network.","First,"]},"we","need","to","feed","the","input","images","to","the",{"-":["network","and","that's"],"+":["network.","And","that\u2019s"]},"our","input",{"-":["layer","we"],"+":["layer.","We"]},"implement","it","using","a","simple",{"-":["tensor","flow","placeholder","the"],"+":["TensorFlow","placeholder.","The"]},"batch","size","is",{"-":["unspecified"],"+":["unspecified,"]},"so","that","we","can","pass","any","number","of","images","in","each",{"-":["batch"],"+":["batch,"]},"in","this",{"-":["example","5","note"],"+":["example,","5.","Note"]},"that","we","directly","send",{"-":["Tony","8","by","28"],"+":["28x28"]},"pixel",{"-":["images"],"+":["images,"]},"with","a","single",{"-":["channel"],"+":["channel,"]},"since","the","images","are",{"-":["grayscale","color"],"+":["greyscale.","Color"]},"images","would","typically","have","3",{"-":["channels"],"+":["channels,"]},"for",{"-":["red"],"+":["red,"]},"green","and",{"-":["blue","and","that's"],"+":["blue.","And","that\u2019s"]},"it","for","the","input",{"-":["layer","next","let's"],"+":["layer.","Next,","let\u2019s"]},"build","the","primary","capsule",{"-":["layer","for"],"+":["layer.","For"]},"each","digit","in","the","batch","it","will","output","32",{"-":["maps"],"+":["maps,"]},"each","containing","a",{"-":["6","by","6"],"+":["6x6"]},"grid","of","8","dimensional",{"-":["vectors","the"],"+":["vectors.","The"]},"capsules","in","this","particular","map","seem","to","detect","the","start","of","a","line",{"-":["segment","that","you"],"+":["segment.","You"]},"can","see","that","the","output","vectors","are","long","in","the","locations","where",{"-":["there's"],"+":["there\u2019s"]},"a","start","of","a",{"-":["line","and"],"+":["line.","And"]},"the","orientation","of","the",{"-":["ad"],"+":["8D"]},"vector","gives","the","pose",{"-":["parameters"],"+":["parameters,"]},"in","this",{"-":["case","I've"],"+":["case,","I\u2019ve"]},"represented","the","rotation",{"-":["angle"],"+":["angle,"]},"but","the",{"-":["vectors","a"],"+":["vector\u2019s","8"]},"dimensional","orientation","would","also","capture","things","like","the","thickness","of","the",{"-":["line"],"+":["line,"]},"the","precise","location","of","the","start","of","the","line","relative","to","the","cell","in","the",{"-":["6","by","6","grid"],"+":["6x6","grid,"]},"and","so",{"-":["the"],"+":["on.","The"]},"implementation","is","really",{"-":["straightforward","first"],"+":["straightforward.","First,"]},"we","define","two","regular","convolutional",{"-":["layers","the"],"+":["layers.","The"]},"input","of","the","first","layer","is",{"-":["X"],"+":["X,"]},"the","placeholder","that","will","contain","the","input","images","we","will","feed","at",{"-":["runtime","the"],"+":["runtime.","The"]},"second","layer","takes","the","output","of","the","first",{"-":["layer","of","course","and"],"+":["layer.","And"]},"we","use","the","parameters","specified","in","the",{"-":["paper","the"],"+":["paper.","The"]},"second","layer","is","configured","to","output","256","feature",{"-":["maps"],"+":["maps.","And"]},"each","feature","map","contains",{"-":[],"+":["a"]},"6x6","grid","of",{"-":["scalars","we"],"+":["scalars.","We"]},"want","a","6x6",{"-":["a"],"+":[]},"grid","of","vectors",{"-":["instead"],"+":["instead,"]},"so","we","use",{"-":["tensor","flows","reshape"],"+":["TensorFlow\u2019s","reshape()"]},"function","to","get","32",{"-":["Maps"],"+":["maps"]},"of",{"-":["eight"],"+":["8"]},"dimensional",{"-":["vectors"],"+":["vectors,"]},"instead","of","256","maps","of",{"-":["scalars","in","fact"],"+":["scalars.","In","fact,"]},"since","the","primary","capsules","will","be","fully","connected","to","the","digit",{"-":["capsules"],"+":["capsules,"]},"we","can","simply","reshape","to","one","long","list","of",{"-":["1152"],"+":["1,152"]},"output","vectors",{"-":["that's","a","thirty-two","times","six","times","six"],"+":["(that\u2019s","32*6*6),"]},"for","each","instance","in","the",{"-":["batch","and"],"+":["batch.","And"]},"the","last","step","is","to","squash","the","vectors","to","ensure","that","their","length","is","always","between",{"-":["zero"],"+":["0"]},"and",{"-":["one","for","this"],"+":["1.","For","this,"]},"we","use","a",{"-":["homemade"],"+":["home","made"]},"squash",{"-":["function","here"],"+":["function.","There"]},"it",{"-":["is","this"],"+":["is.","This"]},"function","implements","the","squash","equation","given","in","the",{"-":["paper","it"],"+":["paper.","It"]},"squashes","every","vector","in","an",{"-":["array"],"+":["array,"]},"along","the","specified",{"-":["dimension"],"+":["dimension,"]},"by","default","the","last",{"-":["one","so"],"+":["one.","So,"]},"as","you","can",{"-":["see"],"+":["see,"]},"it","involves","a","division","by","the","norm","of","the",{"-":["vector"],"+":["vector,"]},"so",{"-":["there's"],"+":["there\u2019s"]},"a","risk","of","a","division","by","zero","if","at","least","one","of","the","vectors","is","a","zero",{"-":["vector","so"],"+":["vector.","So"]},"you","could","just","add","a","tiny","epsilon","value","in","the",{"-":["denominator"],"+":["denominator,"]},"and","it","would","fix","the","division","by","zero",{"-":["problem","however"],"+":["problem.","However"]},"you","would","still","run","into","another",{"-":["issue","the"],"+":["issue.","The"]},"norm","of","a","vector","has","no","defined","gradients","when","the","vector","is",{"-":["zero","so"],"+":["zero.","So"]},"if","you","just","use",{"-":["tensor","flows"],"+":["tensorflow\u2019s","norm()","function","to","compute","the"]},"norm",{"-":["function","to","compute","the","norm"],"+":[]},"in","this",{"-":["equation"],"+":["equation,"]},"then","if","at","least","one","of","the","vectors","is",{"-":["zero"],"+":["zero,"]},"the","gradients","will","be","undefined",{"-":["it"],"+":["(it"]},"will","return",{"-":["n","a","n","and"],"+":["n-a-n,","nan,"]},"not",{"-":["so"],"+":["a","number).","So,"]},"as","a",{"-":["result"],"+":["result,"]},"when",{"-":["greed","in"],"+":["gradient"]},"descent","updates","the","weights","of","our",{"-":["model"],"+":["model,"]},"the","weights","will","end","up","being","undefined","as",{"-":["well","the"],"+":["well.","The"]},"model","would","effectively","be",{"-":["dead","then","you","don't"],"+":["dead.","You","don\u2019t"]},"want",{"-":["that","so"],"+":["that.","So"]},"the","trick","is","to","compute","a","safe","approximation","of","the",{"-":["norm"],"+":["norm,"]},"shown","in","the","equation","on","the",{"-":["right","and"],"+":["right.","And,"]},"that's","about",{"-":["it","that's"],"+":["it,","that\u2019s"]},"all","for","the","primary",{"-":["capsules","apart"],"+":["capsules.","Apart"]},"for","computing","the","norm",{"-":["safely"],"+":["safely,"]},"it","was","pretty",{"-":["straight","forward","on"],"+":["straightforward.","On"]},"to","the","next","layer","where","all","the","complexity",{"-":["is"],"+":["is:"]},"the","digit",{"-":["capsules","there"],"+":["capsules.","There"]},"are","just",{"-":["ten"],"+":["10"]},"of",{"-":["them"],"+":["them,"]},"one","for","each",{"-":["digits"],"+":["digit,"]},"0","to",{"-":["9"],"+":["9,"]},"and",{"-":["the"],"+":["they"]},"output","16","dimensional",{"-":["vectors","in"],"+":["vectors.","In"]},"this","particular",{"-":["example"],"+":["example,"]},"you","can","see","that","the","longest","output","vector","is","the","one","for","digit",{"-":["4","and"],"+":["4.","And"]},"again",{"-":["it's"],"+":["its"]},"orientation","in","the","16","dimensional","space","gives","information","about","the","pose","of","this",{"-":["digit"],"+":["digit,"]},"its",{"-":["rotation"],"+":["rotation,"]},"its",{"-":["thickness"],"+":["thickness,"]},"its",{"-":["Q"],"+":["skew,"]},"its",{"-":["position"],"+":["position,"]},"and","so",{"-":["on","by"],"+":["on.","By"]},"the",{"-":["way"],"+":["way,"]},"note","that","most","of","the","position","information","in","the","first","layer","was","encoded","in","the","location","of","the","active","capsules","in","the","6x6",{"-":["grid","so"],"+":["grid.","So,"]},"for",{"-":["example"],"+":["example,"]},"if","I","shift","the","digit","4","slightly","to","the","left","in","the","input","image",{"-":["they"],"+":[]},"then","different","capsules","in","the","first","layer","get",{"-":["activated","see","so"],"+":["activated.","See?","So,"]},"the","output","of","these","first","layer","capsules","only",{"-":["contains","a"],"+":["contain"]},"local","shift",{"-":["information"],"+":["information,"]},"relative","to","the","position","of","the","capsule","in","the","6x6",{"-":["grid","but"],"+":["grid.","But"]},"in","the","second","capsule",{"-":["layer"],"+":["layer,"]},"the","full","position","information","is","now","encoded","in","the","orientation","of","the","output","vector","in","16","dimensional",{"-":["space","ok"],"+":["space.","Okay,"]},"now",{"-":["let's"],"+":["let\u2019s"]},"see","how","to","implement","this",{"-":["layer","the"],"+":["layer.","The"]},"first","step","is","to","compute","the","predicted","output",{"-":["vectors","since"],"+":["vectors.","Since"]},"this","second","layer","is","fully","connected","to","the","first",{"-":["layer"],"+":["layer,"]},"we","will","compute","one","predicted","output","for","each","pair","of","first","and","second","layer",{"-":["capsules","for","example"],"+":["capsules.","For","example,"]},"using","the","output","of","the","first","primary",{"-":["capsule"],"+":["capsule,"]},"we","can","predict","the","output","vector","of","the","first","digit",{"-":["capsule","for","this"],"+":["capsule.","For","this,"]},"we","just","use","a","transformation","matrix",{"-":["w11"],"+":["W_1,1,"]},"which","will","gradually","be","learned","during",{"-":["training"],"+":["training,"]},"and","we","multiply","it","by","the","output","of","the","first","layer",{"-":["capsule","this"],"+":["capsule.","This"]},"gives","us",{"-":["u","hat","1","1"],"+":["\u00fb_1|1,"]},"which","is","the","predicted","output","of","the","first","digit",{"-":["capsule"],"+":["capsule,"]},"based","on","the","output","of","the","first","primary",{"-":["capsule","since"],"+":["capsule.","Since"]},"the","primary","capsules","output",{"-":["eight"],"+":["8"]},"dimensional",{"-":["vectors"],"+":["vectors,"]},"and","the","digit","capsules","output",{"-":["sixteen"],"+":["16"]},"dimensional",{"-":["vectors"],"+":["vectors,"]},"the","transformation","matrix",{"-":["w11"],"+":["W_1,1"]},"must","be","a",{"-":["sixteen","by","eight","matrix","next"],"+":["16x8","matrix.","Next,"]},"we","try","to","predict","the","output","of","the","second","digit",{"-":["capsule"],"+":["capsule,"]},"still","based","on","the","output","of","the","first","primary",{"-":["capsule","note"],"+":["capsule.","Note"]},"that","we","are","using","a","different","transformation",{"-":["matrix","w12","and"],"+":["matrix,","W_1,2.","And"]},"we","do","the","same","for","the","third",{"-":["capsule"],"+":["digit","capsule,"]},"using",{"-":["W","one","three","and"],"+":["W_1,3.","And"]},"so","on","for","all","the","digit",{"-":["capsules","then"],"+":["capsules.","Then"]},"we","move","on","to","the","second","primary",{"-":["capsule"],"+":["capsule,"]},"and","we","use","its","output","to","predict","the","output","of","the","first","digit",{"-":["capsule","and"],"+":["capsule.","And"]},"so","on","for","all","the","digit",{"-":["capsules","then"],"+":["capsules.","Then"]},"we","move","on","to","the","third","primary",{"-":["capsule"],"+":["capsule,"]},"we","make",{"-":["ten","predictions","and"],"+":["10","predictions.","And"]},"so",{"-":["on"],"+":["on,"]},"you","get","the",{"-":["picture","there"],"+":["picture.","There"]},"are",{"-":["1152"],"+":["1,152"]},"primary","capsules",{"-":["multiply","six","by","six","by","32"],"+":["(multiply","6","*","6","*","32),"]},"and",{"-":["ten"],"+":["10"]},"digit",{"-":["capsules"],"+":["capsules,"]},"so","we","end","up","with",{"-":["eleven","thousand","five","hundred","and","twenty"],"+":["11,520"]},"predicted","output",{"-":["vectors","now"],"+":["vectors.","Now"]},"we","could","just","compute","them","one","by",{"-":["one"],"+":["one,"]},"but","it","would","be","terribly",{"-":["inefficient","so","let's"],"+":["inefficient.","Let\u2019s"]},"see","how","we","can","get","all","the","predicted","output","vectors","in","just","one",{"-":["matte","small","operation","now"],"+":["matmul()","operation.","Now"]},"you","know","that",{"-":["tensorflow","mat","small"],"+":["TensorFlow\u2019s","matmul()"]},"function","lets","you","multiply","two",{"-":["matrices"],"+":["matrices,"]},"but","you","may","not","know","that","you","can","also","use","it","to","multiply","many","matrices","in","one",{"-":["shot","this"],"+":["shot.","This"]},"will","be","incredibly",{"-":["efficient"],"+":["efficient,"]},"especially","if","you","are","using","a","GPU",{"-":["card"],"+":["card,"]},"because","it","will","perform","all","the","matrix","multiplications","in","parallel","in","many","different","GPU",{"-":["threads","so","here's"],"+":["threads.","So","here\u2019s"]},"how","it",{"-":["works","suppose","a","b","c","d","e","f"],"+":["works.","Suppose","A,","B,","C,","D,","E,","F"]},"and",{"-":["g","h","i","j","k","l"],"+":["G,","H,","I,","J,","K,","L,"]},"are","all",{"-":["matrices","you"],"+":["matrices.","You"]},"can","put","these","matrices","in","two",{"-":["arrays"],"+":["arrays,"]},"each","with",{"-":["two"],"+":["2"]},"rows","and",{"-":["three","columns"],"+":["3","columns,"]},"for",{"-":["example","so"],"+":["example.","So"]},"we","have",{"-":["two"],"+":["2"]},"dimensions","for","this","2x3","grid","of",{"-":["matrices"],"+":["matrices,"]},"and","each","matrix","is",{"-":["two-dimensional"],"+":["2","dimensional,"]},"so","these","arrays","are",{"-":["two","plus","two","equals","four"],"+":["2+2=4"]},"dimensional",{"-":["arrays","if"],"+":["arrays.","If"]},"you","pass","these",{"-":["arrays"],"+":["arrays,"]},"these",{"-":["four","d","arrays","two","matmo"],"+":["4D","arrays,","to","matmul(),"]},"it","will","perform","an",{"-":["element-wise"],"+":["elementwise"]},"matrix",{"-":["application"],"+":["multiplication,"]},"so","the","result","will","be","this",{"-":["four"],"+":["4"]},"dimensional","array","containing",{"-":["a","x","G","here","and"],"+":["A","multiplied","by","G,","here,"]},"B","multiplied","by",{"-":["H","here"],"+":["H,","here,"]},"and","so",{"-":["on","so","let's"],"+":["on.","So","let\u2019s"]},"use","this","to","compute","all","the","predicted","output",{"-":["vectors","we"],"+":["vectors.","We"]},"can","create",{"-":[],"+":["a"]},"first",{"-":["4d"],"+":["4D"]},"array","containing","all","the","transformation",{"-":["matrices","there's"],"+":["matrices:","there\u2019s"]},"one","row","per","primary",{"-":["capsule"],"+":["capsule,"]},"and","one","column","per","digit",{"-":["capsule","the"],"+":["capsule.","The"]},"second","array","must","contain","the","output","vectors","of","each","primary",{"-":["capsule","then"],"+":["capsule.","Then"]},"we","just","pass","these","two","arrays","to","the",{"-":["mole","function"],"+":["matmul()","function,"]},"and","it","gives","us","the","predicted","output","vectors","for","all","the","pairs","of","primary","and","digit",{"-":["capsules","since"],"+":["capsules.","Since"]},"we","need","to","predict","the","outputs","of","all",{"-":["ten-digit"],"+":["10","digit"]},"capsules","for","each","primary",{"-":["capsule"],"+":["capsule,"]},"this","array","must","contain",{"-":["ten"],"+":["10"]},"copies","of","the","primary",{"-":["capsules","outputs","we"],"+":["capsules\u2019","outputs.","We"]},"will","use","the","tile","function","to","replicate","the","first","column","of","output",{"-":["vectors","ten","times","but","there's"],"+":["vectors,","10","times.","But","there\u2019s"]},"one","additional",{"-":["catch","we"],"+":["catch.","We"]},"want","to","make","these","predictions","for","all","the","instances","in","the",{"-":["batch"],"+":["batch,"]},"not","just","one",{"-":["instance","so","there's"],"+":["instance.","So","there\u2019s"]},"an","additional","dimension","for","the","batch",{"-":["size","it"],"+":["size.","It"]},"turns","out","that","the","primary","output","vectors","were","already","computed","for","every","single",{"-":["instance"],"+":["instance,"]},"so","the","second","array","is",{"-":["fine","it"],"+":["fine.","It"]},"already","has","this",{"-":["dimension","but"],"+":["dimension.","But"]},"we","need","to","replicate","the",{"-":["4d"],"+":["4D"]},"array","containing","all","the","transformation",{"-":["matrices"],"+":["matrices,"]},"so","that","we","end","up","with","one","copy","per","instance","in","the",{"-":["batch","now"],"+":["batch.","Now"]},"if","you","understand",{"-":["this"],"+":["this,"]},"then","the","code","should","be","pretty",{"-":["clear","first"],"+":["clear.","First,"]},"we","create","a","variable","containing","all","the","transformation",{"-":["matrices","it"],"+":["matrices.","It"]},"has",{"-":["a"],"+":[]},"one","row","per","primary",{"-":["capsule"],"+":["capsule,"]},"one","column","per","digit",{"-":["castle","capsule"],"+":["capsule,"]},"and","it","contains","16","by","8",{"-":["matrices","that's","four","dimensions"],"+":["matrices.","That\u2019s","4","dimensions,"]},"and","we","add","another","dimension","at","the","beginning","of","size","one",{"-":[],"+":["at","the","beginning"]},"to","make","it","easy","to","tile","this","array","for","each","instance","in","the",{"-":["batch","now","the"],"+":["batch.","The"]},"variable","is","initialized",{"-":["randomly"],"+":["randomly,"]},"using","a","normal","distribution","of","standard","deviation","0.01",{"-":["that's"],"+":["(that\u2019s"]},"a",{"-":["hyper","parameter"],"+":["hyperparameter"]},"you","can",{"-":["tweet","and"],"+":["tweak).","And"]},"that's","about",{"-":["it","we","create"],"+":["it.","Create"]},"this",{"-":["variable","next"],"+":["variable!","Next"]},"we","want","to","tile","this","array","for","each",{"-":["instance"],"+":["instance,"]},"so","first","we","need","to","know","the","batch",{"-":["size","we","don't"],"+":["size.","We","don\u2019t"]},"actually","know","it","at","graph","construction",{"-":["time"],"+":["time,"]},"it","will","only","be","known","when","we","run","the",{"-":["graph","but"],"+":["graph.","But"]},"we","can","use",{"-":["tensor","flows","shape","function"],"+":["TensorFlow\u2019s","shape()","function:"]},"it","creates","a","tensor","that",{"-":["will"],"+":["*will*"]},"know","the","shape","at",{"-":["run","time"],"+":["runtime,"]},"and","we","grab","its","first",{"-":["dimension"],"+":["dimension,"]},"which","is","the","batch",{"-":["size","then"],"+":["size.","Then"]},"we","simply","tile","our","big","W",{"-":["array"],"+":["array,"]},"along","the","first",{"-":["dimension"],"+":["dimension,"]},"to","get","one","copy","per",{"-":["instance","now"],"+":["instance.","Now,"]},"recall","that","the","output","of","the","primary","capsules","was","a",{"-":["three"],"+":["3"]},"dimensional",{"-":["array"],"+":["array:"]},"the","first","dimension","is","the","batch",{"-":["size"],"+":["size,"]},"that","we","will","know","at",{"-":["runtime"],"+":["runtime,"]},"then",{"-":["there's"],"+":["there\u2019s"]},"one","row","per",{"-":["capsule"],"+":["capsule,"]},"and","each","capsule","has",{"-":["eight","dimensions","so"],"+":["8","dimensions.","So"]},"we","need","to","reshape","this","array","a","bit","to","get","the","shape","that","we","are","looking",{"-":["for","to"],"+":["for,"]},"to","do","the","big",{"-":["mat","Mulla","operation","first"],"+":["matmul()","operation.","First"]},"we","add","an","extra","dimension","at","the",{"-":["end"],"+":["end,"]},"using",{"-":["tensor","flows","expand","dims","function","and"],"+":["TensorFlow\u2019s","expand_dims()","function.","The"]},"vectors","are","now","represented","as","column",{"-":["vectors"],"+":["vectors,"]},"instead","of",{"-":["one"],"+":["1"]},"dimensional",{"-":["array","each"],"+":["arrays.","Each"]},"of","these","is","a",{"-":[],"+":["column","vector.","A"]},"column","vector",{"-":["column","vector"],"+":[]},"is","a",{"-":["matrix"],"+":["matrix,"]},"a",{"-":["2d","array"],"+":["2D","array,"]},"with","a","single",{"-":["column","then"],"+":["column.","Then"]},"we","add","another",{"-":["dimension"],"+":["dimension,"]},"for","the","digit",{"-":["capsules","and"],"+":["capsules.","And"]},"we","replicate","all","the","output","vectors",{"-":["ten"],"+":["10"]},"times","across","this","new",{"-":["dimension"],"+":["dimension,"]},"once","per","digit",{"-":["capsule","and","lastly"],"+":["capsule.","And","lastly,"]},"we","just","use",{"-":["map","Moll"],"+":["matmul"]},"to","multiply","the","transformation","matrices","with","the","primary",{"-":["capsules"],"+":["capsules\u2019"]},"output",{"-":["vectors"],"+":["vectors,"]},"and","we","get","all","the","digit",{"-":["capsules"],"+":["capsule\u2019s"]},"predicted","outputs","for","each","pair","of","primary","and","digit",{"-":["capsules"],"+":["capsules,"]},"and","for","each","instance","in","the",{"-":["batch","in"],"+":["batch.","In"]},"one",{"-":["shot","and","that's"],"+":["shot.","And","that\u2019s"]},"the","end","of","the","first","step","for","computing","the","digit",{"-":["capsules","outputs"],"+":["capsules\u2019","outputs,"]},"we","now","have","a","bunch","of","predicted","output",{"-":["vectors","the"],"+":["vectors.","The"]},"second","step","is","the","routing","by","agreement",{"-":["algorithm","so","first"],"+":["algorithm.","So","first,"]},"we","set","all","the",{"-":[],"+":["raw"]},"routing","weights","to",{"-":["0","for","this"],"+":["0.","For","this,"]},"we","just","use",{"-":["TF","zeros","there","is"],"+":["tf.zeros().","There\u2019s"]},"one","weight","for","each","pair","of","primary","and",{"-":["digit","capsules","and"],"+":["digits","capsules,"]},"for","each",{"-":["instance","the"],"+":["instance.","The"]},"last","two","dimensions","here","are","equal","to",{"-":["1"],"+":["1,"]},"they","will","be","useful","in","a",{"-":["minute","next"],"+":["minute.","Next"]},"we","compute","the","softmax","of","each","primary",{"-":["capsules"],"+":["capsule\u2019s"]},"10","raw","routing",{"-":["weights","okay","so"],"+":["weights.","Okay?","So"]},"softmax","happens","along","this",{"-":["dimension","next"],"+":["dimension.","Next,"]},"we","compute","the","weighted","sum","of","all","the","predicted","output",{"-":["vectors"],"+":["vectors,"]},"for","each","digit",{"-":["capsule"],"+":["capsule,"]},"using",{"-":[],"+":["the","routing","weights.","The","weighted","sum","is","along","this","dimension.","This","is","pretty","straightforward","TensorFlow","code:","first","multiply"]},"the","routing","weights",{"-":["so","the","weighted","sum","is","along","this","dimension","this","is","pretty","straightforward","tensorflow","code","first","multiply","the","routing","weights"],"+":[]},"and","the","predicted","vectors",{"-":["this"],"+":["(this"]},"is",{"-":["a","element-wise","multiplication"],"+":["an","elementwise","multiplication,"]},"not","a","matrix",{"-":["multiplication"],"+":["multiplication),"]},"then","just","compute","the","sum","over","the","primary","capsule",{"-":["dimension","and"],"+":["dimension.","And"]},"the","two","dimensions","we","added","earlier","for","the","routing","weights","are","useful","in","the","multiplication",{"-":["step"],"+":["step,"]},"so","that","the","two","arrays","have","the","same","number","of",{"-":["dimensions"],"+":["dimensions,"]},"the","same",{"-":["rank","they","don't"],"+":["rank.","They","don\u2019t"]},"have","the","exact","same",{"-":["shape"],"+":["shape,"]},"but","they","have","compatible","shapes","so",{"-":["tensorflow"],"+":["TensorFlow"]},"will","perform",{"-":["broadcasting","now"],"+":["broadcasting.","Now"]},"if","you","don't","know","what","broadcasting",{"-":["is"],"+":["is,"]},"this","should","make","it",{"-":["clear","I'm"],"+":["clear.","I\u2019m"]},"multiplying","two",{"-":["matrices"],"+":["matrices,"]},"but","one","of","them","just","has","one",{"-":["row"],"+":["row,"]},"so",{"-":["tensorflow"],"+":["TensorFlow"]},"will","act","as","if","this","row","were","repeated","the","appropriate","number","of",{"-":["times","you"],"+":["times.","You"]},"could","achieve","the","same","thing","using",{"-":["tiling"],"+":["tiling,"]},"as","we","did",{"-":["earlier"],"+":["earlier,"]},"but","this","is","more",{"-":["efficient","and","you"],"+":["efficient.","You"]},"may","wonder","why","we",{"-":["didn't"],"+":["didn\u2019t"]},"use","broadcasting",{"-":["earlier"],"+":["earlier,"]},"but","the","reason","is","it","does","not","work","for","matrix",{"-":["multiplication","here","we're"],"+":["multiplication.","Here","we","are"]},"doing",{"-":["element","wise","multiplication","okay"],"+":["elementwise","multiplication.","Okay,"]},"back","to","the","digit",{"-":["capsules","we"],"+":["capsules.","We"]},"computed","a","weighted","sum","of","the","predicted",{"-":["vectors"],"+":["vectors,"]},"for","each","digit",{"-":["capsule"],"+":["capsule,"]},"and","we","just","run","the","squash",{"-":["function"],"+":["function,"]},"and","we","get","the","outputs","of","the","digit",{"-":["capsules","all","right","but","wait"],"+":["capsules.","Hurray!","But","wait,"]},"this","is","just","the","end","of","round",{"-":["one"],"+":["1"]},"of","the","routing","by","agreement",{"-":["algorithm","now"],"+":["algorithm.","Now,"]},"on","to","round",{"-":["two","so","first"],"+":["#2.","So","first,"]},"we","need","to","measure","how","good","each","prediction",{"-":["was"],"+":["was,"]},"and","use","this","to","update","the","routing",{"-":["weights","for","example"],"+":["weights.","For","example,"]},"look","at","the","predictions","that","we","made","using","this","primary",{"-":["capsules","output","notice","that"],"+":["capsule\u2019s","output.","Notice","that,"]},"for",{"-":["example"],"+":["example,"]},"the","prediction","for","digit",{"-":["four"],"+":["4,"]},"is",{"-":["excellent","and"],"+":["excellent.","And"]},"this","is","measured","using","the","scalar","product","of","the","predicted","output","vector","and","the","actual","output",{"-":["vector","these"],"+":["vector.","These"]},"two","vectors","are","actually","represented","as","column",{"-":["vectors"],"+":["vectors,"]},"meaning","a","matrix","with","a","single",{"-":["column","so"],"+":["column.","So"]},"to","compute","the","scalar",{"-":["product"],"+":["product,"]},"we","must","transpose","the","predicted","column","vector",{"-":["you","had","J","I"],"+":["\u00fb_j|i"]},"to","get","a","row",{"-":["vector"],"+":["vector,"]},"and","multiply",{"-":["this"],"+":[]},"this","row","vector","and","the","actual","output","vector",{"-":["V","J"],"+":["v_j,"]},"which","is","a","column",{"-":["vector","we"],"+":["vector.","We"]},"will","get","a",{"-":["one","by","one"],"+":["1x1"]},"matrix","containing","the","scalar","product","of","the",{"-":["vectors","and","now"],"+":["vectors.","Now"]},"of","course","we","need","to","do","this","for","each","predicted",{"-":["vector"],"+":["vector,"]},"so","once",{"-":["again"],"+":["again,"]},"we","can","use","the",{"-":["map","mol"],"+":["matmul()"]},"function","to","perform","all","the","matrix","multiplications","in","just","one",{"-":["shot","first"],"+":["shot.","First"]},"we","must","use","the",{"-":["tile"],"+":["tile()"]},"function","to","get","one","copy","of","the","actual","output","vectors",{"-":["V","J"],"+":["v_j,"]},"for","each","primary",{"-":["capsule","then"],"+":["capsule.","Then"]},"we","use",{"-":["map","mo"],"+":["matmul(),"]},"telling","it","to","transpose","each",{"-":["metric"],"+":[]},"matrix","in","the","first",{"-":["array"],"+":["array,"]},"on","the",{"-":["fly"],"+":["fly,"]},"and",{"-":["lo","and","behold"],"+":["lo-and-behold,"]},"we","get","all","the","scalar","products","at",{"-":["once","so","now"],"+":["once.","So","now,"]},"we","have","a","measure","of",{"-":["agreements"],"+":["the","agreement"]},"between","each","predicted","vector","and","the","actual","output",{"-":["vector","we"],"+":["vector.","We"]},"can","then","add","these","scalar","products","to","the","raw",{"-":["whites"],"+":["weights,"]},"using","a","simple",{"-":["addition","and"],"+":["addition.","And"]},"the","rest","of","round","2","is","exactly","the","same","as","the","end","of","round",{"-":["1","the"],"+":["1.","The"]},"code","is","really",{"-":["identical"],"+":["identical,"]},"except",{"-":["we're"],"+":["we\u2019re"]},"now","using","the","raw","routing",{"-":["ways"],"+":["weights"]},"of","round",{"-":["2","we"],"+":["2.","We"]},"compute","their","softmax","to","get","the","actual","routing","weights","for","round",{"-":["2"],"+":["2,"]},"then","we","compute","the","weighted","sum","of","all","the","predicted","vectors","for","each","digit",{"-":["capsule"],"+":["capsule,"]},"and","finally","we","squash","the",{"-":["results","and"],"+":["result.","And"]},"now","we","have","the","new","digit","capsule",{"-":["outputs"],"+":["outputs,"]},"and",{"-":["we","finish"],"+":["we\u2019ve","finished"]},"round",{"-":["2","we"],"+":["2.","We"]},"could","do","a","few","more","rounds","exactly","like","this",{"-":["one"],"+":["one,"]},"but",{"-":["I'll"],"+":["I\u2019ll"]},"stop",{"-":["now"],"+":["now,"]},"and","use","the","current","output","vectors","at","the","end","of","round","2","as","the","output","of","the","digit",{"-":["capsules","now"],"+":["capsules.","Now"]},"you","probably","noticed","that","I","implemented","the","routing",{"-":["algorithms"],"+":["algorithm\u2019s"]},"loop","without","an","actual",{"-":["loop","it's"],"+":["loop.","It\u2019s"]},"a","bit","like","computing","the","sum","of","squares","from","1","to",{"-":["100"],"+":["100,"]},"with","this",{"-":["code","of","course"],"+":["code.","Of","course,"]},"this","will","build","a","very","big",{"-":["tensor","flow","graph","but"],"+":["TensorFlow","graph.","But"]},"it",{"-":["works","you"],"+":["works.","You"]},"can","think","of","it","as","an","unrolled",{"-":["loop","now"],"+":["loop.","Now,","a"]},"cleaner",{"-":["a"],"+":[]},"way","to","do","this","would","be","to","write","a","for","loop","in",{"-":["Python"],"+":["Python,"]},"like",{"-":["this"],"+":["this.","Ah,"]},"much",{"-":["better","however","it's"],"+":["better.","However,","it\u2019s"]},"important","to","understand","that","the","resulting",{"-":["tensor","photograph"],"+":["TensorFlow","graph"]},"will","be","absolutely","identical","to","the","one","produced","by","the","previous",{"-":["code","all","we're"],"+":["code.","All","we","are"]},"doing","here","is","constructing","a",{"-":["graph"],"+":["graph,"]},"and",{"-":["tensorflow"],"+":["TensorFlow"]},"will","not","even","know","that","we",{"-":["use","the"],"+":["used","a"]},"loop","to","build",{"-":["a","graph","again"],"+":["the","graph.","Again,"]},"this","works",{"-":["fine","it's"],"+":["fine,","it\u2019s"]},"just","that","you","end","up","with","a","very","large",{"-":["graph","so"],"+":["graph.","So"]},"you","can","think","of","this","loop","as","a","static",{"-":["loop"],"+":["loop,"]},"that","only","runs","at","graph","construction",{"-":["time","if"],"+":["time.","If"]},"you","want","a","dynamic",{"-":["loop"],"+":["loop,"]},"one","that",{"-":["tensorflow"],"+":["TensorFlow"]},"itself","will",{"-":["run"],"+":["run,"]},"then","you","must","use",{"-":["tensor","flows","while","loop"],"+":["TensorFlow\u2019s","while_loop()"]},"function","like",{"-":["this","the","while","loop"],"+":["this.","The","while_loop()"]},"function","takes",{"-":["three","parameters"],"+":["3","parameters:"]},"the","first","one","is","a","function","that","must","return","a","tensor","that","will","determine","whether","the","loop","should","go","on","or",{"-":["not"],"+":["not,"]},"at","each",{"-":["iteration","the"],"+":["iteration.","The"]},"second","parameter","is","also","a","function","that","must","build","the","body","of","the",{"-":["loop","and"],"+":["loop,"]},"that","will","also","be","evaluated","at","each",{"-":["iteration","and","finally"],"+":["iteration.","And","finally,","the"]},"third","parameter","contains","a","list","of","tensors","that","will","be","sent","to","both","the",{"-":["condition"],"+":["condition()"]},"and",{"-":["loop","body","function"],"+":["loop_body()","functions"]},"at","the","first",{"-":["iteration","for"],"+":["iteration.","For"]},"the","following",{"-":["iterations"],"+":["iterations,"]},"these","functions","will","receive","the","output","of","the",{"-":["loop","body","function","so"],"+":["loop_body()","function.","So"]},"you","can","pause","the","video","if","you","need","to","take","a","closer","look","at","this",{"-":["code","once"],"+":["code.","Once"]},"you","get",{"-":["it"],"+":["it,"]},"you","can","try","modifying","my",{"-":["caps","net"],"+":["capsnet"]},"implementation","to","use","a","dynamic","loop","rather","than","a","static","unrolled",{"-":["loop","apart"],"+":["loop.","Apart"]},"from","making","the","code","cleaner","and","the","graph",{"-":["smaller"],"+":["smaller,"]},"using","a","dynamic","loop","allows","you","to","change","the","number","of","iterations","using","the","exact","same",{"-":["model","also"],"+":["model.","Also,"]},"if","you","set","the",{"-":["swap","memory"],"+":["swap_memory"]},"parameter","of","the",{"-":["while","loop","function"],"+":["while_loop()","function,"]},"if","you","set","it","to",{"-":["true","tensorflow"],"+":["True,","TensorFlow"]},"will","automatically","swap","the","GPU","memory","to","CPU","memory",{"-":["when","it","can"],"+":[]},"to","save","GPU",{"-":["memory","since"],"+":["memory.","Since"]},"CPU",{"-":["Ram"],"+":["RAM"]},"is","much","cheaper","and",{"-":["abundant"],"+":["abundant,"]},"this","can","really","be",{"-":["useful","and","that's","it","we've"],"+":["useful.","And","that\u2019s","it!","We\u2019ve"]},"computed","the","output","of","the","digit",{"-":["capsules","cool","now"],"+":["capsules.","Cool!","Now"]},"the","length","of","each","output","vector","represents","the","probability","that","a","digit","of","that","class","is","present","in","the",{"-":["image","so","let's"],"+":["image.","So,","let\u2019s"]},"compute","these",{"-":["probabilities","for","this"],"+":["probabilities.","For","this,"]},"we",{"-":["cannot"],"+":["can\u2019t"]},"use",{"-":["tensor","flows","norm"],"+":["tensorflow\u2019s","norm()"]},"function","because","training","will","explode","if",{"-":["there's"],"+":["there\u2019s"]},"a","zero","vector","at","any",{"-":["point"],"+":["point,"]},"as","I",{"-":["mentioned","earlier","so"],"+":["mentionned","earlier.","So"]},"instead","we",{"-":["used"],"+":["use"]},"a",{"-":["homemade","safe","norm","function"],"+":["home-made","safe_norm()","function,"]},"similar","to","what","we","did","with","the",{"-":["squash","function","and"],"+":["squash()","function.","And"]},"note","that","the","sum","of","the","probabilities",{"-":["don't"],"+":["don\u2019t"]},"necessarily","add","up","to",{"-":["one"],"+":["1,"]},"because","we","are","not","using","a",{"-":["soft","max","layer","this"],"+":["softmax","layer.","This"]},"makes","it","possible","to","detect","multiple","different","digits","in","the","same","image",{"-":["but"],"+":["(but"]},"they","all","have","to","be","different",{"-":["digits","view"],"+":["digits:"]},"you","can","detect","a","5","and",{"-":["a","3"],"+":["3,"]},"but","you",{"-":["cannot","detect","say"],"+":["can\u2019t","detect,","say,"]},"two",{"-":["fives","next","let's"],"+":["5s).","Next,","let\u2019s"]},"predict","the","most","likely",{"-":["digit","we"],"+":["digit.","We"]},"just",{"-":["used"],"+":["use"]},"the",{"-":["Arg","max"],"+":["argmax()"]},"function","that","gives",{"-":["us"],"+":["use"]},"the","index","of","the","highest",{"-":["probability","the"],"+":["probability.","The"]},"index","happens","to","be","the",{"-":["number","of"],"+":["number,"]},"the","digit",{"-":["itself","note"],"+":["itself.","Note"]},"that","we","first","get","a","tensor","that","has","a","couple","extra","dimensions","of","size",{"-":["one"],"+":["1"]},"at","the",{"-":["end"],"+":["end,"]},"so","we","get","rid","of","them","using","the",{"-":["squeeze","function","if"],"+":["squeeze()","function.","If"]},"we","called",{"-":["squeeze"],"+":["squeeze()"]},"without","specifying","the","axes","to",{"-":["remove"],"+":["remove,"]},"it","would","remove","all","dimensions","of","size",{"-":["1","this"],"+":["1.","This"]},"would","generally","be",{"-":["okay"],"+":["okay,"]},"except","if","the","batch","size","was","equal","to",{"-":["1"],"+":["one,"]},"in","which",{"-":["case"],"+":["case,"]},"we","would","be","left","with","a","scalar",{"-":["value"],"+":["value,"]},"rather","than","an",{"-":["array"],"+":["array,"]},"and","we",{"-":["don't"],"+":["don\u2019t"]},"want",{"-":["that","so"],"+":["that.","So"]},"it's","better","to","specify","the",{"-":["axes","great"],"+":["axes.","Great,"]},"now","we","have","a","capsule","network","that","can","estimate","class","probabilities","and","make",{"-":["predictions","we"],"+":["predictions.","We"]},"can","measure","the",{"-":["models"],"+":["model\u2019s"]},"accuracy","on","the","batch","by","simply","comparing","the","predictions","and","the",{"-":["labels","in"],"+":["labels.","In"]},"this","case","the","prediction","for","the","last","digit","in","the","batch","is",{"-":["wrong","it's","seven"],"+":["wrong,","it\u2019s","7"]},"instead","of",{"-":["one","so"],"+":["1.","So"]},"we","get","80%",{"-":["accuracy","now","the"],"+":["accuracy.","The"]},"code","is","really",{"-":["straightforward"],"+":["straightforward:"]},"we","just","use","the",{"-":["equal"],"+":["equal()"]},"function","to","compare","the","labels","and","the","predictions",{"-":["wipe","red"],"+":["y_pred,"]},"and","this","gives","us","an","array","of",{"-":["boolean","z'"],"+":["booleans,"]},"so","we","cast","these",{"-":["boolean","stew","floats"],"+":["booleans","to","floats,"]},"which","gives","us","a","bunch","of",{"-":["zeros","for"],"+":["0s","(for"]},"bad",{"-":["predictions"],"+":["predictions)"]},"and",{"-":["ones","for"],"+":["1s","(for"]},"good",{"-":["predictions"],"+":["predictions),"]},"and","we","compute","the","mean","to","get","the",{"-":["batch","accuracy","the"],"+":["batch\u2019s","accuracy.","The"]},"labels",{"-":["Y"],"+":["y"]},"are","just","a","regular",{"-":["placeholder","nothing","special","and","that's","it"],"+":["placeholder.","Nothing","special.","And","that\u2019s","it,"]},"we","have","a","full",{"-":["model"],"+":["model,"]},"able","to","make",{"-":["predictions","now","let's"],"+":["predictions.","Now","let\u2019s"]},"look","at","the","training",{"-":["code","this"],"+":["code.","This"]},"diagram","is","about",{"-":["two"],"+":["to"]},"get","pretty","crowded","so",{"-":["I'll"],"+":["I\u2019ll"]},"remove","the","accuracy","for",{"-":["clarity","and","now","first"],"+":["clarity.","And","now,","first,"]},"we","want","to","compute","the","margin",{"-":["loss","it's"],"+":["loss.","It\u2019s"]},"given","by","this",{"-":["equation","by"],"+":["equation.","By"]},"the",{"-":["way"],"+":["way,"]},"I","made","a","mistake","in","my","first",{"-":["video"],"+":["video:"]},"I","squared",{"-":[],"+":["the"]},"norms","instead","of",{"-":["squaring"],"+":[]},"the","max",{"-":["operations","sorry"],"+":["operations.","Sorry"]},"about",{"-":["that","this"],"+":["that.","This"]},"here","is","the","correct",{"-":["equation","computing"],"+":["equation.","Computing"]},"it","is","pretty",{"-":["straightforward"],"+":["straightforward,"]},"so","I",{"-":["won't"],"+":["won\u2019t"]},"go","through","it","in",{"-":["details","the"],"+":["details.","The"]},"only","trick","is","to","understand","how","you","can","easily","compute","all","the",{"-":["TK","values","for"],"+":["T_k","values.","For"]},"a","given",{"-":["instance","TK"],"+":["instance,","T_k"]},"is","equal","to","1","if","a","digit","of",{"-":["cos","K"],"+":["class","k"]},"is","present","in","the",{"-":["image"],"+":["image,"]},"otherwise",{"-":["it's"],"+":["it\u2019s"]},"equal","to",{"-":["0"],"+":["0.","So,"]},"you","can","get","all","the",{"-":["TK"],"+":["T_k"]},"values","for","each","instance","by","simply","converting","the","labels","to",{"-":["one","hot","representation","for","example"],"+":["a","one-hot","representation.","For","example,"]},"if","an",{"-":["instance"],"+":["instance\u2019s"]},"label","is",{"-":["three"],"+":["3,"]},"then","for","this",{"-":["instance"],"+":["instance,"]},"T","will","contain","a","10","dimensional","vector","full","of","zeros","except","for","a","1",{"-":["its"],"+":["at"]},"index",{"-":["3","okay","next"],"+":["3.","Okay!","Next,"]},"we","want","to","compute","the","reconstruction",{"-":["loss","so","first"],"+":["loss.","So","first,"]},"we","must","send","the","outputs","of","the","digit","capsules","to","a","decoder","that","will","try","to","use","them","to","reconstruct","the","input",{"-":["images","this"],"+":["images.","This"]},"decoder","is","just","a","regular",{"-":["feed-forward"],"+":["feedforward"]},"neural","net","composed","of",{"-":["three"],"+":["3"]},"fully","connected",{"-":["layers","it's"],"+":["layers.","It\u2019s"]},"really","simple",{"-":["code"],"+":["code,"]},"so","you","can","pause","the","video","if","you","want",{"-":["and"],"+":["to"]},"take","a","close","look","at",{"-":["it","it"],"+":["it.","It"]},"outputs","an","array","containing","784","values","from","0","to",{"-":["1"],"+":["1,"]},"for","each",{"-":["instance"],"+":["instance,"]},"representing","the","pixel","intensities","of",{"-":["28","by","28"],"+":["28x28"]},"pixel",{"-":["images","and","that's","it"],"+":["images.","And","that\u2019s","it,"]},"we","have","our","reconstructed",{"-":["images","we"],"+":["images!","We"]},"can","now","compute","the","reconstruction",{"-":["loss","this"],"+":["loss.","This"]},"is","just","the","squared","difference","between","the","input","images","and",{"-":["the","reconstructions","since"],"+":["their","reconstructions.","Since"]},"the","input","images","are",{"-":["28","by","28","by","1"],"+":["28x28x1,"]},"we","first","reshape","them","to","one","dimension","per","instance","with","784","values",{"-":["each","then"],"+":["each.","Then"]},"we","compute","the","squared",{"-":["difference","now"],"+":["difference.","Now"]},"we","can","compute","the","final",{"-":["loss","it's"],"+":["loss!","It\u2019s"]},"just","the","sum","of","the","margin","loss","and","the","reconstruction",{"-":["loss","scale"],"+":["loss,","scaled"]},"down","to","let","the","margin","loss","dominate",{"-":[],"+":["training.","Pretty","simple,","as","you","can","see.","Now","let\u2019s","add","the"]},"training",{"-":["pretty","simple","as","you","can","see","now","let's","add","the","training","operation","the"],"+":["operation.","The"]},"paper","mentions","they",{"-":["use","tensor","flows"],"+":["used","TensorFlow\u2019s"]},"implementation","of","the",{"-":["atom","optimizer"],"+":["Adam","optimizer,"]},"using","the","default",{"-":["parameters"],"+":["parameters,"]},"so",{"-":["let's"],"+":["let\u2019s"]},"do",{"-":["that","we"],"+":["that.","We"]},"create","the",{"-":["optimizer"],"+":["optimizer,"]},"and","call",{"-":["it's","minimized"],"+":["its","minimize()"]},"method","to","get","the","training","operation","that","will","tweak","the","model","parameters","to","minimize","the",{"-":["loss","we're"],"+":["loss.","We\u2019re"]},"almost",{"-":["done"],"+":["done,"]},"but",{"-":["there's"],"+":["there\u2019s"]},"one","last","detail","I",{"-":["didn't"],"+":["didn\u2019t"]},"mention","in","the","first",{"-":["video","the"],"+":["video.","The"]},"paper","indicates","that","the","outputs","of","the","digit","capsules","should","all","be","masked","out","except","for","the","ones","corresponding","to","the","target",{"-":[],"+":["digit.","So","instead","of","sending","the"]},"digit",{"-":["so","instead","of","sending","the","digit","capsules"],"+":["capsules\u2019"]},"outputs","directly","to","the",{"-":["decoder"],"+":["decoder,"]},"we","want","to","apply","a","mask",{"-":["first"],"+":["first,"]},"like",{"-":["this","the"],"+":["this.","The"]},"mask","will","have","the","same","shape","as","the","digit","capsules","output",{"-":["array"],"+":["array,"]},"and","it","will","be","equal","to",{"-":["zero"],"+":["0"]},"everywhere","except","for",{"-":["once"],"+":["1s"]},"at","the","location","of","the","target",{"-":["digits","by"],"+":["digits.","By"]},"multiplying","the","digit",{"-":["capsules"],"+":["capsules\u2019"]},"output",{"-":["in"],"+":["and"]},"the",{"-":["mask"],"+":["mask,"]},"we","get","the","input","to","the",{"-":["decoder","but","there's"],"+":["decoder.","But","there\u2019s"]},"one",{"-":["catch","this"],"+":["catch.","This"]},"picture","is","good","for",{"-":["training"],"+":["training,"]},"but","at","test",{"-":["time"],"+":["time,"]},"we",{"-":["won't"],"+":["won\u2019t"]},"have","the",{"-":["labels","so","instead"],"+":["labels.","So","instead,"]},"we","will",{"-":["master"],"+":["mask"]},"the","output","vectors","using","the","predicted","classes","rather","than",{"-":["labels"],"+":["the","labels,"]},"like",{"-":["this","now"],"+":["this.","Now"]},"we","could","build","a","different","graph","for","training","and","for",{"-":["testing"],"+":["testing,"]},"but","it",{"-":["wouldn't"],"+":["wouldn\u2019t"]},"be","very",{"-":["convenient","so","instead","let's"],"+":["convenient.","So","instead,","let\u2019s"]},"build","a",{"-":["conditioned","operation","we"],"+":["condition","operation.","We"]},"will","add","a","boolean",{"-":["a"],"+":[]},"placeholder","called",{"-":["mask","with"],"+":["mask_with_labels.","If","it","is","true,","then","we","use","the"]},"labels",{"-":["if"],"+":["to","build","the","mask.","If"]},"it","is",{"-":["true"],"+":["False,"]},"then","we","use","the",{"-":["labels","to"],"+":["prediction.","Note","the","difference.","Okay.","And","here\u2019s","the","code.","We"]},"build","the",{"-":["mask","if","it","is","false","then","we","use","the","prediction","note","the","difference","okay","and","here's","the","code","we","build","the","mask","of","labels","placeholder"],"+":["mask_with_labels","placeholder,"]},"which","will","default","to",{"-":["false"],"+":["False"]},"so","that","we","only","need","to","set","it","during",{"-":["training","and"],"+":["training.","And"]},"then","we","define","the","reconstruction","targets","using",{"-":["tensorflow","scon","function","it"],"+":["TensorFlow\u2019s","cond()","function.","It"]},"takes",{"-":["three","arguments"],"+":["3","arguments:"]},"the","first","one","is","a","tensor","representing","the",{"-":["condition"],"+":["condition,"]},"in","this","case","simply","the",{"-":["mask","with","labels","placeholder","the"],"+":["mask_with_labels","placeholder.","The"]},"second","parameter","is","a","function","that","returns","the","tensor","to","use","if","the","condition","is",{"-":["true"],"+":["True,"]},"and","the","third","parameter","is","a","function","that","returns","the","tensor","to","use",{"-":["that"],"+":["if"]},"the","condition",{"-":["is","false","then"],"+":["if","False.","Then"]},"to","build","the",{"-":["mask"],"+":["mask,"]},"we","simply","use","the",{"-":["one-hot","function","now","there's"],"+":["one_hot()","function.","Now","there"]},"actually","one","slight","problem","with","this",{"-":["implementation"],"+":["implementation,"]},"and","to","explain",{"-":["it"],"+":["it,"]},"I","need","to","step","back","for","a","second","and","talk","about","how",{"-":["tensorflow"],"+":["TensorFlow"]},"evaluates",{"-":["suppose"],"+":["a","tensor.","Suppose"]},"we","built","this",{"-":["graph"],"+":["graph,"]},"these","are","all","tensorflow",{"-":["operations"],"+":["operations,"]},"and","we","want","to","evaluate","the","output","of","operation",{"-":["a","the"],"+":["A.","The"]},"first","thing",{"-":["tensorflow"],"+":["TensorFlow"]},"will","do","is","resolve","the",{"-":["dependencies"],"+":["dependencies.","It","will","find","all","the","operations","that","A","depends","on,","directly","or","indirectly,","by","traversing","the","graph","backwards.","In","this","case"]},"it","will","find",{"-":["all","the","operations","that","a","depends","on","directly","or","indirectly","by","traversing","the","graph","backwards","in","this","case","we'll","find","C"],"+":["C,"]},"D","and",{"-":["F","next"],"+":["F.","Next,"]},"it","will","run","any",{"-":["of","these","operations"],"+":["operation"]},"that","has","no",{"-":["inputs","these"],"+":["inputs.","These"]},"are","called","root",{"-":["nodes","in"],"+":["nodes.","In"]},"this","case",{"-":[],"+":["F.","Once"]},"F",{"-":["once","F"],"+":[]},"is",{"-":["evaluated","operation"],"+":["evaluated,","operations"]},"C","and","D","now","have","all","the","inputs","they",{"-":["need"],"+":["need,"]},"so","they","can","be",{"-":["evaluated"],"+":["evaluated,"]},"and",{"-":["tensorflow"],"+":["TensorFlow"]},"will","actually","try","to","run","them","in",{"-":["parallel","say"],"+":["parallel.","Say"]},"D","finishes",{"-":["first","a"],"+":["first,","A"]},"still","has","one",{"-":["unev","alyou","ated"],"+":["unevaluated"]},"input","so","it",{"-":["can't"],"+":["can\u2019t"]},"run",{"-":["yet","but"],"+":["yet.","But"]},"as","soon","as","C","is",{"-":["finished","a"],"+":["finished,","A"]},"can","be",{"-":["evaluated","and"],"+":["evaluated.","And"]},"once",{"-":["it's","done"],"+":["it\u2019s","done,"]},"the",{"-":["eval"],"+":["eval()"]},"method","returns","the",{"-":["result","in","we're","good","you"],"+":["result,","and","we\u2019re","good.","You"]},"can","actually","evaluate","multiple","operations","at",{"-":["once"],"+":["once,"]},"for","example",{"-":["a"],"+":["A"]},"and",{"-":["E"],"+":["E,"]},"and","the","process","is","really","the",{"-":["same","it"],"+":["same.","It"]},"finds","all","the",{"-":["dependencies"],"+":["dependencies,"]},"runs","the","root",{"-":["operations"],"+":["operations,"]},"and",{"-":["then"],"+":["then,"]},"you",{"-":["know"],"+":["know,"]},"goes","upward","running","every","operation","whose","inputs","are",{"-":["satisfied","and"],"+":["satisfied.","And"]},"once","it's","got","both","the","values","for",{"-":["a"],"+":["both","A"]},"and",{"-":["E"],"+":["E,"]},"it","returns","the",{"-":["results","so","let's"],"+":["results.","So","let\u2019s"]},"apply","this","to","our","reconstruction",{"-":["targets","this"],"+":["targets.","This"]},"tensor","is","the","output","of","the",{"-":["cond","operation"],"+":["cond()","operation,"]},"which","has",{"-":["three","parameters","mask","with","labels"],"+":["3","parameters,","mask_with_labels,"]},"a","function","that","returns",{"-":["Y"],"+":["y,"]},"and","a","function","that","returns",{"-":["wipe","red","and"],"+":["y_pred.","And"]},"when","we","evaluate","the",{"-":["reconstruction","targets"],"+":["reconstruction_targets,"]},"or","any","tensor","that","depends","on",{"-":["it"],"+":["it,"]},"such","as","the","final",{"-":["loss"],"+":["loss,"]},"which","depends","on","the","reconstruction",{"-":["loss"],"+":["loss,"]},"which","eventually","depends","on","the",{"-":["reconstruction","targets","well"],"+":["reconstruction_targets.","Well,"]},"what","happens",{"-":["is"],"+":["it,"]},"as",{"-":["earlier","tensorflow"],"+":["earlier,","TensorFlow"]},"starts","by","resolving","the",{"-":["dependencies","it"],"+":["dependencies.","It"]},"finds","all","three","bottom",{"-":["nodes","and"],"+":["nodes.","And"]},"it","evaluates","them",{"-":["all","so","Y","pred"],"+":["all!","So","y_pred"]},"may","finish",{"-":["first","since"],"+":["first.","Since"]},"these","operations","are","run","in",{"-":["parallel","there's"],"+":["parallel,","there\u2019s"]},"no","way","to","know","in","which","order","they","will",{"-":["finish","so"],"+":["finish.","So,"]},"you",{"-":["know","Y"],"+":["know,","y"]},"may","finish",{"-":["next","and"],"+":["next.","And"]},"finally",{"-":["mask","what","label","finishes","so"],"+":["mask_with_labels","finishes.","So"]},"suppose","it","evaluates","to",{"-":["true","now"],"+":["True.","Now"]},"the",{"-":["reconstruct","target's"],"+":["reconstruction_targets"]},"has","all","the","inputs","it",{"-":["needs"],"+":["needs,"]},"so","it","can",{"-":["it","can"],"+":[]},"be",{"-":["evaluated","and"],"+":["evaluated.","And"]},"of","course","it","does","the","right",{"-":["thing"],"+":["thing,"]},"since",{"-":["masks","with","labels"],"+":["mask_with_labels"]},"is",{"-":["true"],"+":["True,"]},"it","returns","the","value","of",{"-":["y","which"],"+":["y.","Which"]},"is",{"-":["good","but"],"+":["good.","But"]},"notice","that",{"-":["y","pred"],"+":["y_pred"]},"was","evaluated","for",{"-":["nothing","we're"],"+":["nothing,","we\u2019re"]},"not","using","its",{"-":["output","it's"],"+":["output.","It\u2019s"]},"not","a","big","deal","since","during","training","we","need","to","evaluate","the","margin",{"-":["loss"],"+":["loss,"]},"which","depends","on","the","estimated","class",{"-":["probabilities"],"+":["probabilities,"]},"which","is","just","one","step","away","from","the",{"-":["prediction"],"+":["predictions,"]},"so","computing","the","predictions",{"-":["won't"],"+":["won\u2019t"]},"add","much",{"-":["overhead","but","still","it's"],"+":["overhead.","But","still,","it\u2019s"]},"a","bit",{"-":["unfortunate","now"],"+":["unfortunate.","Now"]},"suppose",{"-":["mask","what","labels"],"+":["mask_with_labels"]},"evaluates","to",{"-":["false","then","again"],"+":["False.","Then,","again,"]},"the",{"-":["reconstruction","targets"],"+":["reconstruction_targets"]},"will","do","the","right",{"-":["thing"],"+":["thing,"]},"it","will","output","the","value","of",{"-":["y","pred","but"],"+":["y_pred.","But"]},"this",{"-":["time","how"],"+":["time,"]},"we","evaluated","y","for",{"-":["nothing","it's"],"+":["nothing.","It\u2019s"]},"just","a",{"-":["placeholder"],"+":["placeholder,"]},"so","it",{"-":["won't"],"+":["won\u2019t"]},"add","much","computation",{"-":["time"],"+":["time,"]},"but","it","means","that","we","must","feed",{"-":["why"],"+":["y,"]},"even","if",{"-":["mask","with","labels"],"+":["mask_with_labels"]},"is",{"-":["false","well"],"+":["False.","Well"]},"actually","we","can","just","pass","an","empty",{"-":["array","and"],"+":["array,"]},"that's",{"-":["fine","so"],"+":["fine.","So"]},"it","will",{"-":["work"],"+":["work,"]},"but","it's","kind","of",{"-":["ugly","so"],"+":["ugly.","So"]},"this","unfortunate","situation","is","due","to","the","fact","that","the","functions","we","passed","to","the",{"-":["con","function"],"+":["cond()","function,"]},"do","not","actually","create","any",{"-":["tensor"],"+":["tensor,"]},"they","just","return","tensors","that","were","created","outside","of","these",{"-":["functions","so","if"],"+":["functions.","If"]},"you","build","tensors","within","these",{"-":["functions"],"+":["functions,"]},"then",{"-":["tensorflow"],"+":["TensorFlow"]},"will","do","what","you",{"-":["expect","it"],"+":["expect.","It"]},"will","stitch","together","the","partial","graphs","created",{"-":["within"],"+":["by"]},"these","functions","into","a","graph","that","will","properly","handle","the",{"-":["dependencies","so"],"+":["dependencies.","So"]},"you","might","be","able","to","modify","my","code","and","fix","this",{"-":["ugliness"],"+":["ugliness."]},"I",{"-":["tried"],"+":["tried,"]},"but","I","ended","up","with","pretty","convoluted",{"-":["code"],"+":["code,"]},"so","I","decided","to","stick","with","this",{"-":["implementation"],"+":["implementation."]},"I","hope","it",{"-":["won't"],"+":["won\u2019t"]},"keep","you","awake","at",{"-":["night","so","we've"],"+":["night.","Sooo!","We\u2019ve"]},"actually",{"-":["finished","here's"],"+":["finished!","Here\u2019s"]},"the","full","picture",{"-":["again","the"],"+":["again.","The"]},"construction","phase","is",{"-":["over"],"+":["over,"]},"our","graph","is",{"-":["built"],"+":["built,"]},"now","on","to","the","execution",{"-":["phase","let's"],"+":["phase,","let\u2019s"]},"run","this",{"-":["graph","first","let's"],"+":["graph.","First,","let\u2019s"]},"look","at","the","training",{"-":["code","it's"],"+":["code.","It\u2019s"]},"really","really","completely",{"-":["standard","you"],"+":["standard.","We"]},"create","a",{"-":["session"],"+":["session,"]},"if","a",{"-":["check","point"],"+":["checkpoint"]},"file",{"-":["exists"],"+":["exists,"]},"load",{"-":["it"],"+":["it,"]},"or","else","initialize","all","the",{"-":["variables","then"],"+":["variables.","Then"]},"run","the","main","training",{"-":["loop"],"+":["loop,"]},"for","a","number","of",{"-":["epochs"],"+":["epochs,"]},"and","for","each",{"-":["epoch"],"+":["epoch,"]},"run","enough","iterations","to","go","through","the","full","training",{"-":["set","and"],"+":["set.","And"]},"inside","this",{"-":["loop"],"+":["loop,"]},"we","simply","evaluate","the","training",{"-":["operation"],"+":["operation,"]},"and","the",{"-":["loss"],"+":["loss,"]},"on","the","next",{"-":["batch","we"],"+":["batch.","We"]},"feed","the","images","and","the","labels","of","the","current","training",{"-":["batch"],"+":["batch,"]},"and","we","set",{"-":["masks","with","labels"],"+":["mask_with_labels"]},"to",{"-":["true","that's"],"+":["True.","That\u2019s"]},"pretty","much","all","there","is","to",{"-":["it","and"],"+":["it.","In"]},"the",{"-":["note","book"],"+":["notebook,"]},"I","also","added","a","simple","implementation","of","early",{"-":["stopping"],"+":["stopping,"]},"and","I","print","out","the",{"-":["progress"],"+":["progress,"]},"plus","I","evaluate","the","model","on","the","validation","set","at","the","end","of","each",{"-":["epoch","but"],"+":["epoch.","But"]},"the","most","important","part","is",{"-":["here","after","training"],"+":["here.","After","training,"]},"I","just","run","a","few",{"-":["tests"],"+":["test"]},"images","through","the","network","and","get","the","predictions","and",{"-":["reconstructions","as"],"+":["reconstructions.","As"]},"you","can",{"-":["see"],"+":["see,"]},"the","predictions","are","all",{"-":["correct"],"+":["correct,"]},"and","the","reconstructions","are","pretty",{"-":["good"],"+":["good,"]},"they're","pretty","close","to","the","original",{"-":["images"],"+":["images,"]},"except","that",{"-":["they're"],"+":["they\u2019re"]},"slightly",{"-":["fuzzier","and"],"+":["fuzzier,"]},"as","you","can",{"-":["see","here's"],"+":["see.","Here\u2019s"]},"the",{"-":["code","there's"],"+":["code,","there\u2019s"]},"really","nothing","special","about",{"-":["it"],"+":["it:"]},"I","take","a","few",{"-":["images"],"+":["test","images,"]},"I","start","a","session","and","load","the",{"-":["model"],"+":["model,"]},"then","I","just","evaluate",{"-":[],"+":["the"]},"predictions",{"-":["the","predictions"],"+":[]},"and","the",{"-":["decoders"],"+":["decoder\u2019s"]},"output",{"-":["and"],"+":["(and"]},"I","also","get","the",{"-":["capsules","output"],"+":["capsules\u2019","outputs"]},"so","I","can","tweak","them",{"-":["later","the"],"+":["later).","The"]},"ugliness","I",{"-":["mentioned"],"+":["mentionned"]},"earlier","is","right",{"-":["here","right"],"+":["here."]},"I'm","forced","to","pass","an","empty",{"-":["array","this"],"+":["array.","This"]},"value","will","be","ignored",{"-":["anyway","and","finally"],"+":["anyway.","And","finally,"]},"the","code","tweaks","the","output","vectors","and","passes","the",{"-":["results"],"+":["result"]},"to","the",{"-":["decoder","so"],"+":["decoder.","So"]},"we","can","see","what","each","of",{"-":[],"+":["of"]},"the",{"-":["sixteen"],"+":["16"]},"dimensions","represent",{"-":["and"],"+":["in"]},"the","digit",{"-":["capsules"],"+":["capsule\u2019s"]},"output",{"-":["vector","for","example"],"+":["vector.","For","example,"]},"this","image","shows","the","reconstructions","we","get","by","tweaking","the","first","parameter",{"-":["so","the"],"+":["(the"]},"notebook","produces","one","such","image","for","each","of","the",{"-":["sixteen","parameters","and"],"+":["16","parameters).","And"]},"as","you","can",{"-":["see"],"+":["see,"]},"in","the",{"-":["first"],"+":["first,"]},"second","and","last",{"-":["rows"],"+":["rows,"]},"we","see","that","the","digits","become","thinner","and",{"-":["thinner","we're"],"+":["thinner,"]},"going","to","the",{"-":["right"],"+":["right,"]},"or","thicker","and","thicker","to","the",{"-":["left"],"+":["left,"]},"so","it","holds","information","about",{"-":["thickness","and"],"+":["thickness.","And"]},"in","the","middle",{"-":["row"],"+":["row,"]},"you","can","see","that","the","bottom","part","of","the","number",{"-":["five"],"+":["5"]},"gets","lifted","towards","the",{"-":["top"],"+":["top,"]},"so","probably","that's","what","this","parameter","does","for","this",{"-":["digit","before"],"+":["digit.","Before"]},"I",{"-":["finish","I'd"],"+":["finish,","I\u2019d"]},"like","to","thank","everyone","who","shared","and","commented","on","my","first",{"-":["video"],"+":["video,"]},"I","really","had","no","idea","it","would","receive","such","an","enthusiastic",{"-":["response"],"+":["response,"]},"and",{"-":["I'm"],"+":["I\u2019m"]},"very","very","grateful","to","all","of",{"-":["you"],"+":["you,"]},"it","definitely","motivates","me","to",{"-":[],"+":["create"]},"more",{"-":["videos","if"],"+":["videos.","If"]},"you","want","to","learn","more","about",{"-":["machine","learning"],"+":["Machine","Learning"]},"and","support","this",{"-":["channel"],"+":["channel,"]},"check","out","my",{"-":["O'Reilly"],"+":["O\u2019Reilly"]},"book",{"-":["hands","on","machine","learning"],"+":["Hands-on","Machine","Learning"]},"with",{"-":["scikit-learn"],"+":["Scikit-Learn"]},"and",{"-":["tensorflow","I"],"+":["TensorFlow,","I\u2019ll"]},"leave","the","links","in","the","video",{"-":["description","if"],"+":["description.","If"]},"you","speak",{"-":["German","there's"],"+":["German,","there\u2019s"]},"actually","a","German","translation","coming","up","for",{"-":["Christmas","and"],"+":["Christmas.","And"]},"if","you","speak",{"-":["French"],"+":["French,"]},"the","translation","is","already",{"-":["available","it"],"+":["available.","It"]},"was","split","in","two","books","but",{"-":["it's"],"+":["it\u2019s"]},"really","the","same",{"-":["content","and","that's"],"+":["content.","And","that\u2019s"]},"all","I",{"-":["have"],"+":["had"]},"for",{"-":["today"],"+":["today,"]},"I","hope","you","enjoyed","this","video","and","that","you","learned","a","thing","or","two","about",{"-":["tensorflow"],"+":["TensorFlow"]},"and","capsule",{"-":["networks","if"],"+":["networks.","If"]},"you",{"-":["did","please","like","share","comment","subscribe"],"+":["did,","please,","like,","share,","comment,","subscribe,"]},"and","click","on","the","bell","icon","next","to","the","subscribe",{"-":["button"],"+":["button,"]},"to","receive","notifications","when","I","upload","new",{"-":["videos","see"],"+":["videos.","See"]},"you","next",{"-":["time"],"+":["time!"]}]},"common_to_both_seq":{"pPN8d0E3900":["","","","","and","in","this","video","","tell","you","all","about","","","a","hot","new","architecture","for","neural","","","","had","the","idea","of","","","several","years","","and","he","published","a","paper","in","2011","that","introduced","many","of","the","key","","but","he","had","a","hard","time","making","them","work","","until","","","few","weeks","","in","October","","a","paper","called","","","","","was","published","by","","","Nicholas","","and","of","course","Geoffrey","","","managed","to","reach","","performance","on","the","","","and","demonstrated","considerably","better","results","than","convolutional","neural","nets","on","highly","overlapping","","","what","are","capsule","networks","","","in","computer","","you","start","with","an","abstract","representation","of","a","","for","example","a","rectangle","at","position","","","and","","","","rotated","by","","","","and","so","","","object","type","has","various","instantiation","","","you","call","some","rendering","","and","","you","get","an","","","","is","just","the","reverse","","","start","with","an","","and","you","try","to","find","what","objects","it","","and","what","their","instantiation","parameters","","","capsule","","is","basically","a","neural","network","that","tries","to","perform","inverse","","","is","composed","of","many","","","capsule","is","any","function","that","tries","to","predict","the","presence","and","the","instantiation","parameters","of","a","particular","object","at","a","given","","","","the","network","above","contains","50","","","arrows","represent","the","output","vectors","of","these","","capsules","output","","","black","arrows","correspond","to","capsules","that","try","to","find","","while","the","blue","arrows","represent","the","output","of","capsules","looking","for","","","length","of","an","activation","vector","represents","the","estimated","probability","that","the","object","the","capsule","is","looking","for","is","indeed","","","","can","see","that","most","arrows","are","","meaning","the","capsules","","detect","","but","two","arrows","are","quite","","","means","that","the","capsules","at","these","locations","are","pretty","confident","that","they","found","what","they","were","looking","","in","this","case","a","","and","","","","the","orientation","of","the","activation","vector","encodes","the","instantiation","parameters","of","the","","for","example","in","this","case","the","","","but","it","could","be","also","its","","how","stretched","or","skewed","it","","its","exact","","","might","be","slight","","and","so","","","","","just","focus","on","the","rotation","","but","in","","real","capsule","","the","activation","vectors","may","have","","","dimensions","or","","","","a","good","way","to","implement","this","is","to","","apply","a","couple","convolutional","","just","like","in","a","regular","convolutional","neural","","","will","output","an","array","containing","a","bunch","of","feature","","","can","then","reshape","this","array","to","get","a","set","of","vectors","for","each","","","","suppose","the","convolutional","layers","output","an","array","","","18","feature","maps","","times","","you","can","easily","reshape","this","array","to","get","","vectors","of","","dimensions","","for","every","","","could","also","get","","vectors","of","","dimensions","","and","so","","","that","would","look","like","the","capsule","network","represented","here","with","two","vectors","at","each","","","last","step","is","to","ensure","that","no","vector","is","longer","than","","since","the","","length","is","meant","to","represent","a","","it","cannot","be","greater","than","","","do","","we","apply","a","squashing","","","preserves","the","","","but","it","squashes","it","to","ensure","that","its","length","is","between","","and","","","key","feature","of","","","is","that","they","preserve","detailed","information","about","the","","location","and","its","","throughout","the","","","","if","I","rotate","the","image","","notice","that","the","activation","vectors","also","change","","","","is","called","","","a","regular","convolutional","neural","","there","are","generally","several","pooling","","and","unfortunately","these","pooling","layers","tend","to","lose","","such","as","the","precise","location","and","pose","of","the","","","really","not","a","big","deal","if","you","just","want","to","classify","the","whole","","but","it","makes","it","challenging","to","perform","accurate","image","segmentation","or","object","detection","","require","precise","","","location","and","","","fact","that","capsules","are","","makes","them","very","promising","for","these","","","so","now","","see","how","capsule","networks","can","handle","objects","that","are","composed","of","a","hierarchy","of","","","","consider","a","boat","centered","at","position","","","","and","","","","and","rotated","by","","","","boat","is","","","composed","of","","","this","case","one","rectangle","and","one","","","this","is","how","","would","be","","","","we","want","to","do","the","","we","want","","","so","we","want","to","go","from","the","image","to","this","whole","hierarchy","of","parts","with","their","instantiation","","","we","could","also","draw","a","","","","using","the","same","","a","rectangle","","a","","","this","","organized","in","a","different","","","the","trick","will","be","to","try","to","","","go","from","this","image","containing","a","rectangle","","a","","and","figure","","not","only","that","the","rectangle","and","triangle","are","at","this","location","and","this","","but","also","that","they","are","part","of","a","","not","a","","","","","","figure","out","how","it","would","do","","","first","step","we","have","already","","we","run","a","couple","","convolutional","","we","reshape","the","output","to","get","","and","we","squash","","","gives","us","the","output","of","the","primary","","","got","the","first","layer","","","next","step","is","where","most","of","the","magic","and","complexity","of","capsule","networks","takes","","","capsule","in","the","first","layer","tries","to","predict","the","output","of","every","capsule","in","the","next","","","might","want","to","pause","to","think","about","what","this","","","","capsules","","","","the","first","layer","","to","predict","what","the","second","layer","capsules","will","","","","","consider","the","capsule","that","detected","the","","","call","it","the","","","","suppose","that","there","are","","two","capsules","in","the","next","","the","","","and","the","","","","the","","","detected","a","rectangle","rotated","by","","","it","predicts","that","the","","","will","detect","a","house","rotated","by","","","that","makes","","and","the","","","will","detect","a","boat","rotated","by","","","as","","","what","would","be","consistent","with","the","orientation","of","the","","","to","make","this","","what","the","","","does","is","","simply","computes","the","dot","product","of","a","transformation","matrix","","","with","its","own","activation","vector","","","","","","","the","network","will","gradually","learn","a","transformation","matrix","for","each","pair","of","capsules","in","the","first","and","second","","","other","","it","will","learn","all","the","","","","for","example","the","angle","between","the","wall","and","the","roof","of","a","","and","so","","","","see","what","the","","","","","","","","a","bit","more","","given","the","rotation","angle","of","the","","it","predicts","that","the","","","will","detect","an","upside-down","","and","","the","","","will","detect","","rotated","by","","","","are","the","positions","that","would","be","consistent","with","the","","","rotation","angle","of","the","","","","","a","bunch","of","predicted","","what","do","we","do","with","","","","you","can","","the","","","and","the","","","strongly","agree","on","what","the","","","will","","","other","","they","agree","that","a","boat","positioned","in","this","way","would","explain","their","own","positions","and","","","they","totally","disagree","on","what","the","","","will","","","it","makes","sense","to","assume","that","the","rectangle","and","triangle","are","part","of","a","","not","a","","","that","we","know","that","the","rectangle","and","triangle","are","part","of","","","the","outputs","of","the","rectangle","capsule","and","the","triangle","capsule","really","concern","only","the","boat","","","","no","need","to","send","these","outputs","to","any","other","","this","would","just","add","","","should","be","sent","only","to","the","boat","","","is","called","routing","by","","","are","several","","","since","capsule","outputs","are","only","routed","to","the","appropriate","capsule","in","the","next","","these","capsules","will","get","a","cleaner","input","signal","","more","","accurately","determine","the","pose","of","the","","","by","looking","at","the","paths","of","the","","you","can","easily","navigate","the","hierarchy","of","","and","know","exactly","which","part","belongs","to","which","","","the","rectangle","","belongs","to","the","","or","the","triangle","belongs","to","the","","and","so","","","routing","","by","agreement","helps","parse","crowded","scenes","with","overlapping","objects","","will","see","this","in","a","few","","","","","look","at","how","","by","","is","implemented","in","","","","I","have","represented","the","various","poses","of","the","","as","predicted","by","the","","","","","","one","of","these","circles","may","represent","what","the","","","thinks","about","the","most","likely","pose","of","the","","and","another","circle","may","represent","what","the","","","","and","if","we","suppose","that","there","are","many","other","","","","then","we","might","get","a","cloud","of","prediction","","for","the","boat","","like","","","this","","there","are","two","","","one","represents","the","rotation","","and","the","other","represents","the","size","of","the","","","I","mentioned","","","parameters","may","capture","many","different","kinds","of","visual","","like","","","and","so","","","","","precise","","","the","first","thing","we","","is","we","compute","the","mean","of","","these","","","gives","us","this","","","next","step","is","to","measure","the","distance","between","each","predicted","vector","and","the","","","","I","will","use","here","the","","distance","","but","capsule","networks","actually","use","the","scalar","","","we","want","to","measure","how","much","each","predicted","vector","agrees","with","the","mean","predicted","","","","this","agreement","","we","can","update","the","","of","every","predicted","vector","","","that","the","predicted","vectors","that","are","far","from","the","mean","now","have","a","very","small","","and","the","ones","closest","to","the","mean","have","a","much","stronger","","","","","represented","them","in","","","we","can","","compute","the","mean","once","again","","I","should","","the","weighted","","and","","notice","that","it","moves","slightly","towards","the","","towards","the","center","of","the","","","","we","can","once","again","update","the","","","now","most","of","the","vectors","within","the","cluster","have","turned","","","","we","can","update","the","","","we","can","repeat","this","process","a","few","","","practice","","","","to","","iterations","are","generally","","","might","remind","","I","","of","the","k-means","clustering","algorithm","if","you","know","","","so","this","is","how","we","find","clusters","of","","","","see","how","the","whole","algorithm","works","in","a","bit","more","","","for","every","predicted","","we","start","by","setting","a","raw","routing","weight","","equal","to","","","we","apply","the","softmax","function","","","to","these","raw","","for","each","primary","","","gives","the","actual","routing","weights","for","each","predicted","","in","this","example","0.5","","","","","we","compute","a","weighted","sum","of","the","","for","each","capsule","in","the","next","","","might","give","vectors","longer","than","","so","as","usual","we","apply","the","squash","","","","","now","have","the","actual","outputs","of","the","","","and","","","","this","is","not","the","final","","","just","the","end","of","the","first","","the","first","","","we","can","see","which","predictions","were","most","","","","the","","","made","a","great","prediction","for","the","","","","","really","matches","it","pretty","","","is","estimated","by","computing","the","scalar","product","of","the","predicted","output","vector","","","","","and","the","actual","product","vector","","","","scalar","product","is","simply","added","to","the","predicted","","raw","routing","","","","the","weight","of","this","particular","predicted","output","is","","","there","is","a","strong","","","scalar","product","is","","","","","so","good","predictions","will","have","a","higher","","","the","other","","the","","","made","a","pretty","bad","prediction","for","the","","","","so","the","scalar","product","in","this","case","will","be","quite","","and","the","raw","routing","","of","","predicted","vector","will","not","grow","","","we","update","the","routing","weights","by","computing","the","softmax","of","the","raw","","once","","","as","you","can","","the","","","predicted","vector","for","the","","","now","has","a","weight","of","","while","","predicted","vector","for","the","","","dropped","down","to","","","most","of","its","output","is","","not","","","","","","","","","the","house","","","again","we","compute","the","weighted","sum","of","all","the","predicted","","vectors","for","each","capsule","in","the","next","","that","is","the","","","","","","","and","","this","","the","","","gets","so","little","input","that","its","output","is","a","tiny","","","the","other","hand","the","","","gets","so","much","input","that","","","","it","outputs","a","vector","much","longer","than","","","again","we","squash","","","","the","end","of","round","","","as","you","can","","in","just","a","couple","","we","have","already","ruled","out","the","house","and","clearly","chosen","the","","","perhaps","one","or","two","more","","we","can","stop","and","proceed","to","the","next","capsule","layer","in","exactly","the","same","","","as","I","mentioned","","routing","by","agreement","is","really","great","to","handle","crowded","","such","as","the","one","represented","in","this","","image","","","","","","","","you","can","see","","a","","bit","of","","you","can","see","a","house","upside","down","in","the","","","if","this","was","the","","then","there","would","be","no","explanation","for","the","bottom","rectangle","or","the","top","","no","reason","for","them","to","be","where","they","","","best","way","to","interpret","the","image","is","that","there","is","a","house","at","the","top","and","a","boat","at","the","","","routing","by","agreement","will","tend","to","choose","this","","since","it","makes","all","the","capsules","perfectly","","each","of","them","making","perfect","predictions","for","the","capsules","in","the","next","","","ambiguity","is","explained","","","so","what","can","you","do","with","a","capsule","network","now","that","you","know","how","it","","","for","","you","can","create","a","nice","image","classifier","of","","","have","one","capsule","per","class","in","the","top","","layer","and","","almost","all","there","is","to","","","you","need","to","add","is","a","layer","that","computes","the","length","of","the","","","activation","","and","this","gives","you","the","estimated","class","","","could","then","just","train","the","network","by","minimizing","the","","","","","","","classification","neural","","and","you","","","be","","","in","the","paper","they","use","a","margin","loss","that","makes","it","possible","to","detect","multiple","classes","in","the","","","without","going","into","too","much","","this","margin","loss","is","such","that","if","an","object","of","class","","is","present","in","the","","then","the","corresponding","top-level","capsule","should","","","a","vector","whose","","length","is","at","least","","","should","be","","","if","an","object","of","class","","is","not","present","in","the","","then","the","capsule","should","output","a","short","","one","whose","","length","is","shorter","than","","","the","total","loss","is","the","sum","of","losses","for","all","","","the","","they","also","add","a","decoder","network","on","top","of","the","capsule","","","just","","","","","","connected","layers","","","with","a","sigmoid","activation","function","in","the","output","","","","learns","to","reconstruct","the","input","image","by","minimizing","the","squared","difference","between","the","reconstructed","image","and","the","input","","","full","loss","is","","margin","loss","we","discussed","","plus","the","reconstruction","loss","","down","considerably","so","as","to","ensure","that","the","margin","loss","dominates","","","benefit","of","applying","this","reconstruction","loss","is","that","it","forces","the","network","to","preserve","all","the","information","required","to","","","reconstruct","the","","up","to","the","top","layer","of","the","capsule","","its","output","","","constraint","acts","a","bit","like","a","","it","reduces","the","risk","of","overfitting","and","helps","generalize","to","new","","","","","you","know","how","a","capsule","","","and","how","to","train","","","look","a","little","bit","at","some","of","the","figures","in","the","","which","I","find","","","","is","figure","","showing","a","full","capsule","network","for","","","","can","see","","the","first","two","regular","convolutional","","whose","output","is","reshaped","and","squashed","to","get","the","activation","vectors","of","the","primary","","","these","primary","capsules","are","organized","in","a","","by","","","with","32","primary","capsules","in","each","cell","of","this","","and","each","primary","capsule","outputs","an","","","","","this","first","layer","of","capsules","is","fully","connected","to","the","","output","","which","output","","dimensional","","","length","of","these","vectors","is","","","used","to","compute","the","margin","","as","explained","","","this","is","figure","","from","the","","","shows","the","decoder","sitting","on","top","of","the","","","","is","composed","of","","fully","connected","","layers","plus","a","fully","connected","sigmoid","layer","which","outputs","784","numbers","that","correspond","to","the","pixel","intensities","of","the","","reconstructed","image","","is","","28","by","28","pixel","","squared","difference","between","this","reconstructed","image","and","the","input","image","gives","the","reconstruction","","","and","this","is","figure","","from","the","","","","","nice","thing","about","capsule","networks","is","that","the","activation","vectors","are","often","","","","","this","image","shows","the","reconstructions","that","you","get","when","you","gradually","modify","one","of","the","16","dimensions","of","the","top","layer","","","","can","see","that","the","first","dimension","seems","to","represent","","","scale","and","","","fourth","dimension","represents","a","localized","","","","","","","","","","","","","","","","","","","","fifth","represents","the","width","of","the","digit","plus","a","slight","translation","to","get","the","exact","","","as","you","can","","","rather","clear","what","most","of","these","parameters","","","to","","","summarize","the","pros","and","","","networks","have","reached","","accuracy","on","","","","","","","","they","got","a","bit","over","","","","which","is","far","from","","","but","","","what","was","first","obtained","with","other","techniques","before","","","years","of","efforts","were","put","into","","so","","still","a","good","","","networks","require","less","training","","","offer","","which","means","that","position","and","pose","information","are","","","this","is","very","promising","for","image","segmentation","and","object","","","routing","by","agreement","algorithm","is","great","for","crowded","","","routing","tree","also","maps","the","hierarchy","of","objects","","so","every","part","is","","to","a","","","","rather","robust","to","","translations","and","other","","","","activation","vectors","","are","","","","","","","","","so","","bet","against","","","there","are","a","few","","","as","I","mentioned","the","results","are","not","yet","","on","","","even","though","","a","good","","","","still","unclear","whether","capsule","networks","can","scale","to","larger","","such","as","the","","","","","","","","will","the","accuracy","","","networks","are","also","quite","slow","to","","in","large","part","because","of","the","routing","by","agreement","algorithm","which","has","an","inner","","as","you","saw","","","there","is","only","one","capsule","of","any","","type","in","a","given","","so","","impossible","for","a","capsule","network","to","detect","two","objects","of","the","same","type","if","they","are","too","close","to","one","","","is","called","","and","","been","observed","in","human","vision","as","","so","","probably","not","a","","","I","highly","recommend","you","take","a","look","at","the","code","of","a","","","","such","as","the","ones","listed","here","","leave","the","links","in","the","video","description","","","you","take","your","","you","should","have","no","problem","understanding","everything","the","code","is","","","main","difficulty","in","implementing","","","is","that","it","contains","an","inner","loop","for","the","routing","by","agreement","","","loops","in","","and","","can","be","a","little","bit","trickier","than","in","","","but","it","can","be","","","","you","","have","a","particular","","then","I","would","say","that","the","","","code","is","","the","easiest","to","","","","all","I","","I","hope","you","enjoyed","this","","","you","","please","thumbs","","","","","","","","","my","first","real","YouTube","","and","if","people","find","it","","","make","some","","","you","want","to","learn","more","about","","","","","","","","","you","may","want","to","read","my","","book","","","","","with","","and","","","covers","a","ton","of","","with","many","code","examples","that","you","will","find","on","my","github","","so","","leave","the","links","in","the","video","","","all","for","","have","fun","and","see","you","next",""],"2Kawrd5szHE":["","","","","and","today","","going","to","show","you","how","to","implement","a","capsule","network","using","","","","my","previous","","I","presented","the","key","ideas","behind","capsule","","a","recently","published","neural","net","","","you","","seen","this","","I","encourage","you","to","do","so","","today","I","will","focus","on","the","","","","I","wrote","a","","notebook","containing","all","the","code","and","detailed","","and","","published","it","on","my","github","account","","always","","put","all","the","links","in","the","video","description","","so","I","encourage","you","to","clone","","and","play","with","","","it","reaches","over","","","","accuracy","on","the","test","","which","is","pretty","","considering","","a","shallow","network","with","just","two","capsule","layers","and","a","total","of","about","","","","a","lot","of","code","in","","","this","","so","I","","go","through","every","single","line","in","this","","but","","explain","the","main","difficulties","I","came","","and","hopefully","this","will","be","useful","to","you","for","other","","","","not","just","","","","","build","","","","we","need","to","feed","the","input","images","to","the","","","","our","input","","","implement","it","using","a","simple","","","","","batch","size","is","","so","that","we","can","pass","any","number","of","images","in","each","","in","this","","","","that","we","directly","send","","","","","pixel","","with","a","single","","since","the","images","are","","","images","would","typically","have","3","","for","","green","and","","","","it","for","the","input","","","","build","the","primary","capsule","","","each","digit","in","the","batch","it","will","output","32","","each","containing","a","","","","grid","of","8","dimensional","","","capsules","in","this","particular","map","seem","to","detect","the","start","of","a","line","","","","can","see","that","the","output","vectors","are","long","in","the","locations","where","","a","start","of","a","","","the","orientation","of","the","","vector","gives","the","pose","","in","this","","","represented","the","rotation","","but","the","","","dimensional","orientation","would","also","capture","things","like","the","thickness","of","the","","the","precise","location","of","the","start","of","the","line","relative","to","the","cell","in","the","","","","","and","so","","implementation","is","really","","","we","define","two","regular","convolutional","","","input","of","the","first","layer","is","","the","placeholder","that","will","contain","the","input","images","we","will","feed","at","","","second","layer","takes","the","output","of","the","first","","","","","we","use","the","parameters","specified","in","the","","","second","layer","is","configured","to","output","256","feature","","each","feature","map","contains","","6x6","grid","of","","","want","a","6x6","","grid","of","vectors","","so","we","use","","","","function","to","get","32","","of","","dimensional","","instead","of","256","maps","of","","","","since","the","primary","capsules","will","be","fully","connected","to","the","digit","","we","can","simply","reshape","to","one","long","list","of","","output","vectors","","","","","","","","for","each","instance","in","the","","","the","last","step","is","to","squash","the","vectors","to","ensure","that","their","length","is","always","between","","and","","","","we","use","a","","squash","","","it","","","function","implements","the","squash","equation","given","in","the","","","squashes","every","vector","in","an","","along","the","specified","","by","default","the","last","","","as","you","can","","it","involves","a","division","by","the","norm","of","the","","so","","a","risk","of","a","division","by","zero","if","at","least","one","of","the","vectors","is","a","zero","","","you","could","just","add","a","tiny","epsilon","value","in","the","","and","it","would","fix","the","division","by","zero","","","you","would","still","run","into","another","","","norm","of","a","vector","has","no","defined","gradients","when","the","vector","is","","","if","you","just","use","","","norm","","","","","","in","this","","then","if","at","least","one","of","the","vectors","is","","the","gradients","will","be","undefined","","will","return","","","","","not","","as","a","","when","","","descent","updates","the","weights","of","our","","the","weights","will","end","up","being","undefined","as","","","model","would","effectively","be","","","","","want","","","the","trick","is","to","compute","a","safe","approximation","of","the","","shown","in","the","equation","on","the","","","that's","about","","","all","for","the","primary","","","for","computing","the","norm","","it","was","pretty","","","","to","the","next","layer","where","all","the","complexity","","the","digit","","","are","just","","of","","one","for","each","","0","to","","and","","output","16","dimensional","","","this","particular","","you","can","see","that","the","longest","output","vector","is","the","one","for","digit","","","again","","orientation","in","the","16","dimensional","space","gives","information","about","the","pose","of","this","","its","","its","","its","","its","","and","so","","","the","","note","that","most","of","the","position","information","in","the","first","layer","was","encoded","in","the","location","of","the","active","capsules","in","the","6x6","","","for","","if","I","shift","the","digit","4","slightly","to","the","left","in","the","input","image","","then","different","capsules","in","the","first","layer","get","","","","the","output","of","these","first","layer","capsules","only","","","local","shift","","relative","to","the","position","of","the","capsule","in","the","6x6","","","in","the","second","capsule","","the","full","position","information","is","now","encoded","in","the","orientation","of","the","output","vector","in","16","dimensional","","","now","","see","how","to","implement","this","","","first","step","is","to","compute","the","predicted","output","","","this","second","layer","is","fully","connected","to","the","first","","we","will","compute","one","predicted","output","for","each","pair","of","first","and","second","layer","","","","using","the","output","of","the","first","primary","","we","can","predict","the","output","vector","of","the","first","digit","","","","we","just","use","a","transformation","matrix","","which","will","gradually","be","learned","during","","and","we","multiply","it","by","the","output","of","the","first","layer","","","gives","us","","","","","which","is","the","predicted","output","of","the","first","digit","","based","on","the","output","of","the","first","primary","","","the","primary","capsules","output","","dimensional","","and","the","digit","capsules","output","","dimensional","","the","transformation","matrix","","must","be","a","","","","","","we","try","to","predict","the","output","of","the","second","digit","","still","based","on","the","output","of","the","first","primary","","","that","we","are","using","a","different","transformation","","","","we","do","the","same","for","the","third","","using","","","","","so","on","for","all","the","digit","","","we","move","on","to","the","second","primary","","and","we","use","its","output","to","predict","the","output","of","the","first","digit","","","so","on","for","all","the","digit","","","we","move","on","to","the","third","primary","","we","make","","","","so","","you","get","the","","","are","","primary","capsules","","","","","","","and","","digit","","so","we","end","up","with","","","","","","","predicted","output","","","we","could","just","compute","them","one","by","","but","it","would","be","terribly","","","","see","how","we","can","get","all","the","predicted","output","vectors","in","just","one","","","","","you","know","that","","","","function","lets","you","multiply","two","","but","you","may","not","know","that","you","can","also","use","it","to","multiply","many","matrices","in","one","","","will","be","incredibly","","especially","if","you","are","using","a","GPU","","because","it","will","perform","all","the","matrix","multiplications","in","parallel","in","many","different","GPU","","","","how","it","","","","","","","","","and","","","","","","","are","all","","","can","put","these","matrices","in","two","","each","with","","rows","and","","","for","","","we","have","","dimensions","for","this","2x3","grid","of","","and","each","matrix","is","","so","these","arrays","are","","","","","","dimensional","","","you","pass","these","","these","","","","","","it","will","perform","an","","matrix","","so","the","result","will","be","this","","dimensional","array","containing","","","","","","B","multiplied","by","","","and","so","","","","use","this","to","compute","all","the","predicted","output","","","can","create","","first","","array","containing","all","the","transformation","","","one","row","per","primary","","and","one","column","per","digit","","","second","array","must","contain","the","output","vectors","of","each","primary","","","we","just","pass","these","two","arrays","to","the","","","and","it","gives","us","the","predicted","output","vectors","for","all","the","pairs","of","primary","and","digit","","","we","need","to","predict","the","outputs","of","all","","capsules","for","each","primary","","this","array","must","contain","","copies","of","the","primary","","","","will","use","the","tile","function","to","replicate","the","first","column","of","output","","","","","","one","additional","","","want","to","make","these","predictions","for","all","the","instances","in","the","","not","just","one","","","","an","additional","dimension","for","the","batch","","","turns","out","that","the","primary","output","vectors","were","already","computed","for","every","single","","so","the","second","array","is","","","already","has","this","","","we","need","to","replicate","the","","array","containing","all","the","transformation","","so","that","we","end","up","with","one","copy","per","instance","in","the","","","if","you","understand","","then","the","code","should","be","pretty","","","we","create","a","variable","containing","all","the","transformation","","","has","","one","row","per","primary","","one","column","per","digit","","","and","it","contains","16","by","8","","","","","and","we","add","another","dimension","at","the","beginning","of","size","one","","to","make","it","easy","to","tile","this","array","for","each","instance","in","the","","","","variable","is","initialized","","using","a","normal","distribution","of","standard","deviation","0.01","","a","","","you","can","","","that's","about","","","","this","","","we","want","to","tile","this","array","for","each","","so","first","we","need","to","know","the","batch","","","","actually","know","it","at","graph","construction","","it","will","only","be","known","when","we","run","the","","","we","can","use","","","","","it","creates","a","tensor","that","","know","the","shape","at","","","and","we","grab","its","first","","which","is","the","batch","","","we","simply","tile","our","big","W","","along","the","first","","to","get","one","copy","per","","","recall","that","the","output","of","the","primary","capsules","was","a","","dimensional","","the","first","dimension","is","the","batch","","that","we","will","know","at","","then","","one","row","per","","and","each","capsule","has","","","","we","need","to","reshape","this","array","a","bit","to","get","the","shape","that","we","are","looking","","","to","do","the","big","","","","","we","add","an","extra","dimension","at","the","","using","","","","","","","vectors","are","now","represented","as","column","","instead","of","","dimensional","","","of","these","is","a","","column","vector","","","is","a","","a","","","with","a","single","","","we","add","another","","for","the","digit","","","we","replicate","all","the","output","vectors","","times","across","this","new","","once","per","digit","","","","we","just","use","","","to","multiply","the","transformation","matrices","with","the","primary","","output","","and","we","get","all","the","digit","","predicted","outputs","for","each","pair","of","primary","and","digit","","and","for","each","instance","in","the","","","one","","","","the","end","of","the","first","step","for","computing","the","digit","","","we","now","have","a","bunch","of","predicted","output","","","second","step","is","the","routing","by","agreement","","","","we","set","all","the","","routing","weights","to","","","","we","just","use","","","","","one","weight","for","each","pair","of","primary","and","","","","for","each","","","last","two","dimensions","here","are","equal","to","","they","will","be","useful","in","a","","","we","compute","the","softmax","of","each","primary","","10","raw","routing","","","","softmax","happens","along","this","","","we","compute","the","weighted","sum","of","all","the","predicted","output","","for","each","digit","","using","","the","routing","weights","","","","","","","","","","","","","","","","","","","","and","the","predicted","vectors","","is","","","","not","a","matrix","","then","just","compute","the","sum","over","the","primary","capsule","","","the","two","dimensions","we","added","earlier","for","the","routing","weights","are","useful","in","the","multiplication","","so","that","the","two","arrays","have","the","same","number","of","","the","same","","","","have","the","exact","same","","but","they","have","compatible","shapes","so","","will","perform","","","if","you","don't","know","what","broadcasting","","this","should","make","it","","","multiplying","two","","but","one","of","them","just","has","one","","so","","will","act","as","if","this","row","were","repeated","the","appropriate","number","of","","","could","achieve","the","same","thing","using","","as","we","did","","but","this","is","more","","","","may","wonder","why","we","","use","broadcasting","","but","the","reason","is","it","does","not","work","for","matrix","","","","doing","","","","","back","to","the","digit","","","computed","a","weighted","sum","of","the","predicted","","for","each","digit","","and","we","just","run","the","squash","","and","we","get","the","outputs","of","the","digit","","","","","","this","is","just","the","end","of","round","","of","the","routing","by","agreement","","","on","to","round","","","","we","need","to","measure","how","good","each","prediction","","and","use","this","to","update","the","routing","","","","look","at","the","predictions","that","we","made","using","this","primary","","","","","for","","the","prediction","for","digit","","is","","","this","is","measured","using","the","scalar","product","of","the","predicted","output","vector","and","the","actual","output","","","two","vectors","are","actually","represented","as","column","","meaning","a","matrix","with","a","single","","","to","compute","the","scalar","","we","must","transpose","the","predicted","column","vector","","","","","to","get","a","row","","and","multiply","","this","row","vector","and","the","actual","output","vector","","","which","is","a","column","","","will","get","a","","","","matrix","containing","the","scalar","product","of","the","","","","of","course","we","need","to","do","this","for","each","predicted","","so","once","","we","can","use","the","","","function","to","perform","all","the","matrix","multiplications","in","just","one","","","we","must","use","the","","function","to","get","one","copy","of","the","actual","output","vectors","","","for","each","primary","","","we","use","","","telling","it","to","transpose","each","","matrix","in","the","first","","on","the","","and","","","","we","get","all","the","scalar","products","at","","","","we","have","a","measure","of","","between","each","predicted","vector","and","the","actual","output","","","can","then","add","these","scalar","products","to","the","raw","","using","a","simple","","","the","rest","of","round","2","is","exactly","the","same","as","the","end","of","round","","","code","is","really","","except","","now","using","the","raw","routing","","of","round","","","compute","their","softmax","to","get","the","actual","routing","weights","for","round","","then","we","compute","the","weighted","sum","of","all","the","predicted","vectors","for","each","digit","","and","finally","we","squash","the","","","now","we","have","the","new","digit","capsule","","and","","","round","","","could","do","a","few","more","rounds","exactly","like","this","","but","","stop","","and","use","the","current","output","vectors","at","the","end","of","round","2","as","the","output","of","the","digit","","","you","probably","noticed","that","I","implemented","the","routing","","loop","without","an","actual","","","a","bit","like","computing","the","sum","of","squares","from","1","to","","with","this","","","","this","will","build","a","very","big","","","","","it","","","can","think","of","it","as","an","unrolled","","","cleaner","","way","to","do","this","would","be","to","write","a","for","loop","in","","like","","much","","","","important","to","understand","that","the","resulting","","","will","be","absolutely","identical","to","the","one","produced","by","the","previous","","","","doing","here","is","constructing","a","","and","","will","not","even","know","that","we","","","loop","to","build","","","","this","works","","","just","that","you","end","up","with","a","very","large","","","you","can","think","of","this","loop","as","a","static","","that","only","runs","at","graph","construction","","","you","want","a","dynamic","","one","that","","itself","will","","then","you","must","use","","","","","function","like","","","","","function","takes","","","the","first","one","is","a","function","that","must","return","a","tensor","that","will","determine","whether","the","loop","should","go","on","or","","at","each","","","second","parameter","is","also","a","function","that","must","build","the","body","of","the","","","that","will","also","be","evaluated","at","each","","","","third","parameter","contains","a","list","of","tensors","that","will","be","sent","to","both","the","","and","","","","at","the","first","","","the","following","","these","functions","will","receive","the","output","of","the","","","","","you","can","pause","the","video","if","you","need","to","take","a","closer","look","at","this","","","you","get","","you","can","try","modifying","my","","","implementation","to","use","a","dynamic","loop","rather","than","a","static","unrolled","","","from","making","the","code","cleaner","and","the","graph","","using","a","dynamic","loop","allows","you","to","change","the","number","of","iterations","using","the","exact","same","","","if","you","set","the","","","parameter","of","the","","","","if","you","set","it","to","","","will","automatically","swap","the","GPU","memory","to","CPU","memory","","","","to","save","GPU","","","CPU","","is","much","cheaper","and","","this","can","really","be","","","","","","computed","the","output","of","the","digit","","","","the","length","of","each","output","vector","represents","the","probability","that","a","digit","of","that","class","is","present","in","the","","","","compute","these","","","","we","","use","","","","function","because","training","will","explode","if","","a","zero","vector","at","any","","as","I","","","","instead","we","","a","","","","","similar","to","what","we","did","with","the","","","","note","that","the","sum","of","the","probabilities","","necessarily","add","up","to","","because","we","are","not","using","a","","","","","makes","it","possible","to","detect","multiple","different","digits","in","the","same","image","","they","all","have","to","be","different","","","you","can","detect","a","5","and","","","but","you","","","","two","","","","predict","the","most","likely","","","just","","the","","","function","that","gives","","the","index","of","the","highest","","","index","happens","to","be","the","","","the","digit","","","that","we","first","get","a","tensor","that","has","a","couple","extra","dimensions","of","size","","at","the","","so","we","get","rid","of","them","using","the","","","","we","called","","without","specifying","the","axes","to","","it","would","remove","all","dimensions","of","size","","","would","generally","be","","except","if","the","batch","size","was","equal","to","","in","which","","we","would","be","left","with","a","scalar","","rather","than","an","","and","we","","want","","","it's","better","to","specify","the","","","now","we","have","a","capsule","network","that","can","estimate","class","probabilities","and","make","","","can","measure","the","","accuracy","on","the","batch","by","simply","comparing","the","predictions","and","the","","","this","case","the","prediction","for","the","last","digit","in","the","batch","is","","","","instead","of","","","we","get","80%","","","","code","is","really","","we","just","use","the","","function","to","compare","the","labels","and","the","predictions","","","and","this","gives","us","an","array","of","","","so","we","cast","these","","","","which","gives","us","a","bunch","of","","","bad","","and","","","good","","and","we","compute","the","mean","to","get","the","","","","labels","","are","just","a","regular","","","","","","","we","have","a","full","","able","to","make","","","","look","at","the","training","","","diagram","is","about","","get","pretty","crowded","so","","remove","the","accuracy","for","","","","","we","want","to","compute","the","margin","","","given","by","this","","","the","","I","made","a","mistake","in","my","first","","I","squared","","norms","instead","of","","the","max","","","about","","","here","is","the","correct","","","it","is","pretty","","so","I","","go","through","it","in","","","only","trick","is","to","understand","how","you","can","easily","compute","all","the","","","","a","given","","","is","equal","to","1","if","a","digit","of","","","is","present","in","the","","otherwise","","equal","to","","you","can","get","all","the","","values","for","each","instance","by","simply","converting","the","labels","to","","","","","","if","an","","label","is","","then","for","this","","T","will","contain","a","10","dimensional","vector","full","of","zeros","except","for","a","1","","index","","","","we","want","to","compute","the","reconstruction","","","","we","must","send","the","outputs","of","the","digit","capsules","to","a","decoder","that","will","try","to","use","them","to","reconstruct","the","input","","","decoder","is","just","a","regular","","neural","net","composed","of","","fully","connected","","","really","simple","","so","you","can","pause","the","video","if","you","want","","take","a","close","look","at","","","outputs","an","array","containing","784","values","from","0","to","","for","each","","representing","the","pixel","intensities","of","","","","pixel","","","","","we","have","our","reconstructed","","","can","now","compute","the","reconstruction","","","is","just","the","squared","difference","between","the","input","images","and","","","","the","input","images","are","","","","","","we","first","reshape","them","to","one","dimension","per","instance","with","784","values","","","we","compute","the","squared","","","we","can","compute","the","final","","","just","the","sum","of","the","margin","loss","and","the","reconstruction","","","down","to","let","the","margin","loss","dominate","","training","","","","","","","","","","","","","","paper","mentions","they","","","","implementation","of","the","","","using","the","default","","so","","do","","","create","the","","and","call","","","method","to","get","the","training","operation","that","will","tweak","the","model","parameters","to","minimize","the","","","almost","","but","","one","last","detail","I","","mention","in","the","first","","","paper","indicates","that","the","outputs","of","the","digit","capsules","should","all","be","masked","out","except","for","the","ones","corresponding","to","the","target","","digit","","","","","","","","outputs","directly","to","the","","we","want","to","apply","a","mask","","like","","","mask","will","have","the","same","shape","as","the","digit","capsules","output","","and","it","will","be","equal","to","","everywhere","except","for","","at","the","location","of","the","target","","","multiplying","the","digit","","output","","the","","we","get","the","input","to","the","","","","one","","","picture","is","good","for","","but","at","test","","we","","have","the","","","","we","will","","the","output","vectors","using","the","predicted","classes","rather","than","","like","","","we","could","build","a","different","graph","for","training","and","for","","but","it","","be","very","","","","","build","a","","","","will","add","a","boolean","","placeholder","called","","","labels","","it","is","","then","we","use","the","","","build","the","","","","","","","","","","","","","","","","","","","","","","","","","","which","will","default","to","","so","that","we","only","need","to","set","it","during","","","then","we","define","the","reconstruction","targets","using","","","","","takes","","","the","first","one","is","a","tensor","representing","the","","in","this","case","simply","the","","","","","","second","parameter","is","a","function","that","returns","the","tensor","to","use","if","the","condition","is","","and","the","third","parameter","is","a","function","that","returns","the","tensor","to","use","","the","condition","","","","to","build","the","","we","simply","use","the","","","","","actually","one","slight","problem","with","this","","and","to","explain","","I","need","to","step","back","for","a","second","and","talk","about","how","","evaluates","","we","built","this","","these","are","all","tensorflow","","and","we","want","to","evaluate","the","output","of","operation","","","first","thing","","will","do","is","resolve","the","","it","will","find","","","","","","","","","","","","","","","","","","","","","","D","and","","","it","will","run","any","","","","that","has","no","","","are","called","root","","","this","case","","F","","","is","","","C","and","D","now","have","all","the","inputs","they","","so","they","can","be","","and","","will","actually","try","to","run","them","in","","","D","finishes","","","still","has","one","","","","input","so","it","","run","","","as","soon","as","C","is","","","can","be","","","once","","","the","","method","returns","the","","","","","","can","actually","evaluate","multiple","operations","at","","for","example","","and","","and","the","process","is","really","the","","","finds","all","the","","runs","the","root","","and","","you","","goes","upward","running","every","operation","whose","inputs","are","","","once","it's","got","both","the","values","for","","and","","it","returns","the","","","","apply","this","to","our","reconstruction","","","tensor","is","the","output","of","the","","","which","has","","","","","","a","function","that","returns","","and","a","function","that","returns","","","","when","we","evaluate","the","","","or","any","tensor","that","depends","on","","such","as","the","final","","which","depends","on","the","reconstruction","","which","eventually","depends","on","the","","","","what","happens","","as","","","starts","by","resolving","the","","","finds","all","three","bottom","","","it","evaluates","them","","","","","may","finish","","","these","operations","are","run","in","","","no","way","to","know","in","which","order","they","will","","","you","","","may","finish","","","finally","","","","","","suppose","it","evaluates","to","","","the","","","has","all","the","inputs","it","","so","it","can","","","be","","","of","course","it","does","the","right","","since","","","","is","","it","returns","the","value","of","","","is","","","notice","that","","","was","evaluated","for","","","not","using","its","","","not","a","big","deal","since","during","training","we","need","to","evaluate","the","margin","","which","depends","on","the","estimated","class","","which","is","just","one","step","away","from","the","","so","computing","the","predictions","","add","much","","","","","a","bit","","","suppose","","","","evaluates","to","","","","the","","","will","do","the","right","","it","will","output","the","value","of","","","","this","","","we","evaluated","y","for","","","just","a","","so","it","","add","much","computation","","but","it","means","that","we","must","feed","","even","if","","","","is","","","actually","we","can","just","pass","an","empty","","","that's","","","it","will","","but","it's","kind","of","","","this","unfortunate","situation","is","due","to","the","fact","that","the","functions","we","passed","to","the","","","do","not","actually","create","any","","they","just","return","tensors","that","were","created","outside","of","these","","","","you","build","tensors","within","these","","then","","will","do","what","you","","","will","stitch","together","the","partial","graphs","created","","these","functions","into","a","graph","that","will","properly","handle","the","","","you","might","be","able","to","modify","my","code","and","fix","this","","I","","but","I","ended","up","with","pretty","convoluted","","so","I","decided","to","stick","with","this","","I","hope","it","","keep","you","awake","at","","","","actually","","","the","full","picture","","","construction","phase","is","","our","graph","is","","now","on","to","the","execution","","","run","this","","","","look","at","the","training","","","really","really","completely","","","create","a","","if","a","","","file","","load","","or","else","initialize","all","the","","","run","the","main","training","","for","a","number","of","","and","for","each","","run","enough","iterations","to","go","through","the","full","training","","","inside","this","","we","simply","evaluate","the","training","","and","the","","on","the","next","","","feed","the","images","and","the","labels","of","the","current","training","","and","we","set","","","","to","","","pretty","much","all","there","is","to","","","the","","","I","also","added","a","simple","implementation","of","early","","and","I","print","out","the","","plus","I","evaluate","the","model","on","the","validation","set","at","the","end","of","each","","","the","most","important","part","is","","","","I","just","run","a","few","","images","through","the","network","and","get","the","predictions","and","","","you","can","","the","predictions","are","all","","and","the","reconstructions","are","pretty","","they're","pretty","close","to","the","original","","except","that","","slightly","","","as","you","can","","","the","","","really","nothing","special","about","","I","take","a","few","","I","start","a","session","and","load","the","","then","I","just","evaluate","","predictions","","","and","the","","output","","I","also","get","the","","","so","I","can","tweak","them","","","ugliness","I","","earlier","is","right","","","I'm","forced","to","pass","an","empty","","","value","will","be","ignored","","","","the","code","tweaks","the","output","vectors","and","passes","the","","to","the","","","we","can","see","what","each","of","","the","","dimensions","represent","","the","digit","","output","","","","this","image","shows","the","reconstructions","we","get","by","tweaking","the","first","parameter","","","notebook","produces","one","such","image","for","each","of","the","","","","as","you","can","","in","the","","second","and","last","","we","see","that","the","digits","become","thinner","and","","","going","to","the","","or","thicker","and","thicker","to","the","","so","it","holds","information","about","","","in","the","middle","","you","can","see","that","the","bottom","part","of","the","number","","gets","lifted","towards","the","","so","probably","that's","what","this","parameter","does","for","this","","","I","","","like","to","thank","everyone","who","shared","and","commented","on","my","first","","I","really","had","no","idea","it","would","receive","such","an","enthusiastic","","and","","very","very","grateful","to","all","of","","it","definitely","motivates","me","to","","more","","","you","want","to","learn","more","about","","","and","support","this","","check","out","my","","book","","","","","with","","and","","","leave","the","links","in","the","video","","","you","speak","","","actually","a","German","translation","coming","up","for","","","if","you","speak","","the","translation","is","already","","","was","split","in","two","books","but","","really","the","same","","","","all","I","","for","","I","hope","you","enjoyed","this","video","and","that","you","learned","a","thing","or","two","about","","and","capsule","","","you","","","","","","","and","click","on","the","bell","icon","next","to","the","subscribe","","to","receive","notifications","when","I","upload","new","","","you","next",""]},"is_autogen_unique":{"pPN8d0E3900":[2,2,2,2,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,0,0,2,0,0,0,2,2,2,2,0,0,0,2,2,0,2,0,0,0,0,2,2,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,2,2,2,0,0,2,2,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,2,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,2,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,2,2,0,0,0,2,0,0,2,2,2,2,0,0,0,0,0,2,0,0,-1,0,0,2,0,0,0,0,0,2,2,0,0,2,2,2,0,0,0,0,0,0,0,0,-1,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,2,0,0,0,0,0,0,0,0,2,0,0,2,0,2,0,0,2,2,0,0,0,2,0,0,2,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,2,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,2,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,1,1,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,2,0,2,2,2,0,0,0,2,2,2,0,0,1,1,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,-1,0,0,2,2,2,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,2,2,0,0,0,2,0,0,2,0,2,2,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,2,0,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,1,0,2,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,-1,0,0,0,0,0,2,0,2,2,0,0,2,2,2,0,2,2,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,2,0,0,2,2,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,2,2,0,0,-1,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,2,2,2,2,2,2,2,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,2,0,-1,0,2,2,0,0,2,0,0,2,2,2,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,2,2,2,2,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,2,0,2,2,0,0,2,2,0,0,0,0,0,2,2,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,-1,0,1,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,1,0,0,0,2,0,0,0,0,0,0,2,0,0,2,2,0,1,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,2,2,0,0,0,2,0,2,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,0,2,0,2,2,0,2,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,2,2,2,0,2,2,0,0,0,0,2,0,0,0,0,0,0,-1,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,-1,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,2,2,0,0,-1,0,0,0,0,0,2,0,0,2,0,0,2,0,-1,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,2,2,2,0,2,0,0,0,2,2,0,0,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,1,1,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,2,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,2,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,2,0,0,2,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,0,2,0,2,2,0,0,0,0,2,2,0,0,0,0,0,2,0,2,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,-1,0,1,1,1,1,1,1,1,1,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,2,2,2,2,2,2,0,-1,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,-1,0,2,2,2,2,2,2,2,0,0,0,2,0,1,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,1,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,2,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,1,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,1,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,2,2,2,2,0,0,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,-1,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,2,0,0,0,0,0,0,2,2,2,0,0,1,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,2,0,2,2,0,0,0,0,0,1,1,0,0,0,0,0,2,0,0,2,2,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,2,0,-1,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,1,1,0,0,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,2,2,0,0,0,2,0,0,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,2,0,2,0,0,0,2,2,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,2,2,2,0,0,0,2,0,0,0,2,2,2,0,0,-1,0,2,2,2,2,2,2,2,2,0,2,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,2,2,2,2,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,-1,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,2,0,0,0,0,0,0,2,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,0,2,0,0,0,2,0,0,0,0,0,0,2,2,0,0,1,0,0,0,2,2,2,0,0,2,0,0,0,0,0,2,2,0,2,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,0,2,2,2,2,0,2,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,2],"2Kawrd5szHE":[2,2,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,2,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,1,1,0,2,0,0,2,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,2,0,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,2,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,2,2,2,0,2,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,2,0,0,2,2,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,2,0,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,-1,0,0,0,2,2,0,0,0,1,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,0,2,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,2,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,2,0,0,0,2,0,2,2,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,1,1,1,1,1,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,2,2,2,2,0,2,0,0,2,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,2,2,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,2,0,2,0,0,0,2,0,0,2,0,2,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,2,0,2,0,2,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,2,0,0,0,0,0,2,0,2,0,0,0,2,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,0,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,2,2,2,0,2,0,0,0,2,2,0,2,0,0,2,2,2,2,2,2,0,2,0,2,0,0,0,0,0,2,2,2,2,2,2,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,0,2,2,0,0,0,0,0,0,2,0,0,2,0,0,2,2,0,2,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,2,2,2,2,2,0,2,2,0,0,0,2,0,2,2,2,2,2,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,2,2,2,2,2,0,0,0,2,2,0,0,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,-1,0,2,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,2,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,1,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,2,0,0,2,2,2,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,2,2,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,2,0,2,0,0,0,2,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,0,2,2,2,2,2,2,0,0,0,0,0,0,2,0,0,2,0,2,2,0,0,0,0,-1,0,0,1,1,0,0,2,0,2,2,0,0,0,2,2,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,2,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,-1,0,0,0,2,2,2,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,0,-1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,2,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,2,2,2,0,0,0,0,2,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,2,0,0,1,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,0,0,2,2,0,0,0,0,0,1,0,0,0,0,2,0,0,2,0,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,2,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,2,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,0,0,2,2,2,2,0,2,2,0,0,0,0,0,0,0,2,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,2,0,0,2,0,0,0,0,2,2,2,2,0,0,2,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,2,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,2,2,0,2,0,0,0,0,2,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,0,2,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,2,2,2,0,0,2,0,2,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,2,2,2,0,2,2,2,0,0,0,0,2,2,0,2,0,2,2,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,0,0,0,2,2,2,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,0,0,2,0,0,0,2,0,0,2,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,2,2,2,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,0,2,0,2,2,0,2,0,0,0,0,0,0,0,0,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,2,2,0,2,0,0,0,0,0,0,0,2,0,0,-1,0,0,0,1,0,0,2,2,0,2,2,0,0,0,0,2,2,0,0,0,2,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,2,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,2,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,2,2,2,0,2,2,2,2,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,-1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,0,0,2,2,0,0,0,2,0,2,0,2,2,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,2,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,2,0,2,0,2,0,0,0,0,0,0,2,2,2,0,2,2,0,0,0,0,2,0,0,0,2,0,2,0,0,2,2,2,0,0,2,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,2,2,2,2,0,0,2,2,2,0,0,0,0,1,0,0,2,2,0,2,0,0,2,0,0,0,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,2,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,0,0,0,0,2,2,2,0,0,0,2,2,0,0,0,2,2,0,0,-1,0,1,1,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,2,0,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,2,2,2,0,0,0,2,0,2,2,0,0,0,0,0,2,2,0,0,2,2,0,2,2,0,2,0,0,0,2,2,2,2,2,0,0,0,0,0,0,2,0,0,2,0,2,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,2,0,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,2,0,0,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,2,0,2,2,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,2,2,0,0,2,2,0,2,2,2,2,2,0,0,0,0,2,2,0,2,2,0,0,0,0,0,2,0,0,0,1,1,0,2,2,0,0,0,0,0,0,2,0,2,2,2,0,2,0,0,0,0,0,2,2,0,2,2,0,0,2,2,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,2,2,2,2,0,0,2,2,0,2,2,2,0,0,2,2,2,0,2,2,0,0,0,0,2,0,0,0,0,0,0,2,2,2,0,2,2,0,0,0,0,2,2,0,0,2,0,0,2,0,0,0,2,0,0,0,0,0,0,0,2,0,0,2,2,2,0,2,2,0,0,0,0,0,0,0,2,2,0,2,2,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,2,2,2,0,2,2,0,0,0,2,2,0,0,0,2,0,0,0,2,0,0,0,0,0,2,2,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,2,0,0,2,0,0,2,2,0,2,0,2,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,2,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,2,2,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,2,0,2,2,0,0,0,2,2,0,2,2,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,-1,0,1,1,0,0,2,0,2,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,2,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,-1,0,2,0,0,2,0,0,2,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,-1,0,2,2,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,2,0,2,2,2,2,0,2,0,2,2,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,2]},"is_manual_unique":{"pPN8d0E3900":[2,2,2,2,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,0,0,2,0,0,0,2,2,2,2,0,0,0,2,2,0,2,0,0,0,0,2,2,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,2,2,2,0,0,2,2,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,2,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,2,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,2,2,0,0,0,2,0,0,2,2,2,2,0,0,0,0,0,2,0,0,-1,0,0,2,0,0,0,0,0,2,2,0,0,2,2,2,0,0,0,0,0,0,0,0,-1,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,2,0,0,0,0,0,0,0,0,2,0,0,2,0,2,0,0,2,2,0,0,0,2,0,0,2,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,2,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,2,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,1,1,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,2,0,2,2,2,0,0,0,2,2,2,0,0,1,1,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,-1,0,0,2,2,2,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,2,2,0,0,0,2,0,0,2,0,2,2,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,2,0,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,1,0,2,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,-1,0,0,0,0,0,2,0,2,2,0,0,2,2,2,0,2,2,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,2,0,0,2,2,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,2,2,0,0,-1,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,2,2,2,2,2,2,2,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,2,0,-1,0,2,2,0,0,2,0,0,2,2,2,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,2,2,2,2,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,2,0,2,2,0,0,2,2,0,0,0,0,0,2,2,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,-1,0,1,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,1,0,0,0,2,0,0,0,0,0,0,2,0,0,2,2,0,1,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,2,2,0,0,0,2,0,2,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,0,2,0,2,2,0,2,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,2,2,2,0,2,2,0,0,0,0,2,0,0,0,0,0,0,-1,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,-1,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,2,2,0,0,-1,0,0,0,0,0,2,0,0,2,0,0,2,0,-1,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,2,2,2,0,2,0,0,0,2,2,0,0,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,1,1,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,2,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,2,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,2,0,0,2,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,0,2,0,2,2,0,0,0,0,2,2,0,0,0,0,0,2,0,2,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,-1,0,1,1,1,1,1,1,1,1,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,2,2,2,2,2,2,0,-1,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,1,1,1,0,0,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,-1,0,2,2,2,2,2,2,2,0,0,0,2,0,1,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,1,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,2,2,2,2,0,0,2,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,1,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,1,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,2,2,2,2,0,0,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,-1,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,2,0,0,0,0,0,0,2,2,2,0,0,1,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,2,0,2,2,0,0,0,0,0,1,1,0,0,0,0,0,2,0,0,2,2,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,2,0,-1,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,1,1,0,0,2,2,0,0,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,2,2,0,0,0,2,0,0,2,2,2,2,2,2,2,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,2,0,2,0,0,0,2,2,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,2,2,2,0,0,0,2,0,0,0,2,2,2,0,0,-1,0,2,2,2,2,2,2,2,2,0,2,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,2,2,2,2,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,-1,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,2,0,0,0,0,0,0,2,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,0,2,0,0,0,2,0,0,0,0,0,0,2,2,0,0,1,0,0,0,2,2,2,0,0,2,0,0,0,0,0,2,2,0,2,0,0,2,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,2,2,2,2,2,2,2,2,0,0,0,0,0,0,2,0,2,2,2,2,0,2,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,2],"2Kawrd5szHE":[2,2,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,2,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,1,1,0,2,0,0,2,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,2,0,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,2,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,2,2,2,0,2,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,2,0,2,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,2,0,0,2,2,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,2,0,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,-1,0,0,0,2,2,0,0,0,1,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,0,2,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,2,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,2,0,0,0,2,0,2,2,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,1,1,1,1,1,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,2,2,2,2,0,2,0,0,2,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,2,2,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,2,0,2,0,0,0,2,0,0,2,0,2,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,2,0,2,0,2,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,2,0,0,0,0,0,2,0,2,0,0,0,2,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,0,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,2,2,2,0,2,0,0,0,2,2,0,2,0,0,2,2,2,2,2,2,0,2,0,2,0,0,0,0,0,2,2,2,2,2,2,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,2,2,2,2,2,0,2,2,2,2,2,2,0,0,2,2,0,0,0,0,0,0,2,0,0,2,0,0,2,2,0,2,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,2,2,2,2,2,0,2,2,0,0,0,2,0,2,2,2,2,2,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,2,2,2,2,2,0,0,0,2,2,0,0,2,2,2,0,0,0,0,0,0,0,0,2,2,0,0,-1,0,2,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,2,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,1,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,-1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,2,2,0,0,2,2,0,0,2,2,2,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,2,2,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,2,0,2,0,0,0,2,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,2,0,2,2,2,2,2,2,0,0,0,0,0,0,2,0,0,2,0,2,2,0,0,0,0,-1,0,0,1,1,0,0,2,0,2,2,0,0,0,2,2,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,2,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,-1,0,0,0,2,2,2,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,0,-1,0,0,0,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,1,0,0,0,0,2,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,2,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,2,2,2,0,0,0,0,2,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,0,2,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,2,0,0,1,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,2,0,0,2,2,0,0,0,0,0,1,0,0,0,0,2,0,0,2,0,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,2,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,2,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,0,0,0,2,2,2,2,0,2,2,0,0,0,0,0,0,0,2,2,0,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,2,2,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,2,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,2,0,0,2,0,0,2,0,0,0,0,2,2,2,2,0,0,2,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,2,0,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,1,1,1,0,0,0,2,2,0,2,0,0,0,0,2,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,0,2,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,2,2,2,0,0,2,0,2,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,2,2,2,0,2,2,2,0,0,0,0,2,2,0,2,0,2,2,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,0,0,0,2,2,2,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,0,0,2,0,0,0,2,0,0,2,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,2,2,2,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,0,2,0,2,2,0,2,0,0,0,0,0,0,0,0,2,2,2,0,2,0,0,0,0,2,2,2,2,2,2,0,0,0,0,2,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,0,0,0,0,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,2,2,0,0,0,2,2,0,2,0,0,0,0,0,0,0,2,0,0,-1,0,0,0,1,0,0,2,2,0,2,2,0,0,0,0,2,2,0,0,0,2,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,0,2,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,2,2,0,0,2,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,2,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,2,2,2,0,2,2,2,2,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,-1,0,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,2,2,2,0,0,0,2,2,0,0,0,2,0,2,0,2,2,0,0,2,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,2,0,2,0,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,-1,0,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,2,2,0,0,0,2,0,2,0,2,0,0,0,0,0,0,2,2,2,0,2,2,0,0,0,0,2,0,0,0,2,0,2,0,0,2,2,2,0,0,2,0,0,0,0,0,0,0,0,0,2,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,2,2,2,2,0,0,2,2,2,0,0,0,0,1,0,0,2,2,0,2,0,0,2,0,0,0,0,2,2,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,2,2,2,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,2,0,0,0,2,0,0,0,0,2,2,2,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,2,0,0,0,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,2,0,0,2,2,0,0,0,0,2,2,2,0,0,0,2,2,0,0,0,2,2,0,0,-1,0,1,1,0,2,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,2,0,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,2,2,2,0,0,0,2,0,2,2,0,0,0,0,0,2,2,0,0,2,2,0,2,2,0,2,0,0,0,2,2,2,2,2,0,0,0,0,0,0,2,0,0,2,0,2,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,2,0,2,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,2,0,0,0,2,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,2,2,2,2,2,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,2,2,2,0,0,2,0,2,2,0,0,0,0,2,2,0,0,0,0,2,2,0,0,0,2,2,2,2,0,0,2,2,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,0,2,2,0,0,2,2,0,2,2,2,2,2,0,0,0,0,2,2,0,2,2,0,0,0,0,0,2,0,0,0,1,1,0,2,2,0,0,0,0,0,0,2,0,2,2,2,0,2,0,0,0,0,0,2,2,0,2,2,0,0,2,2,0,0,0,2,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,2,2,2,2,0,0,2,2,0,2,2,2,0,0,2,2,2,0,2,2,0,0,0,0,2,0,0,0,0,0,0,2,2,2,0,2,2,0,0,0,0,2,2,0,0,2,0,0,2,0,0,0,2,0,0,0,0,0,0,0,2,0,0,2,2,2,0,2,2,0,0,0,0,0,0,0,2,2,0,2,2,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,2,0,0,0,0,2,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,2,2,2,0,2,2,0,0,0,2,2,0,0,0,2,0,0,0,2,0,0,0,0,0,2,2,0,0,2,2,2,0,0,0,0,2,2,0,0,0,2,2,0,0,2,0,0,2,2,0,2,0,2,0,0,0,0,0,2,2,0,0,0,0,2,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,0,2,0,0,2,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,2,2,2,0,2,2,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,0,2,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,2,0,2,2,0,0,0,2,2,0,2,2,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,-1,0,1,1,0,0,2,0,2,0,0,0,0,2,2,0,0,0,0,0,2,2,0,0,2,0,0,0,2,2,0,0,0,0,0,0,2,2,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,0,0,-1,0,2,0,0,2,0,0,2,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,2,0,2,2,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,0,0,2,0,0,0,0,0,-1,0,2,2,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,2,0,2,2,2,2,0,2,0,2,2,0,0,0,0,0,0,2,2,0,0,2,2,0,0,0,0,0,0,0,2,2,0,0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,2,2,2,0,0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,2,2,2,2,2,2,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,2,2,0,0,2]},"autogen_seq":{"pPN8d0E3900":["hey","I'm","overly","Asian","","","","","I'll","","","","","capsule","networks","","","","","","","nets","Jeffrey","Henson","","","","","capsule","Network","","","ago","","","","","","","","","","","","","","ideas","","","","","","","","","","properly","","now","a","","","ago","","","2017","","","","dynamic","routing","between","capsules","","","","Sarah","saber","","frost","","","","","Hinton","they","","","","state-of-the-art","","","","MS","dataset","","","","","","","","","","","","","digits","so","","","","","exactly","well","","","graphics","","","","","","","","","scene","","","","","","","X","20","","y","equals","30","","","16","degrees","and","","","on","each","","","","","","parameters","then","","","","","function","","boom","","","","image","inverse","graphics","","","","","process","you","","","","image","","","","","","","","","contains","","","","","","are","a","","Network","","","","","","","","","","","graphics","it","","","","","capsules","a","","","","","","","","","","","","","","","","","","","","","","location","for","example","","","","","","capsules","the","","","","","","","","capsules","","","vectors","the","","","","","","","","","","rectangles","","","","","","","","","","","","triangles","the","","","","","","","","","","","","","","","","","","","","present","so","you","","","","","","","tiny","","","","didn't","","anything","","","","","","long","this","","","","","","","","","","","","","","","","","","for","","","","","rectangle","","the","triangle","so","","","","","","","","","","","","","object","","","","","","","object's","rotation","","","","","","","thickness","","","","","","is","","","location","it","","","","translations","","","on","for","simplicity","I'll","","","","","","parameter","","","","","","Network","","","","","","five","ten","","","more","in","practice","","","","","","","","","","","","","","layers","","","","","","","","net","this","","","","","","","","","","maps","you","","","","","","","","","","","","","","location","for","example","","","","","","","","containing","say","","","","2","","9","","","","","","","","","two","","","nine","","each","","","location","you","","","","three","","","six","","each","","","on","something","","","","","","","","","","","","","","","location","the","","","","","","","","","","","","one","","","vectors","","","","","","","probability","","","","","","one","to","","this","","","","","function","it","","","vectors","orientation","","","","","","","","","","","","zero","","one","one","","","","capsule","networks","","","","","","","","","object's","","","","pose","","","network","for","example","","","","","","slightly","","","","","","","","slightly","right","this","","","equivariance","in","","","","","net","","","","","","layers","","","","","","","","","information","","","","","","","","","","objects","it's","","","","","","","","","","","","","","image","","","","","","","","","","","","","","which","","","you","know","","","pose","the","","","","","equivariance","","","","","","","applications","alright","","","let's","","","","","","","","","","","","","","","parts","for","example","","","","","","","X","equals","22","","y","equals","28","","","","16","degrees","this","","","compotes","is","","","parts","in","","","","","","","triangle","so","","","","","","","rent","rendered","now","","","","","","reverse","","","universe","graphics","","","","","","","","","","","","","","","","","","parameters","similarly","","","","","","draw","a","house","","","","parts","","","in","","triangle","that","","arm","","","","","way","so","","","","","","","","come","to","","","","","","","","in","","triangle","","","out","","","","","","","","","","","","","","orientation","","","","","","","","","boat","","","house","so","yeah","let's","let's","","","","","","","this","the","","","","","","seen","","","","","of","","layers","","","","","","","vectors","","","","them","this","","","","","","","","capsules","we've","","","","","already","the","","","","","","","","","","","","","","","place","every","","","","","","","","","","","","","","","","","layer","you","","","","","","","","","","means","that","the","","at","the","primary","","","","tried","","","","","","","","","output","for","example","let's","","","","","","","rectangle","I'll","","","","rectangle","capsule","let's","","","","","","","","","","","layer","","house","capsule","","","boat","capsule","since","","rectangle","capsule","","","","","","16","degrees","","","","","house","capsule","","","","","","","16","degrees","","","sense","","","boat","capsule","","","","","","","16","degrees","","well","that's","","","","","","","","","","rectangle","so","","","","prediction","","","rectangle","capsule","","","","","","","","","","","","","W","IJ","","","","","","at","U","at","UI","during","training","","","","","","","","","","","","","","","","","","","layer","in","","words","","","","","","part","hole","relationships","","","","","","","","","","","","","house","","","on","now","let's","","","","triangle","capsule","predicts","right","this","time","it's","","","","interesting","","","","","","","triangle","","","","","house","capsule","","","","","house","","","","boat","capsule","","","about","","","16","degrees","these","","","","","","","","","","you","know","","","","","triangle","now","we've","got","","","","","outputs","","","","","","them","well","as","","","see","","rectangle","capsule","","","triangle","capsules","","","","","","boat","capsule","","output","in","","words","","","","","","","","","","","","","","","","rotations","and","","","","","","","house","capsule","","output","therefore","","","","","","","","","","","","","","","boat","","","house","now","","","","","","","","","","","","the","boat","","","","","","","","","","","","","","","","capsule","this","there's","","","","","","","","","","capsule","","","","","noise","they","","","","","","","","capsule","this","","","","","agreements","there","","","benefits","first","","","","","","","","","","","","","","layer","","","","","","","","","","","accurate","","","","","","","object","second","","","","","","","","activations","","","","","","","","parts","","","","","","","","","objects","like","","","biller","","","","boats","","","","","","","boat","","","on","lastly","","this","","","","","","","","","","we","","","","","","","slides","but","first","let's","","","","running","","agreements","","","","capsule","networks","here","","","","","","","","","boat","","","","","lower","level","capsules","for","example","","","","","","","","","rectangle","capsule","","","","","","","","","boat","","","","","","","","triangle","capsule","thinks","","","","","","","","","","low","level","capsules","","","","","","","","","vectors","","","","capsule","","this","in","","example","","","","post","parameters","","","","","angle","","","","","","","","","boat","as","","","earlier","post","","","","","","","","","features","","skew","thickness","","","on","or","location","a","","location","so","","","","","do","","","","","","","","","predictions","this","","","","vector","the","","","","","","","","","","","","","","meet","vector","so","","","","","","Euclidean","","","","","","","","","","product","basically","","","","","","","","","","","","","","","vector","use","using","","","measure","","","","","weights","","","","","accordingly","note","","","","","","","","","","","","","","","","weight","","","","","","","","","","","","weight","I've","represent","them","","","","black","now","","","","","","","","","or","","","say","","","mean","","","","","","","","","","cluster","","","","","","cluster","so","next","","","","","","","weights","and","","","","","","","","","","","black","and","again","","","","","mean","and","","","","","","","","times","in","","you","know","three","","five","","","","sufficient","this","","","you","","suppose","","","","","","","","","it","okay","","","","","","","","","agreement","now","let's","","","","","","","","","","","details","first","","","","output","","","","","","","","","bij","","","zero","next","","","","","","softmax","function","","","","weights","","","","capsule","this","","","","","","","","","output","","","","","each","equal","weights","next","","","","","","","","predictions","","","","","","","layer","this","","","","","","one","","","","","","","","function","and","voila","we","","","","","","","","house","capsule","","boat","capsule","but","","","","","","output","is","","","","","","","round","","","iteration","now","","","","","","","","accurate","for","example","","rectangle","capsule","","","","","","","boat","capsules","output","it","","","","","closely","this","","","","","","","","","","","","","U","hat","j-jake","I","","","","","","V","J","this","","","","","","","","","outputs","","","weight","bij","so","","","","","","","","","increased","when","","","","","agreement","the","","","","going","to","be","large","","","","","","","","weight","on","","","hand","","rectangle","capsule","","","","","","","","house","capsules","output","","","","","","","","","","","small","","","","","weights","","these","","","","","","much","next","","","","","","","","","","","","","weights","","again","and","","","","see","","rectangle","capsules","","","","","boat","capsule","","","","","","0.8","","it's","","","","","house","capsule","","","","0.2","so","","","","","","","","going","to","go","to","the","boat","capsule","not","","","capsule","once","","","","","","","","","","","outputs","","","","","","","","layer","","","","house","capsule","in","the","boat","capsule","","","","time","","house","capsule","","","","","","","","","","","vector","on","","","","","boat","capsule","","","","","","its","output","that","","","","","","","","1","so","","","","it","and","that's","","","","","2","and","","","","see","","","","","iterations","","","","","","","","","","","","boat","after","","","","","","rounds","","","","","","","","","","","","","","","way","so","","","","earlier","","","","","","","","","","scenes","","","","","","","","","","one","way","to","interpret","this","image","as","","","","there's","","little","","","ambiguity","","","","","","","","","","middle","however","","","","","case","","","","","","","","","","","","","","triangle","","","","","","","","","are","the","","","","","","","","","","","","","","","","","","","","","bottom","and","","","","","","","","","solution","","","","","","","","happy","","","","","","","","","","","","","layer","the","","","","away","ok","","","","","","","","","","","","","","","","works","well","","one","","","","","","","","","course","just","","","","","","","","","capsule","","","that's","","","","","","it","all","","","","","","","","","","","","","","top","layer","","vectors","","","","","","","","probabilities","you","","","","","","","","","","cross","entropy","loss","has","an","irregular","","","network","","","know","you'd","","done","however","","","","","","","","","","","","","","","","","","","image","so","","","","","","detail","","","","","","","","","","","","K","","","","","image","","","","","","","help","put","","","","squared","","","","","0.9","it","","","long","conversely","","","","","","K","","","","","","image","","","","","","","","vector","","","squared","","","","","0.1","so","","","","","","","","","","","classes","in","","paper","","","","","","","","","","","","network","it's","","a","three","you","know","three","","","fully","connected","","","","","","","","","layer","and","it","","","","","","","","","","","","","","","","","","","image","the","","","","","","","","","earlier","","","","","scaled","","","","","","","","","","","","training","the","","","","","","","","","","","","","","","","","","","","construct","to","","","image","","","","","","","","","network","","","layer","this","","","","","","","regularizer","","","","","","","","","","","","examples","and","that's","it","","","","","","Network","works","","","","","it","let's","","","","","","","","","","","","paper","","","","interesting","so","this","","","one","","","","","","","mmm","mist","you","","","the","","","","","","layers","","","","","","","","","","","","","","","capsules","and","","","","","","","","six","","six","grid","","","","","","","","","","grid","","","","","","","eight","dimensional","vector","so","","","","","","","","","","","ten","","capsules","","","sixteen","","lectures","the","","","","","","yoots","is","","","","","","loss","","","earlier","now","","","","two","","","paper","it","","","","","","","","","caps","net","it","","","","two","","","relu","","","","","","","","","","","","","","","","","","","","richens","","","which","","","","","","","the","","","","","","","","","","","","","","loss","right","","","","","four","","","paper","also","interesting","one","","","","","","","","","","","","","quite","interpretable","for","example","","","","","","","","","","","","","","","","","","","","","","capsules","output","you","","","","","","","","","","you","know","","","thickness","the","","","","","","skew","if","you","look","at","how","it","that","number","4","is","modified","from","the","left","to","the","right","the","","","","","","","","","","","","","","","","position","so","","","","see","it's","","","","","","","","do","ok","","conclude","let's","","","","","cons","capsule","","","","state-of-the-art","","","a","missed","on","so","far","10","they","","","","","","10","percent","error","","","","","a","state-of-the-art","","it's","-","","","","","","","","","you","know","","","","","","","them","","it's","","","","start","capsule","","","","","data","they","","equivariance","","","","","","","","","preserved","and","","","","","","","","","","detection","the","","","","","","","","","scenes","the","","","","","","","","","parts","","","","","associated","","","whole","and","it's","","","","rotations","","","","FIM","transformations","the","","","","","somewhat","interpretable","and","finally","obviously","its","sentence","idea","","don't","","","it","however","","","","","cons","first","","","","","","","","","state-of-the-art","","cipher","10","","","it's","","","start","plus","it's","","","","","","","","","","images","","","","image","net","data","set","you","know","what","","","","be","capsule","","","","","","","Train","","","","","","","","","","","","","","","loop","","","","earlier","finally","","","","","","","","","","","","","location","","it's","","","","","","","","","","","","","","","","","","","","","another","this","","","crowding","","it's","","","","","","","well","","it's","","","","showstopper","alright","","","","","","","","","","","","","caps","net","implementation","","","","","","","I'll","","","","","","","","below","if","","","","time","","","","","","","","","","","doing","the","","","","","caps","nets","","","","","","","","","","","","","algorithm","implementing","","","Kerris","","tensorflow","","","","","","","","","pi","torch","","","","","done","so","if","","don't","","","","preference","","","","","","","PI","torch","","","probably","","","","understand","and","that's","","","had","","","","","","video","if","","did","","","up","share","comment","subscribe","blah","blah","blah","it's","","","","","video","","","","","","useful","I'm","","","more","if","","","","","","","machine","learning","deep","learning","and","deep","reinforcement","learning","","","","","","","Holly","","hands","on","machine","learning","","scikit-learn","","tensorflow","it","","","","","topics","","","","","","","","","","","","account","","I'll","","","","","","","description","that's","","","today","","","","","","","time"],"2Kawrd5szHE":["hi","I'm","over","LaVon","","","I'm","","","","","","","","","","","","tensor","flow","in","","","video","","","","","","","","networks","","","","","","architecture","if","","haven't","","","video","","","","","","","now","","","","","","","tensor","flow","implementation","","","","jupiter","","","","","","","","explanations","","i","","","","","","","as","","I'll","","","","","","","","","below","","","","","","","it","","","","it","so","","","","ninety-nine","point","four","","","","","set","","","","good","","it's","","","","","","","","","","","","","","1200","capsules","there's","","","","","","the","in","","notebook","","","won't","","","","","","","","video","","I'll","","","","","","","across","","","","","","","","","","","tensor","flow","implementations","","","caps","nets","okay","let's","","a","network","first","","","","","","","","","","network","and","that's","","","layer","we","","","","","","tensor","flow","placeholder","the","","","","unspecified","","","","","","","","","","","","batch","","","example","5","note","","","","","Tony","8","by","28","","images","","","","channel","","","","","grayscale","color","","","","","","channels","","red","","","blue","and","that's","","","","","layer","next","let's","","","","","layer","for","","","","","","","","","","maps","","","","6","by","6","","","","","vectors","the","","","","","","","","","","","","","","segment","that","you","","","","","","","","","","","","","there's","","","","","line","and","","","","","ad","","","","","parameters","","","case","I've","","","","angle","","","vectors","a","","","","","","","","","","","","line","","","","","","","","","","","","","","","","6","by","6","grid","","","the","","","","straightforward","first","","","","","","layers","the","","","","","","","X","","","","","","","","","","","","","runtime","the","","","","","","","","","layer","of","course","and","","","","","","","","paper","the","","","","","","","","","maps","","","","","","","","","scalars","we","","","","a","","","","instead","","","","tensor","flows","reshape","","","","","Maps","","eight","","vectors","","","","","","scalars","in","fact","","","","","","","","","","","","capsules","","","","","","","","","","1152","","","that's","a","thirty-two","times","six","times","six","","","","","","batch","and","","","","","","","","","","","","","","","","","zero","","one","for","this","","","","homemade","","function","here","","is","this","","","","","","","","","paper","it","","","","","","array","","","","dimension","","","","","one","so","","","","see","","","","","","","","","","vector","","there's","","","","","","","","","","","","","","","","","","vector","so","","","","","","","","","","","denominator","","","","","","","","","problem","however","","","","","","","issue","the","","","","","","","","","","","","","zero","so","","","","","tensor","flows","","function","to","compute","the","norm","","","equation","","","","","","","","","","zero","","","","","","it","","","n","a","n","and","","so","","","result","","greed","in","","","","","","","model","","","","","","","","","well","the","","","","","dead","then","you","don't","","that","so","","","","","","","","","","","norm","","","","","","","right","and","","","it","that's","","","","","capsules","apart","","","","","safely","","","","straight","forward","on","","","","","","","","","is","","","capsules","there","","","ten","","them","","","","digits","","","9","","the","","","","vectors","in","","","example","","","","","","","","","","","","","","4","and","","it's","","","","","","","","","","","","","","digit","","rotation","","thickness","","Q","","position","","","on","by","","way","","","","","","","","","","","","","","","","","","","","","","","","grid","so","","example","","","","","","","","","","","","","","","they","","","","","","","","","activated","see","so","","","","","","","","","contains","a","","","information","","","","","","","","","","","grid","but","","","","","layer","","","","","","","","","","","","","","","","","","space","ok","","let's","","","","","","layer","the","","","","","","","","","vectors","since","","","","","","","","","","layer","","","","","","","","","","","","","","","capsules","for","example","","","","","","","","capsule","","","","","","","","","","","capsule","for","this","","","","","","","w11","","","","","","","training","","","","","","","","","","","","capsule","this","","","u","hat","1","1","","","","","","","","","","capsule","","","","","","","","","capsule","since","","","","","eight","","vectors","","","","","","sixteen","","vectors","","","","w11","","","","sixteen","by","eight","matrix","next","","","","","","","","","","","capsule","","","","","","","","","","capsule","note","","","","","","","","matrix","w12","and","","","","","","","","capsule","","W","one","three","and","","","","","","","capsules","then","","","","","","","","capsule","","","","","","","","","","","","","","capsule","and","","","","","","","capsules","then","","","","","","","","capsule","","","ten","predictions","and","","on","","","","picture","there","","1152","","","multiply","six","by","six","by","32","","ten","","capsules","","","","","","eleven","thousand","five","hundred","and","twenty","","","vectors","now","","","","","","","","one","","","","","","inefficient","so","let's","","","","","","","","","","","","","","matte","small","operation","now","","","","tensorflow","mat","small","","","","","","matrices","","","","","","","","","","","","","","","","","","shot","this","","","","efficient","","","","","","","","card","","","","","","","","","","","","","","","threads","so","here's","","","works","suppose","a","b","c","d","e","f","","g","h","i","j","k","l","","","matrices","you","","","","","","","arrays","","","two","","","three","columns","","example","so","","","two","","","","","","","matrices","","","","","two-dimensional","","","","","two","plus","two","equals","four","","arrays","if","","","","arrays","","four","d","arrays","two","matmo","","","","","element-wise","","application","","","","","","","four","","","","a","x","G","here","and","","","","H","here","","","on","so","let's","","","","","","","","","vectors","we","","","","","4d","","","","","","matrices","there's","","","","","capsule","","","","","","capsule","the","","","","","","","","","","","capsule","then","","","","","","","","","mole","function","","","","","","","","","","","","","","","","","capsules","since","","","","","","","","","ten-digit","","","","","capsule","","","","","ten","","","","","capsules","outputs","we","","","","","","","","","","","","","vectors","ten","times","but","there's","","","catch","we","","","","","","","","","","","","batch","","","","instance","so","there's","","","","","","","size","it","","","","","","","","","","","","","","instance","","","","","","fine","it","","","","dimension","but","","","","","","4d","","","","","","matrices","","","","","","","","","","","","","batch","now","","","","this","","","","","","","clear","first","","","","","","","","","matrices","it","","a","","","","","capsule","","","","","castle","capsule","","","","","","","matrices","that's","four","dimensions","","","","","","","","","","","","","","","","","","","","","","","","","","batch","now","the","","","","randomly","","","","","","","","","that's","","hyper","parameter","","","tweet","and","","","it","we","create","","variable","next","","","","","","","","","instance","","","","","","","","","size","we","don't","","","","","","","time","","","","","","","","","","graph","but","","","","tensor","flows","shape","function","","","","","","will","","","","","run","time","","","","","","dimension","","","","","size","then","","","","","","","array","","","","dimension","","","","","","instance","now","","","","","","","","","","","three","","array","","","","","","","size","","","","","","runtime","","there's","","","","capsule","","","","","eight","dimensions","so","","","","","","","","","","","","","","","","","for","to","","","","","mat","Mulla","operation","first","","","","","","","","end","","tensor","flows","expand","dims","function","and","","","","","","","vectors","","","one","","array","each","","","","","","","","column","vector","","","matrix","","2d","array","","","","column","then","","","","dimension","","","","capsules","and","","","","","","","ten","","","","","dimension","","","","capsule","and","lastly","","","","map","Moll","","","","","","","","","capsules","","vectors","","","","","","","capsules","","","","","","","","","","capsules","","","","","","","batch","in","","shot","and","that's","","","","","","","","","","","capsules","outputs","","","","","","","","","vectors","the","","","","","","","","algorithm","so","first","","","","","","","","","0","for","this","","","","TF","zeros","there","is","","","","","","","","","digit","capsules","and","","","instance","the","","","","","","","","1","","","","","","","minute","next","","","","","","","","capsules","","","","weights","okay","so","","","","","dimension","next","","","","","","","","","","","vectors","","","","capsule","","","","","","so","the","weighted","sum","is","along","this","dimension","this","is","pretty","straightforward","tensorflow","code","first","multiply","the","routing","weights","","","","","this","","a","element-wise","multiplication","","","","multiplication","","","","","","","","","","dimension","and","","","","","","","","","","","","","","","","step","","","","","","","","","","","dimensions","","","rank","they","don't","","","","","shape","","","","","","","tensorflow","","","broadcasting","now","","","","","","","is","","","","","clear","I'm","","","matrices","","","","","","","","row","","tensorflow","","","","","","","","","","","","","times","you","","","","","","","tiling","","","","earlier","","","","","efficient","and","you","","","","","didn't","","","earlier","","","","","","","","","","","multiplication","here","we're","","element","wise","multiplication","okay","","","","","capsules","we","","","","","","","","vectors","","","","capsule","","","","","","","function","","","","","","","","","capsules","all","right","but","wait","","","","","","","","one","","","","","","algorithm","now","","","","two","so","first","","","","","","","","","was","","","","","","","","weights","for","example","","","","","","","","","","","capsules","output","notice","that","","example","","","","","four","","excellent","and","","","","","","","","","","","","","","","","","vector","these","","","","","","","","vectors","","","","","","","column","so","","","","","product","","","","","","","","you","had","J","I","","","","","vector","","","this","","","","","","","","","V","J","","","","","vector","we","","","","one","by","one","","","","","","","","vectors","and","now","","","","","","","","","","","vector","","","again","","","","","map","mol","","","","","","","","","","","shot","first","","","","","tile","","","","","","","","","","","V","J","","","","capsule","then","","","map","mo","","","","","","metric","","","","","array","","","fly","","lo","and","behold","","","","","","","","once","so","now","","","","","","agreements","","","","","","","","","vector","we","","","","","","","","","","whites","","","","addition","and","","","","","","","","","","","","","","","1","the","","","","identical","","we're","","","","","","ways","","","2","we","","","","","","","","","","","","2","","","","","","","","","","","","","","","capsule","","","","","","results","and","","","","","","","","outputs","","we","finish","","2","we","","","","","","","","","","one","","I'll","","now","","","","","","","","","","","","","","","","","","","capsules","now","","","","","","","","","algorithms","","","","","loop","it's","","","","","","","","","","","","100","","","code","of","course","","","","","","","tensor","flow","graph","but","","works","you","","","","","","","","loop","now","","a","","","","","","","","","","","","","Python","","this","","better","however","it's","","","","","","","tensor","photograph","","","","","","","","","","","","code","all","we're","","","","","","graph","","tensorflow","","","","","","","use","the","","","","a","graph","again","","","fine","it's","","","","","","","","","","graph","so","","","","","","","","","","loop","","","","","","","time","if","","","","","loop","","","tensorflow","","","run","","","","","tensor","flows","while","loop","","","this","the","while","loop","","","three","parameters","","","","","","","","","","","","","","","","","","","","","","not","","","iteration","the","","","","","","","","","","","","","","loop","and","","","","","","","","iteration","and","finally","","","","","","","","","","","","","","","condition","","loop","body","function","","","","iteration","for","","","iterations","","","","","","","","","loop","body","function","so","","","","","","","","","","","","","","","","code","once","","","it","","","","","","caps","net","","","","","","","","","","","","loop","apart","","","","","","","","","smaller","","","","","","","","","","","","","","","","","model","also","","","","","swap","memory","","","","while","loop","function","","","","","","true","tensorflow","","","","","","","","","","when","it","can","","","","memory","since","","Ram","","","","","abundant","","","","","useful","and","that's","it","we've","","","","","","","capsules","cool","now","","","","","","","","","","","","","","","","","","","","image","so","let's","","","probabilities","for","this","","cannot","","tensor","flows","norm","","","","","","","there's","","","","","","point","","","mentioned","earlier","so","","","used","","homemade","safe","norm","function","","","","","","","","squash","function","and","","","","","","","","don't","","","","","one","","","","","","","soft","max","layer","this","","","","","","","","","","","","","but","","","","","","","digits","view","","","","","","","a","3","","","cannot","detect","say","","fives","next","let's","","","","","digit","we","","used","","Arg","max","","","","us","","","","","","probability","the","","","","","","number","of","","","itself","note","","","","","","","","","","","","","","","one","","","end","","","","","","","","","squeeze","function","if","","","squeeze","","","","","","remove","","","","","","","","1","this","","","","okay","","","","","","","","","1","","","case","","","","","","","","value","","","","array","","","don't","","that","so","","","","","","axes","great","","","","","","","","","","","","","","predictions","we","","","","models","","","","","","","","","","","","labels","in","","","","","","","","","","","","","wrong","it's","seven","","","one","so","","","","accuracy","now","the","","","","straightforward","","","","","equal","","","","","","","","","wipe","red","","","","","","","","boolean","z'","","","","","boolean","stew","floats","","","","","","","zeros","for","","predictions","","ones","for","","predictions","","","","","","","","","batch","accuracy","the","","Y","","","","","placeholder","nothing","special","and","that's","it","","","","","model","","","","predictions","now","let's","","","","","code","this","","","","two","","","","","I'll","","","","","clarity","and","now","first","","","","","","","loss","it's","","","","equation","by","","way","","","","","","","","video","","","","","","","squaring","","","operations","sorry","","that","this","","","","","equation","computing","","","","straightforward","","","won't","","","","","details","the","","","","","","","","","","","","","TK","values","for","","","instance","TK","","","","","","","","","cos","K","","","","","image","","it's","","","0","","","","","","TK","","","","","","","","","","","one","hot","representation","for","example","","","instance","","","three","","","","instance","","","","","","","","","","","","","","","its","","3","okay","next","","","","","","","loss","so","first","","","","","","","","","","","","","","","","","","","","","","","images","this","","","","","","feed-forward","","","","","three","","","layers","it's","","","code","","","","","","","","","","and","","","","","","it","it","","","","","","","","","","1","","","instance","","","","","","28","by","28","","images","and","that's","it","","","","","images","we","","","","","","loss","this","","","","","","","","","","","the","reconstructions","since","","","","","28","by","28","by","1","","","","","","","","","","","","","each","then","","","","","difference","now","","","","","","loss","it's","","","","","","","","","","","loss","scale","","","","","","","","","","pretty","simple","as","you","can","see","now","let's","add","the","training","operation","the","","","","use","tensor","flows","","","","atom","optimizer","","","","parameters","","let's","","that","we","","","optimizer","","","it's","minimized","","","","","","","","","","","","","","","","loss","we're","","done","","there's","","","","","didn't","","","","","video","the","","","","","","","","","","","","","","","","","","","","","","","","","so","instead","of","sending","the","digit","capsules","","","","","decoder","","","","","","","first","","this","the","","","","","","","","","","","","array","","","","","","","zero","","","","once","","","","","","","digits","by","","","","capsules","","in","","mask","","","","","","","decoder","but","there's","","catch","this","","","","","training","","","","time","","won't","","","labels","so","instead","","","master","","","","","","","","","","labels","","this","now","","","","","","","","","","","testing","","","wouldn't","","","convenient","so","instead","let's","","","conditioned","operation","we","","","","","a","","","mask","with","","if","","","true","","","","","labels","to","","","mask","if","it","is","false","then","we","use","the","prediction","note","the","difference","okay","and","here's","the","code","we","build","the","mask","of","labels","placeholder","","","","","false","","","","","","","","","","training","and","","","","","","","","tensorflow","scon","function","it","","three","arguments","","","","","","","","","condition","","","","","","mask","with","labels","placeholder","the","","","","","","","","","","","","","","","","true","","","","","","","","","","","","","","that","","","is","false","then","","","","mask","","","","","one-hot","function","now","there's","","","","","","","implementation","","","","it","","","","","","","","","","","","","tensorflow","","suppose","","","","graph","","","","","operations","","","","","","","","","","a","the","","","tensorflow","","","","","","dependencies","","","","all","the","operations","that","a","depends","on","directly","or","indirectly","by","traversing","the","graph","backwards","in","this","case","we'll","find","C","","","F","next","","","","","of","these","operations","","","","inputs","these","","","","nodes","in","","","","","once","F","","evaluated","operation","","","","","","","","","","need","","","","","evaluated","","tensorflow","","","","","","","","parallel","say","","","first","a","","","","unev","alyou","ated","","","","can't","","yet","but","","","","","","finished","a","","","evaluated","and","","it's","done","","eval","","","","result","in","we're","good","you","","","","","","","once","","","a","","E","","","","","","","same","it","","","","dependencies","","","","operations","","then","","know","","","","","","","","","satisfied","and","","","","","","","","a","","E","","","","results","so","let's","","","","","","targets","this","","","","","","","cond","operation","","","three","parameters","mask","with","labels","","","","","Y","","","","","","wipe","red","and","","","","","reconstruction","targets","","","","","","","it","","","","","loss","","","","","","loss","","","","","","reconstruction","targets","well","","","is","","earlier","tensorflow","","","","","dependencies","it","","","","","nodes","and","","","","all","so","Y","pred","","","first","since","","","","","","parallel","there's","","","","","","","","","","finish","so","","know","Y","","","next","and","","mask","what","label","finishes","so","","","","","true","now","","reconstruct","target's","","","","","","needs","","","","it","can","","evaluated","and","","","","","","","thing","","masks","with","labels","","true","","","","","","y","which","","good","but","","","y","pred","","","","nothing","we're","","","","output","it's","","","","","","","","","","","","","","loss","","","","","","","probabilities","","","","","","","","","prediction","","","","","won't","","","overhead","but","still","it's","","","unfortunate","now","","mask","what","labels","","","false","then","again","","reconstruction","targets","","","","","thing","","","","","","","y","pred","but","","time","how","","","","","nothing","it's","","","placeholder","","","won't","","","","time","","","","","","","","why","","","mask","with","labels","","false","well","","","","","","","","array","and","","fine","so","","","work","","","","","ugly","so","","","","","","","","","","","","","","","","con","function","","","","","","tensor","","","","","","","","","","","functions","so","if","","","","","","functions","","tensorflow","","","","","expect","it","","","","","","","","within","","","","","","","","","","","dependencies","so","","","","","","","","","","","","ugliness","","tried","","","","","","","","code","","","","","","","","implementation","","","","won't","","","","","night","so","we've","","finished","here's","","","","again","the","","","","over","","","","built","","","","","","phase","let's","","","graph","first","let's","","","","","code","it's","","","","standard","you","","","session","","","check","point","","exists","","it","","","","","","variables","then","","","","","loop","","","","","epochs","","","","epoch","","","","","","","","","","set","and","","","loop","","","","","","operation","","","loss","","","","batch","we","","","","","","","","","","","batch","","","","masks","with","labels","","true","that's","","","","","","","it","and","","note","book","","","","","","","","","stopping","","","","","","progress","","","","","","","","","","","","","","","epoch","but","","","","","","here","after","training","","","","","","tests","","","","","","","","","","reconstructions","as","","","see","","","","","correct","","","","","","good","","","","","","","images","","","they're","","fuzzier","and","","","","see","here's","","code","there's","","","","","it","","","","","images","","","","","","","","model","","","","","","","the","predictions","","","decoders","","and","","","","","capsules","output","","","","","","later","the","","","mentioned","","","","here","right","","","","","","","array","this","","","","","anyway","and","finally","","","","","","","","","","results","","","decoder","so","","","","","","","","","sixteen","","","and","","","capsules","","vector","for","example","","","","","","","","","","","","","so","the","","","","","","","","","","sixteen","parameters","and","","","","see","","","first","","","","rows","","","","","","","","","thinner","we're","","","","right","","","","","","","left","","","","","","thickness","and","","","","row","","","","","","","","","","","five","","","","","top","","","","","","","","","","digit","before","","finish","I'd","","","","","","","","","","","","video","","","","","","","","","","","","response","","I'm","","","","","","","you","","","","","","","","videos","if","","","","","","","machine","learning","","","","channel","","","","O'Reilly","","hands","on","machine","learning","","scikit-learn","","tensorflow","I","","","","","","","description","if","","","German","there's","","","","","","","","Christmas","and","","","","French","","","","","available","it","","","","","","","it's","","","","content","and","that's","","","have","","today","","","","","","","","","","","","","","","","tensorflow","","","networks","if","","did","please","like","share","comment","subscribe","","","","","","","","","","","button","","","","","","","","videos","see","","","time"]},"manual_seq":{"pPN8d0E3900":["Hey! I\u2019m Aur\u00e9lien G\u00e9ron,","Hey! I\u2019m Aur\u00e9lien G\u00e9ron,","Hey! I\u2019m Aur\u00e9lien G\u00e9ron,","Hey! I\u2019m Aur\u00e9lien G\u00e9ron,","","","","","I\u2019ll","","","","","Capsule Networks,","Capsule Networks,","","","","","","","nets. Geoffrey Hinton","nets. Geoffrey Hinton","nets. Geoffrey Hinton","","","","","Capsule Networks","Capsule Networks","","","ago,","","","","","","","","","","","","","","ideas,","","","","","","","","","","properly,","","now. A","now. A","","","ago,","","","2017,","","","","\u201cDynamic Routing Between Capsules\u201d","\u201cDynamic Routing Between Capsules\u201d","\u201cDynamic Routing Between Capsules\u201d","\u201cDynamic Routing Between Capsules\u201d","","","","Sara Sabour,","Sara Sabour,","","Frosst","","","","","Hinton. They","Hinton. They","","","","state of the art","","","","MNIST dataset,","MNIST dataset,","","","","","","","","","","","","","digits. So","digits. So","","","","","exactly? Well,","exactly? Well,","","","graphics,","","","","","","","","","scene,","","","","","","","x=20","x=20","","y=30,","y=30,","y=30,","","","16\u00b0,","16\u00b0,","16\u00b0,","","","on. Each","on. Each","","","","","","parameters. Then","parameters. Then","","","","","function,","","boom,","","","","image. Inverse graphics,","image. Inverse graphics,","image. Inverse graphics,","","","","","process. You","process. You","","","","image,","","","","","","","","","contains,","","","","","","are. A","are. A","","network","","","","","","","","","","","graphics. It","graphics. It","","","","","capsules. A","capsules. A","","","","","","","","","","","","","","","","","","","","","","location. For example,","location. For example,","location. For example,","","","","","","capsules. The","capsules. The","","","","","","","","capsules. The","","","vectors. The","vectors. The","","","","","","","","","","rectangles,","","","","","","","","","","","","triangles. The","triangles. The","","","","","","","","","","","","","","","","","","","","present. You","present. You","present. You","","","","","","","tiny,","","","","didn\u2019t","","anything,","","","","","","long. This","long. This","","","","","","","","","","","","","","","","","","for,","","","","","rectangle,","","a triangle. Next,","a triangle. Next,","a triangle. Next,","","","","","","","","","","","","","object,","","","","","","","object\u2019s rotation,","object\u2019s rotation,","","","","","","","thickness,","","","","","","is,","","","position (there","position (there","","","","translations),","","","on. For simplicity, I\u2019ll","on. For simplicity, I\u2019ll","on. For simplicity, I\u2019ll","on. For simplicity, I\u2019ll","","","","","","parameter,","","","a","","","network,","","","","","","5, 10","5, 10","","","more. In practice,","more. In practice,","more. In practice,","","","","","","","","","first","","","","","layers,","","","","","","","","net. This","net. This","","","","","","","","","","maps. You","maps. You","","","","","","","","","","","","","","location. For example,","location. For example,","location. For example,","","","","","","","","containing, say,","containing, say,","","","","(2","","9),","","","","","","","","","2","","","9","","each,","","","location. You","location. You","","","","3","","","6","","each,","","","on. Something","on. Something","","","","","","","","","","","","","","","location. The","location. The","","","","","","","","","","","","1,","","","vector\u2019s","","","","","","","probability,","","","","","","1. To","1. To","","this,","","","","","function. It","function. It","","","vector\u2019s orientation,","vector\u2019s orientation,","","","","","","","","","","","","0","","1. One","1. One","","","","Capsule Networks","Capsule Networks","","","","","","","","","object\u2019s","","","","pose,","","","network. For example,","network. For example,","network. For example,","","","","","","slightly,","","","","","","","","slightly. Right? This","slightly. Right? This","slightly. Right? This","","","equivariance. In","equivariance. In","","","","","net,","","","","","","layers,","","","","","","","","","information,","","","","","","","","","","objects. It\u2019s","objects. It\u2019s","","","","","","","","","","","","","","image,","","","","","","","","","","","","","","(which","","","","","","","pose). The","pose). The","","","","","equivariant","","","","","","","applications. All right,","applications. All right,","","","let\u2019s","","","","","","","","","","","","","","","parts. For example,","parts. For example,","parts. For example,","","","","","","","x=22","x=22","x=22","","y=28,","y=28,","y=28,","","","","16\u00b0. This","16\u00b0. This","16\u00b0. This","","","","","","","parts. In","parts. In","","","","","","","triangle. So","triangle. So","","","","it","","","rendered. Now","rendered. Now","rendered. Now","","","","","","reverse,","","","inverse graphics,","inverse graphics,","","","","","","","","","","","","","","","","","","parameters. Similarly,","parameters. Similarly,","","","","","","house,","house,","house,","","","","parts,","","","and","","triangle, but","triangle, but","","time","","","","","way. So","way. So","","","","","","","","","","","","","","","","","and","","triangle,","","","out,","","","","","","","","","","","","","","orientation,","","","","","","","","","boat,","","","house. So, yeah, let\u2019s","house. So, yeah, let\u2019s","house. So, yeah, let\u2019s","house. So, yeah, let\u2019s","house. So, yeah, let\u2019s","","","","","","","this. The","this. The","","","","","","seen:","","","","","","","layers,","","","","","","","vectors,","","","","them. This","them. This","","","","","","","","capsules. We\u2019ve","capsules. We\u2019ve","","","","","already. The","already. The","","","","","","","","","","","","","","","place. Every","place. Every","","","","","","","","","","","","","","","","","layer. You","layer. You","","","","","","","","","","means. The","means. The","means. The","","in","in","in","","","","try","","","","","","","","","output. For example, let\u2019s","output. For example, let\u2019s","output. For example, let\u2019s","output. For example, let\u2019s","","","","","","","rectangle. I\u2019ll","rectangle. I\u2019ll","","","","rectangle-capsule. Let\u2019s","rectangle-capsule. Let\u2019s","rectangle-capsule. Let\u2019s","","","","","just","","","","","","layer,","","house-capsule","house-capsule","","","boat-capsule. Since","boat-capsule. Since","boat-capsule. Since","","rectangle-capsule","rectangle-capsule","","","","","","16\u00b0,","16\u00b0,","","","","","house-capsule","house-capsule","","","","","","","16\u00b0,","16\u00b0,","","","sense,","","","boat-capsule","boat-capsule","","","","","","","16\u00b0","16\u00b0","","well. That\u2019s","well. That\u2019s","","","","","","","","","","rectangle. So,","rectangle. So,","","","","prediction,","","","rectangle-capsule","rectangle-capsule","","","it","","","","","","","","","","W_i,j","W_i,j","","","","","","u_i. During training,","u_i. During training,","u_i. During training,","u_i. During training,","u_i. During training,","u_i. During training,","","","","","","","","","","","","","","","","","","","layer. In","layer. In","","words,","","","","","","part-whole relationships,","part-whole relationships,","part-whole relationships,","","","","","","","","","","","","","house,","","","on. Now let\u2019s","on. Now let\u2019s","on. Now let\u2019s","","","","triangle-capsule predicts. This time, it\u2019s","triangle-capsule predicts. This time, it\u2019s","triangle-capsule predicts. This time, it\u2019s","triangle-capsule predicts. This time, it\u2019s","triangle-capsule predicts. This time, it\u2019s","triangle-capsule predicts. This time, it\u2019s","triangle-capsule predicts. This time, it\u2019s","","","","interesting:","","","","","","","triangle,","","","","","house-capsule","house-capsule","","","","","house,","","that","","boat-capsule","boat-capsule","","","a boat","","","16\u00b0. These","16\u00b0. These","16\u00b0. These","","","","","","","","","","","","","","","","triangle. Now we have","triangle. Now we have","triangle. Now we have","triangle. Now we have","","","","","outputs,","","","","","","them? As","them? As","them? As","","","see,","","rectangle-capsule","rectangle-capsule","","","triangle-capsule","triangle-capsule","","","","","","boat-capsule","boat-capsule","","output. In","output. In","","words,","","","","","","","","","","","","","","","","rotations. And","rotations. And","","","","","","","house-capsule","house-capsule","","output. Therefore,","output. Therefore,","","","","","","","","","","","","","","","boat,","","","house. Now","house. Now","","","","","","","","","","","","a boat,","a boat,","","","","","","","","","","","","","","","","capsule, there\u2019s","capsule, there\u2019s","capsule, there\u2019s","","","","","","","","","","capsule,","","","","","noise. They","noise. They","","","","","","","","capsule. This","capsule. This","","","","","agreement. There","agreement. There","","","benefits: first,","benefits: first,","","","","","","","","","","","","","","layer,","","","","","","","","","and will","","","","","","","","","object. Second,","object. Second,","","","","","","","","activations,","","","","","","","","parts,","","","","","","","","","object (like,","object (like,","","","","","","","boat,","","","","","","","boat,","","","on). Lastly,","on). Lastly,","","","","","","","","","","","","(we","","","","","","","slides). But first, let\u2019s","slides). But first, let\u2019s","slides). But first, let\u2019s","slides). But first, let\u2019s","","","","routing","","agreement","","","","Capsule Networks. Here,","Capsule Networks. Here,","Capsule Networks. Here,","","","","","","","","","boat,","","","","","lower-level capsules. For example,","lower-level capsules. For example,","lower-level capsules. For example,","lower-level capsules. For example,","lower-level capsules. For example,","","","","","","","","","rectangle-capsule","rectangle-capsule","","","","","","","","","boat,","","","","","","","","triangle-capsule thinks,","triangle-capsule thinks,","triangle-capsule thinks,","","","","","","","","","","low-level capsules,","low-level capsules,","low-level capsules,","","","","","","","","","vectors,","","","","capsule,","","this. In","this. In","","example,","","","","pose parameters:","pose parameters:","","","","","angle,","","","","","","","","","boat. As","boat. As","","","earlier, pose","earlier, pose","","","","","","","","","features,","","skew, thickness,","skew, thickness,","","","on. Or","on. Or","on. Or","on. Or","","location. So","location. So","","","","","do,","","","","","","","all","","predictions. This","predictions. This","","","","vector. The","vector. The","","","","","","","","","","","","","","mean vector.","mean vector.","mean vector.","","","","","","euclidian","","here,","","","","","","","","product. Basically,","product. Basically,","","","","","","","","","","","","","","","vector. Using","vector. Using","vector. Using","","","measure,","","","","","weight","","","","","accordingly. Note","accordingly. Note","","","","","","","","","","","","","","","","weight,","","","","","","","","","","","","weight. I\u2019ve","weight. I\u2019ve","weight. I\u2019ve","weight. I\u2019ve","","","","black. Now","black. Now","","","just","","","","","","(or","","","say,","","","mean),","","you\u2019ll","","","","","","","","cluster,","","","","","","cluster. So next,","cluster. So next,","cluster. So next,","","","","","","","weights. And","weights. And","","","","","","","","","","","black. And again,","black. And again,","black. And again,","","","","","mean. And","mean. And","","","","","","","","times. In","times. In","","3","3","3","","5","","","","sufficient. This","sufficient. This","","","you,","","suppose,","","","","","","","","","it. Okay,","it. Okay,","","","","","","","","","agreement. Now let\u2019s","agreement. Now let\u2019s","agreement. Now let\u2019s","","","","","","","","","","","details. First,","details. First,","","","","output,","","","","","","","","","b_i,j","","","0. Next,","0. Next,","","","","","","","","","","","weights,","","","","capsule. This","capsule. This","","","","","","","","","output,","","","","","each. Next","each. Next","each. Next","each. Next","","","","","","","","predictions,","","","","","","","layer. This","layer. This","","","","","","1,","","","","","","","","function. And voil\u00e0! We","function. And voil\u00e0! We","function. And voil\u00e0! We","function. And voil\u00e0! We","","","","","","","","house-capsule","house-capsule","","boat-capsule. But","boat-capsule. But","boat-capsule. But","","","","","","output, it\u2019s","output, it\u2019s","","","","","","","round,","","","iteration. Now","iteration. Now","","","","","","","","accurate. For example,","accurate. For example,","accurate. For example,","","rectangle-capsule","rectangle-capsule","","","","","","","boat-capsule\u2019s output. It","boat-capsule\u2019s output. It","boat-capsule\u2019s output. It","boat-capsule\u2019s output. It","","","","","closely. This","closely. This","","","","","","","","","","","","","\u00fb_j|i","\u00fb_j|i","\u00fb_j|i","\u00fb_j|i","","","","","","v_j. This","v_j. This","v_j. This","","","","","","","","","output\u2019s","","","weight, b_i,j. So","weight, b_i,j. So","weight, b_i,j. So","","","","","","","","","increased. When","increased. When","","","","","agreement, this","agreement, this","","","","large,","large,","large,","large,","","","","","","","","weight. On","weight. On","","","hand,","","rectangle-capsule","rectangle-capsule","","","","","","","","house-capsule\u2019s output,","house-capsule\u2019s output,","house-capsule\u2019s output,","","","","","","","","","","","small,","","","","","weight","","this","","","","","","much. Next,","much. Next,","","","","","","","","","","","","","weights,","","again. And","again. And","","","","see,","","rectangle-capsule\u2019s","rectangle-capsule\u2019s","","","","","boat-capsule","boat-capsule","","","","","","0.8,","","it\u2019s","","","","","house-capsule","house-capsule","","","","0.2. So","0.2. So","","","","","","now going to go to the boat capsule,","","","","","","","","","","","","capsule. Once","capsule. Once","","","","","","","","","","","output","","","","","","","","layer,","","","","house-capsule","house-capsule","house-capsule","house-capsule","house-capsule","house-capsule","","the boat-capsule. And","","time,","","house-capsule","house-capsule","","","","","","","","","","","vector. On","vector. On","","","","","boat-capsule","boat-capsule","","","","","","","","","","","","","","","","1. So","1. So","","","","it. And that\u2019s","it. And that\u2019s","it. And that\u2019s","","","","","#2. And","#2. And","","","","see,","","","","","iterations,","","","","","","","","","","","","boat. After","boat. After","","","","","","rounds,","","","","","","","","","","","","","","","way. So","way. So","","","","earlier,","","","","","","","","","","scenes,","","","","","","","","image. One way to interpret this","","(as","(as","(as","(as","(as","(as","(as","","","","there is","","","","","ambiguity),","","","","","","","","","","middle. However,","middle. However,","","","","","case,","","","","","","","","","","","","","","triangle,","","","","","","","","","are. The","are. The","","","","","","","","","","","","","","","","","","","","","bottom. And","bottom. And","","","","","","","","","solution,","","","","","","","","happy,","","","","","","","","","","","","","layer. The","layer. The","","","","away. Okay,","away. Okay,","","","","","","","","","","","","","","","","works. Well","works. Well","","one,","","","","","","","","","course. Just","course. Just","","","","","","","","","","","","that\u2019s","","","","","","it. All","it. All","","","","","","","","","","","","","","top-layer","top-layer","","vectors,","","","","","","","","probabilities. You","probabilities. You","","","","","","","","","","cross-entropy loss, as in a regular","cross-entropy loss, as in a regular","cross-entropy loss, as in a regular","cross-entropy loss, as in a regular","cross-entropy loss, as in a regular","cross-entropy loss, as in a regular","","","network,","","","would","would","","done. However,","done. However,","","","","","","","","","","","","","","","","","","","image. So","image. So","","","","","","details,","","","","","","","","","","","","k","","","","","image,","","","","","","","output","output","","","","","","","","","0.9. It","0.9. It","","","long. Conversely,","long. Conversely,","","","","","","k","","","","","","image,","","","","","","","","vector,","","","","","","","","0.1. So","0.1. So","","","","","","","","","","","classes. In","classes. In","","paper,","","","","","","","","","","","","network. It\u2019s","network. It\u2019s","","3 fully","3 fully","3 fully","3 fully","3 fully","","","","","","","","","","","","","layer. It","layer. It","layer. It","","","","","","","","","","","","","","","","","","","image. The","image. The","","","","the","","","","","earlier,","","","","","(scaled","","","","","","","","","","","","training). The","training). The","","","","","","","","","","","","","","","","","","","","","","","","image,","","","","","","","","","network,","","","layer. This","layer. This","","","","","","","regularizer:","","","","","","","","","","","","examples. And that\u2019s it,","examples. And that\u2019s it,","examples. And that\u2019s it,","examples. And that\u2019s it,","","","","","","network works,","network works,","","","","","it. Let\u2019s","it. Let\u2019s","","","","","","","","","","","","paper,","","","","interesting. This","interesting. This","interesting. This","","","1 from the paper,","","","","","","","MNIST. You","MNIST. You","MNIST. You","","","","","","","","","layers,","","","","","","","","","","","","","","","capsules. And","capsules. And","","","","","","","","6","","6 grid,","6 grid,","","","","","","","","","","grid,","","","","","","","8-dimensional vector. So","8-dimensional vector. So","8-dimensional vector. So","8-dimensional vector. So","","","","","","","","","","","10","","capsules,","","","16","","vectors. The","vectors. The","","","","","","","","","","","","","loss,","","","earlier. Now","earlier. Now","","","","2","","","paper. It","paper. It","","","","","","","","","capsnet. It","capsnet. It","capsnet. It","","","","2","","","ReLU","","","","","","","","","","","","","","","","","","","","","","","(which","","a","","","","","image). The","","","","","","","","","","","","","","loss. Right,","loss. Right,","","","","","4","","","paper. One","paper. One","paper. One","paper. One","","","","","","","","","","","","","interpretable. For example,","interpretable. For example,","interpretable. For example,","interpretable. For example,","","","","","","","","","","","","","","","","","","","","","","capsules\u2019 output. You","capsules\u2019 output. You","capsules\u2019 output. You","","","","","","","","","","","","","","thickness. The","thickness. The","","","","","","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","skew. The","","","","","","","","","","","","","","","","position. So","position. So","","","","see, it\u2019s","see, it\u2019s","","","","","","","","do. Okay,","do. Okay,","","conclude, let\u2019s","conclude, let\u2019s","","","","","cons. Capsule","cons. Capsule","","","","state of the art","","","MNIST. On CIFAR10,","MNIST. On CIFAR10,","MNIST. On CIFAR10,","MNIST. On CIFAR10,","MNIST. On CIFAR10,","MNIST. On CIFAR10,","MNIST. On CIFAR10,","","","","","","10% error,","10% error,","10% error,","","","","","state of the art,","state of the art,","","it\u2019s similar to","it\u2019s similar to","","","","","","","","","","","","","","","","","them,","","it\u2019s","","","","start. Capsule","start. Capsule","","","","","data. They","data. They","","equivariance,","","","","","","","","","preserved. And","preserved. And","","","","","","","","","","detection. The","detection. The","","","","","","","","","scenes. The","scenes. The","","","","","","","","","parts,","","","","","assigned","","","whole. And it\u2019s","whole. And it\u2019s","whole. And it\u2019s","","","","rotations,","","","","affine transformations. The","affine transformations. The","affine transformations. The","","","somewhat","","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","interpretable. And finally, obviously, it\u2019s Hinton\u2019s idea,","","don\u2019t","","","it. However,","it. However,","","","","","cons: first,","cons: first,","","","","","","","","","state of the art","","CIFAR10,","CIFAR10,","","","it\u2019s","","","start. Plus, it\u2019s","start. Plus, it\u2019s","start. Plus, it\u2019s","","","","","","","","","","images,","","","","ImageNet dataset. What","ImageNet dataset. What","ImageNet dataset. What","ImageNet dataset. What","ImageNet dataset. What","ImageNet dataset. What","ImageNet dataset. What","","","","be? Capsule","be? Capsule","","","","","","","train,","","","","","","","","","","","","","","","loop,","","","","earlier. Finally,","earlier. Finally,","","","","","","","","given","","","","","location,","","it\u2019s","","","","","","","","","","","","","","","","","","","","","another. This","another. This","","","crowding,","","it has","","","","","","","well,","","it\u2019s","","","","show-stopper. All right!","show-stopper. All right!","","","","","","","","","","","","","CapsNet implementation,","CapsNet implementation,","CapsNet implementation,","","","","","","","(I\u2019ll","","","","","","","","below). If","below). If","","","","time,","","","","","","","","","","","doing. The","doing. The","","","","","CapsNets","CapsNets","","","","","","","","","","","","","algorithm. Implementing","algorithm. Implementing","","","Keras","","TensorFlow","","","","","","","","","PyTorch,","PyTorch,","","","","","done. If","done. If","done. If","","don\u2019t","","","","preference,","","","","","","","PyTorch","PyTorch","","","","","","","understand. And that\u2019s","understand. And that\u2019s","understand. And that\u2019s","","","had,","","","","","","video. If","video. If","","did,","","","up, share, comment, subscribe, blablabla. It\u2019s","up, share, comment, subscribe, blablabla. It\u2019s","up, share, comment, subscribe, blablabla. It\u2019s","up, share, comment, subscribe, blablabla. It\u2019s","up, share, comment, subscribe, blablabla. It\u2019s","up, share, comment, subscribe, blablabla. It\u2019s","up, share, comment, subscribe, blablabla. It\u2019s","up, share, comment, subscribe, blablabla. It\u2019s","","","","","video,","","","","","","useful, I might","useful, I might","","","more. If","more. If","","","","","","","Machine Learning, Deep Learning and Deep Reinforcement Learning,","Machine Learning, Deep Learning and Deep Reinforcement Learning,","Machine Learning, Deep Learning and Deep Reinforcement Learning,","Machine Learning, Deep Learning and Deep Reinforcement Learning,","Machine Learning, Deep Learning and Deep Reinforcement Learning,","Machine Learning, Deep Learning and Deep Reinforcement Learning,","Machine Learning, Deep Learning and Deep Reinforcement Learning,","Machine Learning, Deep Learning and Deep Reinforcement Learning,","","","","","","","O\u2019Reilly","","Hands-on Machine Learning","Hands-on Machine Learning","Hands-on Machine Learning","Hands-on Machine Learning","","Scikit-Learn","","TensorFlow. It","TensorFlow. It","","","","","topics,","","","","","","","","","","","","account,","","I\u2019ll","","","","","","","description. That\u2019s","description. That\u2019s","","","today,","","","","","","","time!"],"2Kawrd5szHE":["Hi, I\u2019m Aur\u00e9lien G\u00e9ron,","Hi, I\u2019m Aur\u00e9lien G\u00e9ron,","Hi, I\u2019m Aur\u00e9lien G\u00e9ron,","Hi, I\u2019m Aur\u00e9lien G\u00e9ron,","","","I\u2019m","","","","","","","","","","","","TensorFlow. In","TensorFlow. In","TensorFlow. In","","","video,","","","","","","","","networks,","","","","","","architecture. If","architecture. If","","haven\u2019t","","","video,","","","","","","","now,","","","","","","","TensorFlow implementation.","TensorFlow implementation.","TensorFlow implementation.","","","","Jupyter","","","","","","","","explanations,","","I","","","","","","","(as","","I\u2019ll","","","","","","","","","below),","","","","","","","it,","","","","it. So,","it. So,","","","","99.4%","99.4%","99.4%","","","","","set,","","","","good,","","it\u2019s","","","","","","","","","","","","","","1,200 capsules. There\u2019s","1,200 capsules. There\u2019s","1,200 capsules. There\u2019s","","","","","","","","","notebook,","","","won\u2019t","","","","","","","","video,","","I\u2019ll","","","","","","","across,","","","","","","","","","","","TensorFlow implementations,","TensorFlow implementations,","TensorFlow implementations,","","","CapsNets. Okay, let\u2019s","CapsNets. Okay, let\u2019s","CapsNets. Okay, let\u2019s","CapsNets. Okay, let\u2019s","","the network. First,","the network. First,","the network. First,","","","","","","","","","","network. And that\u2019s","network. And that\u2019s","network. And that\u2019s","","","layer. We","layer. We","","","","","","TensorFlow placeholder. The","TensorFlow placeholder. The","TensorFlow placeholder. The","TensorFlow placeholder. The","","","","unspecified,","","","","","","","","","","","","batch,","","","example, 5. Note","example, 5. Note","example, 5. Note","","","","","28x28","28x28","28x28","28x28","","images,","","","","channel,","","","","","greyscale. Color","greyscale. Color","","","","","","channels,","","red,","","","blue. And that\u2019s","blue. And that\u2019s","blue. And that\u2019s","","","","","layer. Next, let\u2019s","layer. Next, let\u2019s","layer. Next, let\u2019s","","","","","layer. For","layer. For","","","","","","","","","","maps,","","","","6x6","6x6","6x6","","","","","vectors. The","vectors. The","","","","","","","","","","","","","","segment. You","segment. You","segment. You","","","","","","","","","","","","","there\u2019s","","","","","line. And","line. And","","","","","8D","","","","","parameters,","","","case, I\u2019ve","case, I\u2019ve","","","","angle,","","","vector\u2019s 8","vector\u2019s 8","","","","","","","","","","","","line,","","","","","","","","","","","","","","","","6x6 grid,","6x6 grid,","6x6 grid,","6x6 grid,","","","on. The","","","","straightforward. First,","straightforward. First,","","","","","","layers. The","layers. The","","","","","","","X,","","","","","","","","","","","","","runtime. The","runtime. The","","","","","","","","","layer. And","layer. And","layer. And","layer. And","","","","","","","","paper. The","paper. The","","","","","","","","","maps. And","","","","","a","","","","scalars. We","scalars. We","","","","","","","","instead,","","","","TensorFlow\u2019s reshape()","TensorFlow\u2019s reshape()","TensorFlow\u2019s reshape()","","","","","maps","","8","","vectors,","","","","","","scalars. In fact,","scalars. In fact,","scalars. In fact,","","","","","","","","","","","","capsules,","","","","","","","","","","1,152","","","(that\u2019s 32*6*6),","(that\u2019s 32*6*6),","(that\u2019s 32*6*6),","(that\u2019s 32*6*6),","(that\u2019s 32*6*6),","(that\u2019s 32*6*6),","(that\u2019s 32*6*6),","","","","","","batch. And","batch. And","","","","","","","","","","","","","","","","","0","","1. For this,","1. For this,","1. For this,","","","","home made","","function. There","function. There","","is. This","is. This","","","","","","","","","paper. It","paper. It","","","","","","array,","","","","dimension,","","","","","one. So,","one. So,","","","","see,","","","","","","","","","","vector,","","there\u2019s","","","","","","","","","","","","","","","","","","vector. So","vector. So","","","","","","","","","","","denominator,","","","","","","","","","problem. However","problem. However","","","","","","","issue. The","issue. The","","","","","","","","","","","","","zero. So","zero. So","","","","","tensorflow\u2019s norm() function to compute the","tensorflow\u2019s norm() function to compute the","","","","","","","","","equation,","","","","","","","","","","zero,","","","","","","(it","","","n-a-n, nan,","n-a-n, nan,","n-a-n, nan,","n-a-n, nan,","","a number). So,","","","result,","","gradient","gradient","","","","","","","model,","","","","","","","","","well. The","well. The","","","","","dead. You don\u2019t","dead. You don\u2019t","dead. You don\u2019t","dead. You don\u2019t","","that. So","that. So","","","","","","","","","","","norm,","","","","","","","right. And,","right. And,","","","it, that\u2019s","it, that\u2019s","","","","","capsules. Apart","capsules. Apart","","","","","safely,","","","","straightforward. On","straightforward. On","straightforward. On","","","","","","","","","is:","","","capsules. There","capsules. There","","","10","","them,","","","","digit,","","","9,","","they","","","","vectors. In","vectors. In","","","example,","","","","","","","","","","","","","","4. And","4. And","","its","","","","","","","","","","","","","","digit,","","rotation,","","thickness,","","skew,","","position,","","","on. By","on. By","","way,","","","","","","","","","","","","","","","","","","","","","","","","grid. So,","grid. So,","","example,","","","","","","","","","","","","","","","","","","","","","","","","activated. See? So,","activated. See? So,","activated. See? So,","","","","","","","","","contain","contain","","","information,","","","","","","","","","","","grid. But","grid. But","","","","","layer,","","","","","","","","","","","","","","","","","","space. Okay,","space. Okay,","","let\u2019s","","","","","","layer. The","layer. The","","","","","","","","","vectors. Since","vectors. Since","","","","","","","","","","layer,","","","","","","","","","","","","","","","capsules. For example,","capsules. For example,","capsules. For example,","","","","","","","","capsule,","","","","","","","","","","","capsule. For this,","capsule. For this,","capsule. For this,","","","","","","","W_1,1,","","","","","","","training,","","","","","","","","","","","","capsule. This","capsule. This","","","\u00fb_1|1,","\u00fb_1|1,","\u00fb_1|1,","\u00fb_1|1,","","","","","","","","","","capsule,","","","","","","","","","capsule. Since","capsule. Since","","","","","8","","vectors,","","","","","","16","","vectors,","","","","W_1,1","","","","16x8 matrix. Next,","16x8 matrix. Next,","16x8 matrix. Next,","16x8 matrix. Next,","16x8 matrix. Next,","","","","","","","","","","","capsule,","","","","","","","","","","capsule. Note","capsule. Note","","","","","","","","matrix, W_1,2. And","matrix, W_1,2. And","matrix, W_1,2. And","","","","","","","","digit capsule,","","W_1,3. And","W_1,3. And","W_1,3. And","W_1,3. And","","","","","","","capsules. Then","capsules. Then","","","","","","","","capsule,","","","","","","","","","","","","","","capsule. And","capsule. And","","","","","","","capsules. Then","capsules. Then","","","","","","","","capsule,","","","10 predictions. And","10 predictions. And","10 predictions. And","","on,","","","","picture. There","picture. There","","1,152","","","(multiply 6 * 6 * 32),","(multiply 6 * 6 * 32),","(multiply 6 * 6 * 32),","(multiply 6 * 6 * 32),","(multiply 6 * 6 * 32),","(multiply 6 * 6 * 32),","","10","","capsules,","","","","","","11,520","11,520","11,520","11,520","11,520","11,520","","","vectors. Now","vectors. Now","","","","","","","","one,","","","","","","inefficient. Let\u2019s","inefficient. Let\u2019s","inefficient. Let\u2019s","","","","","","","","","","","","","","matmul() operation. Now","matmul() operation. Now","matmul() operation. Now","matmul() operation. Now","","","","TensorFlow\u2019s matmul()","TensorFlow\u2019s matmul()","TensorFlow\u2019s matmul()","","","","","","matrices,","","","","","","","","","","","","","","","","","","shot. This","shot. This","","","","efficient,","","","","","","","","card,","","","","","","","","","","","","","","","threads. So here\u2019s","threads. So here\u2019s","threads. So here\u2019s","","","works. Suppose A, B, C, D, E, F","works. Suppose A, B, C, D, E, F","works. Suppose A, B, C, D, E, F","works. Suppose A, B, C, D, E, F","works. Suppose A, B, C, D, E, F","works. Suppose A, B, C, D, E, F","works. Suppose A, B, C, D, E, F","works. Suppose A, B, C, D, E, F","","G, H, I, J, K, L,","G, H, I, J, K, L,","G, H, I, J, K, L,","G, H, I, J, K, L,","G, H, I, J, K, L,","G, H, I, J, K, L,","","","matrices. You","matrices. You","","","","","","","arrays,","","","2","","","3 columns,","3 columns,","","example. So","example. So","","","2","","","","","","","matrices,","","","","","2 dimensional,","","","","","2+2=4","2+2=4","2+2=4","2+2=4","2+2=4","","arrays. If","arrays. If","","","","arrays,","","4D arrays, to matmul(),","4D arrays, to matmul(),","4D arrays, to matmul(),","4D arrays, to matmul(),","4D arrays, to matmul(),","","","","","elementwise","","multiplication,","","","","","","","4","","","","A multiplied by G, here,","A multiplied by G, here,","A multiplied by G, here,","A multiplied by G, here,","A multiplied by G, here,","","","","H, here,","H, here,","","","on. So let\u2019s","on. So let\u2019s","on. So let\u2019s","","","","","","","","","vectors. We","vectors. We","","","a","","4D","","","","","","matrices: there\u2019s","matrices: there\u2019s","","","","","capsule,","","","","","","capsule. The","capsule. The","","","","","","","","","","","capsule. Then","capsule. Then","","","","","","","","","matmul() function,","matmul() function,","","","","","","","","","","","","","","","","","capsules. Since","capsules. Since","","","","","","","","","10 digit","","","","","capsule,","","","","","10","","","","","capsules\u2019 outputs. We","capsules\u2019 outputs. We","capsules\u2019 outputs. We","","","","","","","","","","","","","vectors, 10 times. But there\u2019s","vectors, 10 times. But there\u2019s","vectors, 10 times. But there\u2019s","vectors, 10 times. But there\u2019s","vectors, 10 times. But there\u2019s","","","catch. We","catch. We","","","","","","","","","","","","batch,","","","","instance. So there\u2019s","instance. So there\u2019s","instance. So there\u2019s","","","","","","","size. It","size. It","","","","","","","","","","","","","","instance,","","","","","","fine. It","fine. It","","","","dimension. But","dimension. But","","","","","","4D","","","","","","matrices,","","","","","","","","","","","","","batch. Now","batch. Now","","","","this,","","","","","","","clear. First,","clear. First,","","","","","","","","","matrices. It","matrices. It","","","","","","","capsule,","","","","","capsule,","capsule,","","","","","","","matrices. That\u2019s 4 dimensions,","matrices. That\u2019s 4 dimensions,","matrices. That\u2019s 4 dimensions,","matrices. That\u2019s 4 dimensions,","","","","","","","","","","","","at the beginning","","","","","","","","","","","","","","batch. The","batch. The","batch. The","","","","randomly,","","","","","","","","","(that\u2019s","","hyperparameter","hyperparameter","","","tweak). And","tweak). And","","","it. Create","it. Create","it. Create","","variable! Next","variable! Next","","","","","","","","","instance,","","","","","","","","","size. We don\u2019t","size. We don\u2019t","size. We don\u2019t","","","","","","","time,","","","","","","","","","","graph. But","graph. But","","","","TensorFlow\u2019s shape() function:","TensorFlow\u2019s shape() function:","TensorFlow\u2019s shape() function:","TensorFlow\u2019s shape() function:","","","","","","*will*","","","","","runtime,","runtime,","","","","","","dimension,","","","","","size. Then","size. Then","","","","","","","array,","","","","dimension,","","","","","","instance. Now,","instance. Now,","","","","","","","","","","","3","","array:","","","","","","","size,","","","","","","runtime,","","there\u2019s","","","","capsule,","","","","","8 dimensions. So","8 dimensions. So","8 dimensions. So","","","","","","","","","","","","","","","","","for,","for,","","","","","matmul() operation. First","matmul() operation. First","matmul() operation. First","matmul() operation. First","","","","","","","","end,","","TensorFlow\u2019s expand_dims() function. The","TensorFlow\u2019s expand_dims() function. The","TensorFlow\u2019s expand_dims() function. The","TensorFlow\u2019s expand_dims() function. The","TensorFlow\u2019s expand_dims() function. The","TensorFlow\u2019s expand_dims() function. The","","","","","","","vectors,","","","1","","arrays. Each","arrays. Each","","","","","column vector. A","","","","","","","matrix,","","2D array,","2D array,","","","","column. Then","column. Then","","","","dimension,","","","","capsules. And","capsules. And","","","","","","","10","","","","","dimension,","","","","capsule. And lastly,","capsule. And lastly,","capsule. And lastly,","","","","matmul","matmul","","","","","","","","","capsules\u2019","","vectors,","","","","","","","capsule\u2019s","","","","","","","","","","capsules,","","","","","","","batch. In","batch. In","","shot. And that\u2019s","shot. And that\u2019s","shot. And that\u2019s","","","","","","","","","","","capsules\u2019 outputs,","capsules\u2019 outputs,","","","","","","","","","vectors. The","vectors. The","","","","","","","","algorithm. So first,","algorithm. So first,","algorithm. So first,","","","","","raw","","","","0. For this,","0. For this,","0. For this,","","","","tf.zeros(). There\u2019s","tf.zeros(). There\u2019s","tf.zeros(). There\u2019s","tf.zeros(). There\u2019s","","","","","","","","","digits capsules,","digits capsules,","digits capsules,","","","instance. The","instance. The","","","","","","","","1,","","","","","","","minute. Next","minute. Next","","","","","","","","capsule\u2019s","","","","weights. Okay? So","weights. Okay? So","weights. Okay? So","","","","","dimension. Next,","dimension. Next,","","","","","","","","","","","vectors,","","","","capsule,","","the routing weights. The weighted sum is along this dimension. This is pretty straightforward TensorFlow code: first multiply","","","","","","","","","","","","","","","","","","","","","","","","","","","(this","","an elementwise multiplication,","an elementwise multiplication,","an elementwise multiplication,","","","","multiplication),","","","","","","","","","","dimension. And","dimension. And","","","","","","","","","","","","","","","","step,","","","","","","","","","","","dimensions,","","","rank. They don\u2019t","rank. They don\u2019t","rank. They don\u2019t","","","","","shape,","","","","","","","TensorFlow","","","broadcasting. Now","broadcasting. Now","","","","","","","is,","","","","","clear. I\u2019m","clear. I\u2019m","","","matrices,","","","","","","","","row,","","TensorFlow","","","","","","","","","","","","","times. You","times. You","","","","","","","tiling,","","","","earlier,","","","","","efficient. You","efficient. You","efficient. You","","","","","didn\u2019t","","","earlier,","","","","","","","","","","","multiplication. Here we are","multiplication. Here we are","multiplication. Here we are","","elementwise multiplication. Okay,","elementwise multiplication. Okay,","elementwise multiplication. Okay,","elementwise multiplication. Okay,","","","","","capsules. We","capsules. We","","","","","","","","vectors,","","","","capsule,","","","","","","","function,","","","","","","","","","capsules. Hurray! But wait,","capsules. Hurray! But wait,","capsules. Hurray! But wait,","capsules. Hurray! But wait,","capsules. Hurray! But wait,","","","","","","","","1","","","","","","algorithm. Now,","algorithm. Now,","","","","#2. So first,","#2. So first,","#2. So first,","","","","","","","","","was,","","","","","","","","weights. For example,","weights. For example,","weights. For example,","","","","","","","","","","","capsule\u2019s output. Notice that,","capsule\u2019s output. Notice that,","capsule\u2019s output. Notice that,","capsule\u2019s output. Notice that,","","example,","","","","","4,","","excellent. And","excellent. And","","","","","","","","","","","","","","","","","vector. These","vector. These","","","","","","","","vectors,","","","","","","","column. So","column. So","","","","","product,","","","","","","","","\u00fb_j|i","\u00fb_j|i","\u00fb_j|i","\u00fb_j|i","","","","","vector,","","","","","","","","","","","","v_j,","v_j,","","","","","vector. We","vector. We","","","","1x1","1x1","1x1","","","","","","","","vectors. Now","vectors. Now","vectors. Now","","","","","","","","","","","vector,","","","again,","","","","","matmul()","matmul()","","","","","","","","","","","shot. First","shot. First","","","","","tile()","","","","","","","","","","","v_j,","v_j,","","","","capsule. Then","capsule. Then","","","matmul(),","matmul(),","","","","","","","","","","","array,","","","fly,","","lo-and-behold,","lo-and-behold,","lo-and-behold,","","","","","","","","once. So now,","once. So now,","once. So now,","","","","","","the agreement","","","","","","","","","vector. We","vector. We","","","","","","","","","","weights,","","","","addition. And","addition. And","","","","","","","","","","","","","","","1. The","1. The","","","","identical,","","we\u2019re","","","","","","weights","","","2. We","2. We","","","","","","","","","","","","2,","","","","","","","","","","","","","","","capsule,","","","","","","result. And","result. And","","","","","","","","outputs,","","we\u2019ve finished","we\u2019ve finished","","2. We","2. We","","","","","","","","","","one,","","I\u2019ll","","now,","","","","","","","","","","","","","","","","","","","capsules. Now","capsules. Now","","","","","","","","","algorithm\u2019s","","","","","loop. It\u2019s","loop. It\u2019s","","","","","","","","","","","","100,","","","code. Of course,","code. Of course,","code. Of course,","","","","","","","TensorFlow graph. But","TensorFlow graph. But","TensorFlow graph. But","TensorFlow graph. But","","works. You","works. You","","","","","","","","loop. Now, a","loop. Now, a","","","","","","","","","","","","","","","Python,","","this. Ah,","","better. However, it\u2019s","better. However, it\u2019s","better. However, it\u2019s","","","","","","","TensorFlow graph","TensorFlow graph","","","","","","","","","","","","code. All we are","code. All we are","code. All we are","","","","","","graph,","","TensorFlow","","","","","","","used a","used a","","","","the graph. Again,","the graph. Again,","the graph. Again,","","","fine, it\u2019s","fine, it\u2019s","","","","","","","","","","graph. So","graph. So","","","","","","","","","","loop,","","","","","","","time. If","time. If","","","","","loop,","","","TensorFlow","","","run,","","","","","TensorFlow\u2019s while_loop()","TensorFlow\u2019s while_loop()","TensorFlow\u2019s while_loop()","TensorFlow\u2019s while_loop()","","","this. The while_loop()","this. The while_loop()","this. The while_loop()","this. The while_loop()","","","3 parameters:","3 parameters:","","","","","","","","","","","","","","","","","","","","","","not,","","","iteration. The","iteration. The","","","","","","","","","","","","","","loop,","loop,","","","","","","","","iteration. And finally, the","iteration. And finally, the","iteration. And finally, the","","","","","","","","","","","","","","","condition()","","loop_body() functions","loop_body() functions","loop_body() functions","","","","iteration. For","iteration. For","","","iterations,","","","","","","","","","loop_body() function. So","loop_body() function. So","loop_body() function. So","loop_body() function. So","","","","","","","","","","","","","","","","code. Once","code. Once","","","it,","","","","","","capsnet","capsnet","","","","","","","","","","","","loop. Apart","loop. Apart","","","","","","","","","smaller,","","","","","","","","","","","","","","","","","model. Also,","model. Also,","","","","","swap_memory","swap_memory","","","","while_loop() function,","while_loop() function,","while_loop() function,","","","","","","True, TensorFlow","True, TensorFlow","","","","","","","","","","","","","","","","memory. Since","memory. Since","","RAM","","","","","abundant,","","","","","useful. And that\u2019s it! We\u2019ve","useful. And that\u2019s it! We\u2019ve","useful. And that\u2019s it! We\u2019ve","useful. And that\u2019s it! We\u2019ve","useful. And that\u2019s it! We\u2019ve","","","","","","","capsules. Cool! Now","capsules. Cool! Now","capsules. Cool! Now","","","","","","","","","","","","","","","","","","","","image. So, let\u2019s","image. So, let\u2019s","image. So, let\u2019s","","","probabilities. For this,","probabilities. For this,","probabilities. For this,","","can\u2019t","","tensorflow\u2019s norm()","tensorflow\u2019s norm()","tensorflow\u2019s norm()","","","","","","","there\u2019s","","","","","","point,","","","mentionned earlier. So","mentionned earlier. So","mentionned earlier. So","","","use","","home-made safe_norm() function,","home-made safe_norm() function,","home-made safe_norm() function,","home-made safe_norm() function,","","","","","","","","squash() function. And","squash() function. And","squash() function. And","","","","","","","","don\u2019t","","","","","1,","","","","","","","softmax layer. This","softmax layer. This","softmax layer. This","softmax layer. This","","","","","","","","","","","","","(but","","","","","","","digits:","digits:","","","","","","","3,","3,","","","can\u2019t detect, say,","can\u2019t detect, say,","can\u2019t detect, say,","","5s). Next, let\u2019s","5s). Next, let\u2019s","5s). Next, let\u2019s","","","","","digit. We","digit. We","","use","","argmax()","argmax()","","","","use","","","","","","probability. The","probability. The","","","","","","number,","number,","","","itself. Note","itself. Note","","","","","","","","","","","","","","","1","","","end,","","","","","","","","","squeeze() function. If","squeeze() function. If","squeeze() function. If","","","squeeze()","","","","","","remove,","","","","","","","","1. This","1. This","","","","okay,","","","","","","","","","one,","","","case,","","","","","","","","value,","","","","array,","","","don\u2019t","","that. So","that. So","","","","","","axes. Great,","axes. Great,","","","","","","","","","","","","","","predictions. We","predictions. We","","","","model\u2019s","","","","","","","","","","","","labels. In","labels. In","","","","","","","","","","","","","wrong, it\u2019s 7","wrong, it\u2019s 7","wrong, it\u2019s 7","","","1. So","1. So","","","","accuracy. The","accuracy. The","accuracy. The","","","","straightforward:","","","","","equal()","","","","","","","","","y_pred,","y_pred,","","","","","","","","booleans,","booleans,","","","","","booleans to floats,","booleans to floats,","booleans to floats,","","","","","","","0s (for","0s (for","","predictions)","","1s (for","1s (for","","predictions),","","","","","","","","","batch\u2019s accuracy. The","batch\u2019s accuracy. The","batch\u2019s accuracy. The","","y","","","","","placeholder. Nothing special. And that\u2019s it,","placeholder. Nothing special. And that\u2019s it,","placeholder. Nothing special. And that\u2019s it,","placeholder. Nothing special. And that\u2019s it,","placeholder. Nothing special. And that\u2019s it,","placeholder. Nothing special. And that\u2019s it,","","","","","model,","","","","predictions. Now let\u2019s","predictions. Now let\u2019s","predictions. Now let\u2019s","","","","","code. This","code. This","","","","to","","","","","I\u2019ll","","","","","clarity. And now, first,","clarity. And now, first,","clarity. And now, first,","clarity. And now, first,","","","","","","","loss. It\u2019s","loss. It\u2019s","","","","equation. By","equation. By","","way,","","","","","","","","video:","","","the","","","","","","","operations. Sorry","operations. Sorry","","that. This","that. This","","","","","equation. Computing","equation. Computing","","","","straightforward,","","","won\u2019t","","","","","details. The","details. The","","","","","","","","","","","","","T_k values. For","T_k values. For","T_k values. For","","","instance, T_k","instance, T_k","","","","","","","","","class k","class k","","","","","image,","","it\u2019s","","","0. So,","","","","","","T_k","","","","","","","","","","","a one-hot representation. For example,","a one-hot representation. For example,","a one-hot representation. For example,","a one-hot representation. For example,","a one-hot representation. For example,","","","instance\u2019s","","","3,","","","","instance,","","","","","","","","","","","","","","","at","","3. Okay! Next,","3. Okay! Next,","3. Okay! Next,","","","","","","","loss. So first,","loss. So first,","loss. So first,","","","","","","","","","","","","","","","","","","","","","","","images. This","images. This","","","","","","feedforward","","","","","3","","","layers. It\u2019s","layers. It\u2019s","","","code,","","","","","","","","","","to","","","","","","it. It","it. It","","","","","","","","","","1,","","","instance,","","","","","","28x28","28x28","28x28","","images. And that\u2019s it,","images. And that\u2019s it,","images. And that\u2019s it,","images. And that\u2019s it,","","","","","images! We","images! We","","","","","","loss. This","loss. This","","","","","","","","","","","their reconstructions. Since","their reconstructions. Since","their reconstructions. Since","","","","","28x28x1,","28x28x1,","28x28x1,","28x28x1,","28x28x1,","","","","","","","","","","","","","each. Then","each. Then","","","","","difference. Now","difference. Now","","","","","","loss! It\u2019s","loss! It\u2019s","","","","","","","","","","","loss, scaled","loss, scaled","","","","","","","","training. Pretty simple, as you can see. Now let\u2019s add the","","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","operation. The","","","","used TensorFlow\u2019s","used TensorFlow\u2019s","used TensorFlow\u2019s","","","","Adam optimizer,","Adam optimizer,","","","","parameters,","","let\u2019s","","that. We","that. We","","","optimizer,","","","its minimize()","its minimize()","","","","","","","","","","","","","","","","loss. We\u2019re","loss. We\u2019re","","done,","","there\u2019s","","","","","didn\u2019t","","","","","video. The","video. The","","","","","","","","","","","","","","","","","","","","","","","digit. So instead of sending the","","capsules\u2019","capsules\u2019","capsules\u2019","capsules\u2019","capsules\u2019","capsules\u2019","capsules\u2019","","","","","decoder,","","","","","","","first,","","this. The","this. The","","","","","","","","","","","","array,","","","","","","","0","","","","1s","","","","","","","digits. By","digits. By","","","","capsules\u2019","","and","","mask,","","","","","","","decoder. But there\u2019s","decoder. But there\u2019s","decoder. But there\u2019s","","catch. This","catch. This","","","","","training,","","","","time,","","won\u2019t","","","labels. So instead,","labels. So instead,","labels. So instead,","","","mask","","","","","","","","","","the labels,","","this. Now","this. Now","","","","","","","","","","","testing,","","","wouldn\u2019t","","","convenient. So instead, let\u2019s","convenient. So instead, let\u2019s","convenient. So instead, let\u2019s","convenient. So instead, let\u2019s","","","condition operation. We","condition operation. We","condition operation. We","","","","","","","","mask_with_labels. If it is true, then we use the","mask_with_labels. If it is true, then we use the","","to build the mask. If","","","False,","","","","","prediction. Note the difference. Okay. And here\u2019s the code. We","prediction. Note the difference. Okay. And here\u2019s the code. We","","","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","mask_with_labels placeholder,","","","","","False","","","","","","","","","","training. And","training. And","","","","","","","","TensorFlow\u2019s cond() function. It","TensorFlow\u2019s cond() function. It","TensorFlow\u2019s cond() function. It","TensorFlow\u2019s cond() function. It","","3 arguments:","3 arguments:","","","","","","","","","condition,","","","","","","mask_with_labels placeholder. The","mask_with_labels placeholder. The","mask_with_labels placeholder. The","mask_with_labels placeholder. The","mask_with_labels placeholder. The","","","","","","","","","","","","","","","","True,","","","","","","","","","","","","","","if","","","if False. Then","if False. Then","if False. Then","","","","mask,","","","","","one_hot() function. Now there","one_hot() function. Now there","one_hot() function. Now there","one_hot() function. Now there","","","","","","","implementation,","","","","it,","","","","","","","","","","","","","TensorFlow","","a tensor. Suppose","","","","graph,","","","","","operations,","","","","","","","","","","A. The","A. The","","","TensorFlow","","","","","","dependencies. It will find all the operations that A depends on, directly or indirectly, by traversing the graph backwards. In this case","","","","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","C,","","","F. Next,","F. Next,","","","","","operation","operation","operation","","","","inputs. These","inputs. These","","","","nodes. In","nodes. In","","","F. Once","","","","","evaluated, operations","evaluated, operations","","","","","","","","","","need,","","","","","evaluated,","","TensorFlow","","","","","","","","parallel. Say","parallel. Say","","","first, A","first, A","","","","unevaluated","unevaluated","unevaluated","","","","can\u2019t","","yet. But","yet. But","","","","","","finished, A","finished, A","","","evaluated. And","evaluated. And","","it\u2019s done,","it\u2019s done,","","eval()","","","","result, and we\u2019re good. You","result, and we\u2019re good. You","result, and we\u2019re good. You","result, and we\u2019re good. You","result, and we\u2019re good. You","","","","","","","once,","","","A","","E,","","","","","","","same. It","same. It","","","","dependencies,","","","","operations,","","then,","","know,","","","","","","","","","satisfied. And","satisfied. And","","","","","","","","both A","","E,","","","","results. So let\u2019s","results. So let\u2019s","results. So let\u2019s","","","","","","targets. This","targets. This","","","","","","","cond() operation,","cond() operation,","","","3 parameters, mask_with_labels,","3 parameters, mask_with_labels,","3 parameters, mask_with_labels,","3 parameters, mask_with_labels,","3 parameters, mask_with_labels,","","","","","y,","","","","","","y_pred. And","y_pred. And","y_pred. And","","","","","reconstruction_targets,","reconstruction_targets,","","","","","","","it,","","","","","loss,","","","","","","loss,","","","","","","reconstruction_targets. Well,","reconstruction_targets. Well,","reconstruction_targets. Well,","","","it,","","earlier, TensorFlow","earlier, TensorFlow","","","","","dependencies. It","dependencies. It","","","","","nodes. And","nodes. And","","","","all! So y_pred","all! So y_pred","all! So y_pred","all! So y_pred","","","first. Since","first. Since","","","","","","parallel, there\u2019s","parallel, there\u2019s","","","","","","","","","","finish. So,","finish. So,","","know, y","know, y","","","next. And","next. And","","mask_with_labels finishes. So","mask_with_labels finishes. So","mask_with_labels finishes. So","mask_with_labels finishes. So","mask_with_labels finishes. So","","","","","True. Now","True. Now","","reconstruction_targets","reconstruction_targets","","","","","","needs,","","","","","","","evaluated. And","evaluated. And","","","","","","","thing,","","mask_with_labels","mask_with_labels","mask_with_labels","","True,","","","","","","y. Which","y. Which","","good. But","good. But","","","y_pred","y_pred","","","","nothing, we\u2019re","nothing, we\u2019re","","","","output. It\u2019s","output. It\u2019s","","","","","","","","","","","","","","loss,","","","","","","","probabilities,","","","","","","","","","predictions,","","","","","won\u2019t","","","overhead. But still, it\u2019s","overhead. But still, it\u2019s","overhead. But still, it\u2019s","overhead. But still, it\u2019s","","","unfortunate. Now","unfortunate. Now","","mask_with_labels","mask_with_labels","mask_with_labels","","","False. Then, again,","False. Then, again,","False. Then, again,","","reconstruction_targets","reconstruction_targets","","","","","thing,","","","","","","","y_pred. But","y_pred. But","y_pred. But","","time,","time,","","","","","nothing. It\u2019s","nothing. It\u2019s","","","placeholder,","","","won\u2019t","","","","time,","","","","","","","","y,","","","mask_with_labels","mask_with_labels","mask_with_labels","","False. Well","False. Well","","","","","","","","array,","array,","","fine. So","fine. So","","","work,","","","","","ugly. So","ugly. So","","","","","","","","","","","","","","","","cond() function,","cond() function,","","","","","","tensor,","","","","","","","","","","","functions. If","functions. If","functions. If","","","","","","functions,","","TensorFlow","","","","","expect. It","expect. It","","","","","","","","by","","","","","","","","","","","dependencies. So","dependencies. So","","","","","","","","","","","","ugliness.","","tried,","","","","","","","","code,","","","","","","","","implementation.","","","","won\u2019t","","","","","night. Sooo! We\u2019ve","night. Sooo! We\u2019ve","night. Sooo! We\u2019ve","","finished! Here\u2019s","finished! Here\u2019s","","","","again. The","again. The","","","","over,","","","","built,","","","","","","phase, let\u2019s","phase, let\u2019s","","","graph. First, let\u2019s","graph. First, let\u2019s","graph. First, let\u2019s","","","","","code. It\u2019s","code. It\u2019s","","","","standard. We","standard. We","","","session,","","","checkpoint","checkpoint","","exists,","","it,","","","","","","variables. Then","variables. Then","","","","","loop,","","","","","epochs,","","","","epoch,","","","","","","","","","","set. And","set. And","","","loop,","","","","","","operation,","","","loss,","","","","batch. We","batch. We","","","","","","","","","","","batch,","","","","mask_with_labels","mask_with_labels","mask_with_labels","","True. That\u2019s","True. That\u2019s","","","","","","","it. In","it. In","","notebook,","notebook,","","","","","","","","","stopping,","","","","","","progress,","","","","","","","","","","","","","","","epoch. But","epoch. But","","","","","","here. After training,","here. After training,","here. After training,","","","","","","test","","","","","","","","","","reconstructions. As","reconstructions. As","","","see,","","","","","correct,","","","","","","good,","","","","","","","images,","","","they\u2019re","","fuzzier,","fuzzier,","","","","see. Here\u2019s","see. Here\u2019s","","code, there\u2019s","code, there\u2019s","","","","","it:","","","","","test images,","","","","","","","","model,","","","","","the","","","","","","decoder\u2019s","","(and","","","","","capsules\u2019 outputs","capsules\u2019 outputs","","","","","","later). The","later). The","","","mentionned","","","","here.","here.","","","","","","","array. This","array. This","","","","","anyway. And finally,","anyway. And finally,","anyway. And finally,","","","","","","","","","","result","","","decoder. So","decoder. So","","","","","","","of","","16","","","in","","","capsule\u2019s","","vector. For example,","vector. For example,","vector. For example,","","","","","","","","","","","","","(the","(the","","","","","","","","","","16 parameters). And","16 parameters). And","16 parameters). And","","","","see,","","","first,","","","","rows,","","","","","","","","","thinner,","thinner,","","","","right,","","","","","","","left,","","","","","","thickness. And","thickness. And","","","","row,","","","","","","","","","","","5","","","","","top,","","","","","","","","","","digit. Before","digit. Before","","finish, I\u2019d","finish, I\u2019d","","","","","","","","","","","","video,","","","","","","","","","","","","response,","","I\u2019m","","","","","","","you,","","","","","","create","","videos. If","videos. If","","","","","","","Machine Learning","Machine Learning","","","","channel,","","","","O\u2019Reilly","","Hands-on Machine Learning","Hands-on Machine Learning","Hands-on Machine Learning","Hands-on Machine Learning","","Scikit-Learn","","TensorFlow, I\u2019ll","TensorFlow, I\u2019ll","","","","","","","description. If","description. If","","","German, there\u2019s","German, there\u2019s","","","","","","","","Christmas. And","Christmas. And","","","","French,","","","","","available. It","available. It","","","","","","","it\u2019s","","","","content. And that\u2019s","content. And that\u2019s","content. And that\u2019s","","","had","","today,","","","","","","","","","","","","","","","","TensorFlow","","","networks. If","networks. If","","did, please, like, share, comment, subscribe,","did, please, like, share, comment, subscribe,","did, please, like, share, comment, subscribe,","did, please, like, share, comment, subscribe,","did, please, like, share, comment, subscribe,","did, please, like, share, comment, subscribe,","","","","","","","","","","","button,","","","","","","","","videos. See","videos. See","","","time!"]},"manual_addl_rep":{"pPN8d0E3900":[3,3,3,3,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,2,2,2,0,0,2,2,2,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,2,0,2,2,2,0,0,0,2,2,2,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,2,2,2,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,1,1,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,2,2,2,0,1,1,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,6,6,6,6,6,6,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,3,3,3,3,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,2,2,2,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,1,1,0,2,2,2,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,2,2,2,0,1,1,0,0,0,0,0,0,3,3,3,3,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,3,3,3,3,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,6,6,6,6,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,18,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,6,6,6,6,6,6,6,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,7,7,7,7,7,7,7,7,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,6,6,6,6,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,7,7,7,7,7,7,7,7,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,7,7,7,7,7,7,7,7,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0],"2Kawrd5szHE":[3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,3,3,3,3,0,2,2,2,0,0,0,0,0,0,0,0,0,2,2,2,0,0,1,1,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,6,6,6,6,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,3,3,3,3,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,1,1,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0,0,0,0,0,5,5,5,5,5,5,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,7,7,7,7,7,7,7,7,0,5,5,5,5,5,5,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,1,1,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,1,1,0,0,2,2,2,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,2,2,2,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,2,2,2,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,2,2,2,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,2,2,2,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,3,3,3,3,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,3,3,3,3,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,2,2,2,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,3,3,3,3,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,2,2,2,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,2,2,2,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,2,2,2,0,2,2,2,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,1,1,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,2,2,2,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,5,5,5,5,5,5,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,3,3,3,3,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,12,12,12,12,12,12,12,12,12,12,12,12,12,0,0,0,2,2,2,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,6,6,6,6,6,6,6,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,2,2,2,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,24,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,3,3,3,3,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,20,0,0,1,1,0,0,0,0,2,2,2,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,2,2,2,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,1,1,0,1,1,0,0,0,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,4,4,4,4,4,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,0,1,1,0,0,0,3,3,3,3,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,0,4,4,4,4,4,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,1,1,0,1,1,0,0,1,1,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,1,1,0,2,2,2,0,0,2,2,2,0,1,1,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,1,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,1,1,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,1,1,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,2,2,2,0,0,0,0,1,1,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,1,1,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,3,3,3,3,0,0,0,1,1,0,0,0,0,0,0,1,1,0,0,1,1,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,2,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,5,5,5,5,5,5,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0]}}